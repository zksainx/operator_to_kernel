shared memory,pid,dur,stream,ts,est. achieved occupancy %,tid,cat,warps per SM,grid,registers per thread,correlation,ph,name,device,External id,queued,blocks per SM,context,block
0,0,3.2,7,6939565294282.014,20,7,kernel,9.523809,"[200, 1, 1]",32,49929,X,"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, unsigned int, long)",0,14341,0,2.380952,1,"[128, 1, 1]"
0,0,2.144,7,6939565294559.486,5,7,kernel,2.380952,"[50, 1, 1]",22,49942,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14351,0,0.595238,1,"[128, 1, 1]"
0,0,2.688,7,6939565294716.285,20,7,kernel,9.523809,"[200, 1, 1]",32,49961,X,"void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<long const, unsigned int>, int, int, unsigned int, unsigned int, long)",0,14355,0,2.380952,1,"[128, 1, 1]"
0,0,1.44,7,6939565294835.517,5,7,kernel,2.380952,"[50, 1, 1]",22,49974,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14365,0,0.595238,1,"[128, 1, 1]"
26624,0,15.264,7,6939565295438.172,29,7,kernel,13.714286,"[24, 2, 3]",82,49994,X,ampere_sgemm_64x32_sliced1x4_tn,0,14376,0,1.714286,1,"[256, 1, 1]"
0,0,2.944,7,6939565295454.748,67,7,kernel,36.57143,"[48, 4, 1]",44,49996,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14376,0,2.285714,1,"[32, 16, 1]"
0,0,2.24,7,6939565295745.436,30,7,kernel,14.285714,"[300, 1, 1]",16,50010,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,14390,0,3.571429,1,"[128, 1, 1]"
36352,0,14.912,7,6939565296115.067,1,7,kernel,0.380952,"[1, 8, 1]",168,50063,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,14417,0,0.095238,1,"[32, 4, 1]"
26624,0,8.0,7,6939565296287.547,13,7,kernel,6.095238,"[8, 2, 4]",82,50087,X,ampere_sgemm_64x32_sliced1x4_tn,0,14431,0,0.761905,1,"[256, 1, 1]"
0,0,2.176,7,6939565296301.787,25,7,kernel,12.190476,"[16, 4, 1]",44,50089,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14431,0,0.761905,1,"[32, 16, 1]"
0,0,2.4,7,6939565296514.299,20,7,kernel,9.523809,"[100, 1, 1]",47,50120,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14434,0,1.190476,1,"[256, 1, 1]"
0,0,1.472,7,6939565296576.155,5,7,kernel,2.380952,"[50, 1, 1]",22,50130,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14439,0,0.595238,1,"[128, 1, 1]"
24,0,3.776,7,6939565296764.89,5,7,kernel,2.380952,"[50, 1, 1]",43,50159,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14441,0,0.595238,1,"[32, 4, 1]"
26624,0,19.68,7,6939565296964.538,13,7,kernel,6.095238,"[32, 2, 1]",82,50179,X,ampere_sgemm_64x32_sliced1x4_tn,0,14453,0,0.761905,1,"[256, 1, 1]"
0,0,1.599,7,6939565297065.082,20,7,kernel,9.523809,"[200, 1, 1]",18,50190,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,14456,0,2.380952,1,"[128, 1, 1]"
0,0,3.04,7,6939565297199.097,79,7,kernel,38.095238,"[400, 1, 1]",47,50223,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14458,0,4.761905,1,"[256, 1, 1]"
26624,0,17.312,7,6939565297331.161,16,7,kernel,7.619048,"[8, 2, 5]",82,50242,X,ampere_sgemm_64x32_sliced1x4_tn,0,14469,0,0.952381,1,"[256, 1, 1]"
0,0,2.176,7,6939565297349.273,25,7,kernel,12.190476,"[16, 4, 1]",44,50244,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14469,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565297462.265,20,7,kernel,9.523809,"[100, 1, 1]",47,50275,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14472,0,1.190476,1,"[256, 1, 1]"
0,0,1.792,7,6939565297512.633,5,7,kernel,2.380952,"[50, 1, 1]",22,50285,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14477,0,0.595238,1,"[128, 1, 1]"
24,0,3.552,7,6939565297674.489,5,7,kernel,2.380952,"[50, 1, 1]",43,50314,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14479,0,0.595238,1,"[32, 4, 1]"
26624,0,15.136,7,6939565297934.072,29,7,kernel,13.714286,"[24, 2, 3]",82,50334,X,ampere_sgemm_64x32_sliced1x4_tn,0,14491,0,1.714286,1,"[256, 1, 1]"
0,0,2.944,7,6939565297949.944,67,7,kernel,36.57143,"[48, 4, 1]",44,50336,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14491,0,2.285714,1,"[32, 16, 1]"
0,0,2.336,7,6939565298125.528,30,7,kernel,14.285714,"[300, 1, 1]",16,50350,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,14505,0,3.571429,1,"[128, 1, 1]"
36352,0,14.529,7,6939565298442.327,1,7,kernel,0.380952,"[1, 8, 1]",168,50403,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,14532,0,0.095238,1,"[32, 4, 1]"
26624,0,8.064,7,6939565298588.407,13,7,kernel,6.095238,"[8, 2, 4]",82,50427,X,ampere_sgemm_64x32_sliced1x4_tn,0,14546,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565298600.919,25,7,kernel,12.190476,"[16, 4, 1]",44,50429,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14546,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565298770.807,20,7,kernel,9.523809,"[100, 1, 1]",47,50460,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14549,0,1.190476,1,"[256, 1, 1]"
0,0,1.536,7,6939565298824.631,5,7,kernel,2.380952,"[50, 1, 1]",22,50470,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14554,0,0.595238,1,"[128, 1, 1]"
24,0,3.648,7,6939565298947.191,5,7,kernel,2.380952,"[50, 1, 1]",43,50499,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14556,0,0.595238,1,"[32, 4, 1]"
26624,0,19.68,7,6939565299113.046,13,7,kernel,6.095238,"[32, 2, 1]",82,50519,X,ampere_sgemm_64x32_sliced1x4_tn,0,14568,0,0.761905,1,"[256, 1, 1]"
0,0,1.632,7,6939565299177.526,20,7,kernel,9.523809,"[200, 1, 1]",18,50530,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,14571,0,2.380952,1,"[128, 1, 1]"
0,0,3.008,7,6939565299292.726,79,7,kernel,38.095238,"[400, 1, 1]",47,50563,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14573,0,4.761905,1,"[256, 1, 1]"
26624,0,17.152,7,6939565299409.366,16,7,kernel,7.619048,"[8, 2, 5]",82,50582,X,ampere_sgemm_64x32_sliced1x4_tn,0,14584,0,0.952381,1,"[256, 1, 1]"
0,0,2.144,7,6939565299427.254,25,7,kernel,12.190476,"[16, 4, 1]",44,50584,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14584,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565299529.43,20,7,kernel,9.523809,"[100, 1, 1]",47,50615,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14587,0,1.190476,1,"[256, 1, 1]"
0,0,1.697,7,6939565299575.733,5,7,kernel,2.380952,"[50, 1, 1]",22,50625,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14592,0,0.595238,1,"[128, 1, 1]"
24,0,3.551,7,6939565299690.71,5,7,kernel,2.380952,"[50, 1, 1]",43,50654,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14594,0,0.595238,1,"[32, 4, 1]"
26624,0,15.104,7,6939565299929.877,29,7,kernel,13.714286,"[24, 2, 3]",82,50674,X,ampere_sgemm_64x32_sliced1x4_tn,0,14606,0,1.714286,1,"[256, 1, 1]"
0,0,2.976,7,6939565299945.749,67,7,kernel,36.57143,"[48, 4, 1]",44,50676,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14606,0,2.285714,1,"[32, 16, 1]"
0,0,2.272,7,6939565300105.269,30,7,kernel,14.285714,"[300, 1, 1]",16,50690,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,14620,0,3.571429,1,"[128, 1, 1]"
36352,0,14.431,7,6939565300425.813,1,7,kernel,0.380952,"[1, 8, 1]",168,50743,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,14647,0,0.095238,1,"[32, 4, 1]"
26624,0,7.84,7,6939565300563.348,13,7,kernel,6.095238,"[8, 2, 4]",82,50767,X,ampere_sgemm_64x32_sliced1x4_tn,0,14661,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565300575.924,25,7,kernel,12.190476,"[16, 4, 1]",44,50769,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14661,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565300724.724,20,7,kernel,9.523809,"[100, 1, 1]",47,50800,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14664,0,1.190476,1,"[256, 1, 1]"
0,0,1.536,7,6939565300775.348,5,7,kernel,2.380952,"[50, 1, 1]",22,50810,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14669,0,0.595238,1,"[128, 1, 1]"
24,0,3.616,7,6939565300895.827,5,7,kernel,2.380952,"[50, 1, 1]",43,50839,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14671,0,0.595238,1,"[32, 4, 1]"
26624,0,19.52,7,6939565301043.571,13,7,kernel,6.095238,"[32, 2, 1]",82,50859,X,ampere_sgemm_64x32_sliced1x4_tn,0,14683,0,0.761905,1,"[256, 1, 1]"
0,0,1.632,7,6939565301106.419,20,7,kernel,9.523809,"[200, 1, 1]",18,50870,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,14686,0,2.380952,1,"[128, 1, 1]"
0,0,3.008,7,6939565301220.467,79,7,kernel,38.095238,"[400, 1, 1]",47,50903,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14688,0,4.761905,1,"[256, 1, 1]"
26624,0,17.216,7,6939565301336.051,16,7,kernel,7.619048,"[8, 2, 5]",82,50922,X,ampere_sgemm_64x32_sliced1x4_tn,0,14699,0,0.952381,1,"[256, 1, 1]"
0,0,2.176,7,6939565301353.971,25,7,kernel,12.190476,"[16, 4, 1]",44,50924,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14699,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565301460.787,20,7,kernel,9.523809,"[100, 1, 1]",47,50955,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14702,0,1.190476,1,"[256, 1, 1]"
0,0,1.632,7,6939565301502.995,5,7,kernel,2.380952,"[50, 1, 1]",22,50965,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14707,0,0.595238,1,"[128, 1, 1]"
24,0,3.519,7,6939565301627.859,5,7,kernel,2.380952,"[50, 1, 1]",43,50994,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14709,0,0.595238,1,"[32, 4, 1]"
26624,0,15.392,7,6939565301884.306,29,7,kernel,13.714286,"[24, 2, 3]",82,51014,X,ampere_sgemm_64x32_sliced1x4_tn,0,14721,0,1.714286,1,"[256, 1, 1]"
0,0,2.976,7,6939565301900.466,67,7,kernel,36.57143,"[48, 4, 1]",44,51016,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14721,0,2.285714,1,"[32, 16, 1]"
0,0,2.336,7,6939565302060.818,30,7,kernel,14.285714,"[300, 1, 1]",16,51030,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,14735,0,3.571429,1,"[128, 1, 1]"
36352,0,14.528,7,6939565302380.593,1,7,kernel,0.380952,"[1, 8, 1]",168,51083,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,14762,0,0.095238,1,"[32, 4, 1]"
26624,0,7.936,7,6939565302519.345,13,7,kernel,6.095238,"[8, 2, 4]",82,51107,X,ampere_sgemm_64x32_sliced1x4_tn,0,14776,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565302531.473,25,7,kernel,12.190476,"[16, 4, 1]",44,51109,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14776,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565302681.617,20,7,kernel,9.523809,"[100, 1, 1]",47,51140,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14779,0,1.190476,1,"[256, 1, 1]"
0,0,1.568,7,6939565302734.193,5,7,kernel,2.380952,"[50, 1, 1]",22,51150,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14784,0,0.595238,1,"[128, 1, 1]"
24,0,3.648,7,6939565302850.801,5,7,kernel,2.380952,"[50, 1, 1]",43,51179,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14786,0,0.595238,1,"[32, 4, 1]"
26624,0,19.776,7,6939565302994.16,13,7,kernel,6.095238,"[32, 2, 1]",82,51199,X,ampere_sgemm_64x32_sliced1x4_tn,0,14798,0,0.761905,1,"[256, 1, 1]"
0,0,1.632,7,6939565303056.528,20,7,kernel,9.523809,"[200, 1, 1]",18,51210,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,14801,0,2.380952,1,"[128, 1, 1]"
0,0,3.008,7,6939565303169.072,79,7,kernel,38.095238,"[400, 1, 1]",47,51243,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14803,0,4.761905,1,"[256, 1, 1]"
26624,0,17.28,7,6939565303287.024,16,7,kernel,7.619048,"[8, 2, 5]",82,51262,X,ampere_sgemm_64x32_sliced1x4_tn,0,14814,0,0.952381,1,"[256, 1, 1]"
0,0,2.144,7,6939565303305.136,25,7,kernel,12.190476,"[16, 4, 1]",44,51264,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14814,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565303414.448,20,7,kernel,9.523809,"[100, 1, 1]",47,51295,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14817,0,1.190476,1,"[256, 1, 1]"
0,0,1.663,7,6939565303457.936,5,7,kernel,2.380952,"[50, 1, 1]",22,51305,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14822,0,0.595238,1,"[128, 1, 1]"
24,0,3.52,7,6939565303574.959,5,7,kernel,2.380952,"[50, 1, 1]",43,51334,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14824,0,0.595238,1,"[32, 4, 1]"
26624,0,14.816,7,6939565303806.063,29,7,kernel,13.714286,"[24, 2, 3]",82,51354,X,ampere_sgemm_64x32_sliced1x4_tn,0,14836,0,1.714286,1,"[256, 1, 1]"
0,0,2.976,7,6939565303821.615,67,7,kernel,36.57143,"[48, 4, 1]",44,51356,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14836,0,2.285714,1,"[32, 16, 1]"
0,0,2.304,7,6939565304067.054,30,7,kernel,14.285714,"[300, 1, 1]",16,51370,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,14850,0,3.571429,1,"[128, 1, 1]"
36352,0,14.24,7,6939565304382.094,1,7,kernel,0.380952,"[1, 8, 1]",168,51423,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,14877,0,0.095238,1,"[32, 4, 1]"
26624,0,7.808,7,6939565304518.35,13,7,kernel,6.095238,"[8, 2, 4]",82,51447,X,ampere_sgemm_64x32_sliced1x4_tn,0,14891,0,0.761905,1,"[256, 1, 1]"
0,0,2.176,7,6939565304530.702,25,7,kernel,12.190476,"[16, 4, 1]",44,51449,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14891,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565304698.03,20,7,kernel,9.523809,"[100, 1, 1]",47,51480,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14894,0,1.190476,1,"[256, 1, 1]"
0,0,1.472,7,6939565304751.598,5,7,kernel,2.380952,"[50, 1, 1]",22,51490,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14899,0,0.595238,1,"[128, 1, 1]"
24,0,3.616,7,6939565304872.846,5,7,kernel,2.380952,"[50, 1, 1]",43,51519,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14901,0,0.595238,1,"[32, 4, 1]"
26624,0,19.84,7,6939565305021.197,13,7,kernel,6.095238,"[32, 2, 1]",82,51539,X,ampere_sgemm_64x32_sliced1x4_tn,0,14913,0,0.761905,1,"[256, 1, 1]"
0,0,1.632,7,6939565305084.141,20,7,kernel,9.523809,"[200, 1, 1]",18,51550,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,14916,0,2.380952,1,"[128, 1, 1]"
0,0,2.976,7,6939565305196.621,79,7,kernel,38.095238,"[400, 1, 1]",47,51583,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14918,0,4.761905,1,"[256, 1, 1]"
26624,0,17.215,7,6939565305316.845,16,7,kernel,7.619048,"[8, 2, 5]",82,51602,X,ampere_sgemm_64x32_sliced1x4_tn,0,14929,0,0.952381,1,"[256, 1, 1]"
0,0,2.176,7,6939565305334.829,25,7,kernel,12.190476,"[16, 4, 1]",44,51604,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14929,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565305427.34,20,7,kernel,9.523809,"[100, 1, 1]",47,51635,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,14932,0,1.190476,1,"[256, 1, 1]"
0,0,1.76,7,6939565305469.452,5,7,kernel,2.380952,"[50, 1, 1]",22,51645,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,14937,0,0.595238,1,"[128, 1, 1]"
24,0,3.52,7,6939565305596.236,5,7,kernel,2.380952,"[50, 1, 1]",43,51674,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,14939,0,0.595238,1,"[32, 4, 1]"
26624,0,15.072,7,6939565305850.156,29,7,kernel,13.714286,"[24, 2, 3]",82,51694,X,ampere_sgemm_64x32_sliced1x4_tn,0,14951,0,1.714286,1,"[256, 1, 1]"
0,0,3.04,7,6939565305865.964,67,7,kernel,36.57143,"[48, 4, 1]",44,51696,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,14951,0,2.285714,1,"[32, 16, 1]"
0,0,2.368,7,6939565306038.444,30,7,kernel,14.285714,"[300, 1, 1]",16,51710,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,14965,0,3.571429,1,"[128, 1, 1]"
36352,0,14.432,7,6939565306361.323,1,7,kernel,0.380952,"[1, 8, 1]",168,51763,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,14992,0,0.095238,1,"[32, 4, 1]"
26624,0,7.904,7,6939565306500.491,13,7,kernel,6.095238,"[8, 2, 4]",82,51787,X,ampere_sgemm_64x32_sliced1x4_tn,0,15006,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565306513.259,25,7,kernel,12.190476,"[16, 4, 1]",44,51789,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15006,0,0.761905,1,"[32, 16, 1]"
0,0,2.432,7,6939565306666.187,20,7,kernel,9.523809,"[100, 1, 1]",47,51820,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15009,0,1.190476,1,"[256, 1, 1]"
0,0,1.44,7,6939565306718.667,5,7,kernel,2.380952,"[50, 1, 1]",22,51830,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15014,0,0.595238,1,"[128, 1, 1]"
24,0,3.584,7,6939565306841.45,5,7,kernel,2.380952,"[50, 1, 1]",43,51859,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15016,0,0.595238,1,"[32, 4, 1]"
26624,0,19.84,7,6939565306987.978,13,7,kernel,6.095238,"[32, 2, 1]",82,51879,X,ampere_sgemm_64x32_sliced1x4_tn,0,15028,0,0.761905,1,"[256, 1, 1]"
0,0,1.664,7,6939565307053.578,20,7,kernel,9.523809,"[200, 1, 1]",18,51890,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,15031,0,2.380952,1,"[128, 1, 1]"
0,0,2.976,7,6939565307169.45,79,7,kernel,38.095238,"[400, 1, 1]",47,51923,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15033,0,4.761905,1,"[256, 1, 1]"
26624,0,17.023,7,6939565307294.762,16,7,kernel,7.619048,"[8, 2, 5]",82,51942,X,ampere_sgemm_64x32_sliced1x4_tn,0,15044,0,0.952381,1,"[256, 1, 1]"
0,0,2.144,7,6939565307312.586,25,7,kernel,12.190476,"[16, 4, 1]",44,51944,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15044,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565307416.841,20,7,kernel,9.523809,"[100, 1, 1]",47,51975,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15047,0,1.190476,1,"[256, 1, 1]"
0,0,1.6,7,6939565307464.137,5,7,kernel,2.380952,"[50, 1, 1]",22,51985,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15052,0,0.595238,1,"[128, 1, 1]"
24,0,3.552,7,6939565307578.057,5,7,kernel,2.380952,"[50, 1, 1]",43,52014,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15054,0,0.595238,1,"[32, 4, 1]"
24,0,3.552,7,6939565307710.281,5,7,kernel,2.380952,"[50, 1, 1]",43,52043,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15061,0,0.595238,1,"[32, 4, 1]"
26624,0,15.137,7,6939565308029.992,29,7,kernel,13.714286,"[24, 2, 3]",82,52063,X,ampere_sgemm_64x32_sliced1x4_tn,0,15073,0,1.714286,1,"[256, 1, 1]"
0,0,2.976,7,6939565308045.832,67,7,kernel,36.57143,"[48, 4, 1]",44,52065,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15073,0,2.285714,1,"[32, 16, 1]"
0,0,2.176,7,6939565308212.008,30,7,kernel,14.285714,"[300, 1, 1]",16,52079,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15087,0,3.571429,1,"[128, 1, 1]"
36352,0,14.657,7,6939565308538.407,1,7,kernel,0.380952,"[1, 8, 1]",168,52132,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15114,0,0.095238,1,"[32, 4, 1]"
26624,0,7.904,7,6939565308674.311,13,7,kernel,6.095238,"[8, 2, 4]",82,52156,X,ampere_sgemm_64x32_sliced1x4_tn,0,15128,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565308686.984,25,7,kernel,12.190476,"[16, 4, 1]",44,52158,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15128,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565308838.343,20,7,kernel,9.523809,"[100, 1, 1]",47,52189,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15131,0,1.190476,1,"[256, 1, 1]"
0,0,1.472,7,6939565308891.591,5,7,kernel,2.380952,"[50, 1, 1]",22,52199,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15136,0,0.595238,1,"[128, 1, 1]"
24,0,4.064,7,6939565309047.911,5,7,kernel,2.380952,"[50, 1, 1]",43,52228,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15138,0,0.595238,1,"[32, 4, 1]"
26624,0,8.224,7,6939565309416.39,13,7,kernel,6.095238,"[8, 2, 4]",82,52248,X,ampere_sgemm_64x32_sliced1x4_tn,0,15156,0,0.761905,1,"[256, 1, 1]"
0,0,2.016,7,6939565309429.702,25,7,kernel,12.190476,"[16, 4, 1]",44,52250,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15156,0,0.761905,1,"[32, 16, 1]"
26624,0,11.36,7,6939565309529.19,25,7,kernel,12.190476,"[16, 2, 4]",82,52270,X,ampere_sgemm_64x32_sliced1x4_tn,0,15164,0,1.52381,1,"[256, 1, 1]"
0,0,2.4,7,6939565309541.446,51,7,kernel,24.380953,"[32, 4, 1]",44,52272,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15164,0,1.52381,1,"[32, 16, 1]"
0,0,2.176,7,6939565309758.758,20,7,kernel,9.523809,"[200, 1, 1]",16,52286,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15178,0,2.380952,1,"[128, 1, 1]"
36352,0,14.272,7,6939565310076.037,1,7,kernel,0.380952,"[1, 8, 1]",168,52336,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15203,0,0.095238,1,"[32, 4, 1]"
26624,0,8.064,7,6939565310218.437,13,7,kernel,6.095238,"[8, 2, 4]",82,52360,X,ampere_sgemm_64x32_sliced1x4_tn,0,15217,0,0.761905,1,"[256, 1, 1]"
0,0,2.176,7,6939565310230.597,25,7,kernel,12.190476,"[16, 4, 1]",44,52362,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15217,0,0.761905,1,"[32, 16, 1]"
0,0,2.337,7,6939565310392.132,20,7,kernel,9.523809,"[100, 1, 1]",47,52393,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15220,0,1.190476,1,"[256, 1, 1]"
0,0,1.761,7,6939565310447.556,5,7,kernel,2.380952,"[50, 1, 1]",22,52403,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15225,0,0.595238,1,"[128, 1, 1]"
24,0,3.552,7,6939565310572.708,5,7,kernel,2.380952,"[50, 1, 1]",43,52432,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15227,0,0.595238,1,"[32, 4, 1]"
26624,0,19.84,7,6939565310835.684,13,7,kernel,6.095238,"[32, 2, 1]",82,52452,X,ampere_sgemm_64x32_sliced1x4_tn,0,15239,0,0.761905,1,"[256, 1, 1]"
0,0,1.6,7,6939565310904.004,20,7,kernel,9.523809,"[200, 1, 1]",18,52463,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,15242,0,2.380952,1,"[128, 1, 1]"
0,0,3.007,7,6939565311023.716,79,7,kernel,38.095238,"[400, 1, 1]",47,52496,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15244,0,4.761905,1,"[256, 1, 1]"
26624,0,17.217,7,6939565311144.195,16,7,kernel,7.619048,"[8, 2, 5]",82,52515,X,ampere_sgemm_64x32_sliced1x4_tn,0,15255,0,0.952381,1,"[256, 1, 1]"
0,0,2.144,7,6939565311162.275,25,7,kernel,12.190476,"[16, 4, 1]",44,52517,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15255,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565311272.323,20,7,kernel,9.523809,"[100, 1, 1]",47,52548,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15258,0,1.190476,1,"[256, 1, 1]"
0,0,1.632,7,6939565311319.971,5,7,kernel,2.380952,"[50, 1, 1]",22,52558,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15263,0,0.595238,1,"[128, 1, 1]"
24,0,3.968,7,6939565311440.803,5,7,kernel,2.380952,"[50, 1, 1]",43,52587,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15265,0,0.595238,1,"[32, 4, 1]"
26624,0,15.232,7,6939565311676.547,29,7,kernel,13.714286,"[24, 2, 3]",82,52607,X,ampere_sgemm_64x32_sliced1x4_tn,0,15277,0,1.714286,1,"[256, 1, 1]"
0,0,2.943,7,6939565311692.611,67,7,kernel,36.57143,"[48, 4, 1]",44,52609,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15277,0,2.285714,1,"[32, 16, 1]"
0,0,2.336,7,6939565311853.059,30,7,kernel,14.285714,"[300, 1, 1]",16,52623,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15291,0,3.571429,1,"[128, 1, 1]"
36352,0,14.368,7,6939565312176.962,1,7,kernel,0.380952,"[1, 8, 1]",168,52676,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15318,0,0.095238,1,"[32, 4, 1]"
26624,0,7.873,7,6939565312319.649,13,7,kernel,6.095238,"[8, 2, 4]",82,52700,X,ampere_sgemm_64x32_sliced1x4_tn,0,15332,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565312330.498,25,7,kernel,12.190476,"[16, 4, 1]",44,52702,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15332,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565312485.473,20,7,kernel,9.523809,"[100, 1, 1]",47,52733,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15335,0,1.190476,1,"[256, 1, 1]"
0,0,1.473,7,6939565312538.369,5,7,kernel,2.380952,"[50, 1, 1]",22,52743,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15340,0,0.595238,1,"[128, 1, 1]"
24,0,3.552,7,6939565312658.529,5,7,kernel,2.380952,"[50, 1, 1]",43,52772,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15342,0,0.595238,1,"[32, 4, 1]"
26624,0,8.128,7,6939565312929.185,13,7,kernel,6.095238,"[8, 2, 4]",82,52792,X,ampere_sgemm_64x32_sliced1x4_tn,0,15360,0,0.761905,1,"[256, 1, 1]"
0,0,1.984,7,6939565312941.505,25,7,kernel,12.190476,"[16, 4, 1]",44,52794,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15360,0,0.761905,1,"[32, 16, 1]"
26624,0,11.233,7,6939565313108.192,25,7,kernel,12.190476,"[16, 2, 4]",82,52814,X,ampere_sgemm_64x32_sliced1x4_tn,0,15368,0,1.52381,1,"[256, 1, 1]"
0,0,2.401,7,6939565313120.256,51,7,kernel,24.380953,"[32, 4, 1]",44,52816,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15368,0,1.52381,1,"[32, 16, 1]"
0,0,2.208,7,6939565313274.272,20,7,kernel,9.523809,"[200, 1, 1]",16,52830,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15382,0,2.380952,1,"[128, 1, 1]"
36352,0,14.56,7,6939565313586.976,1,7,kernel,0.380952,"[1, 8, 1]",168,52880,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15407,0,0.095238,1,"[32, 4, 1]"
26624,0,8.032,7,6939565313723.391,13,7,kernel,6.095238,"[8, 2, 4]",82,52904,X,ampere_sgemm_64x32_sliced1x4_tn,0,15421,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565313735.103,25,7,kernel,12.190476,"[16, 4, 1]",44,52906,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15421,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565313890.495,20,7,kernel,9.523809,"[100, 1, 1]",47,52937,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15424,0,1.190476,1,"[256, 1, 1]"
0,0,1.6,7,6939565313942.879,5,7,kernel,2.380952,"[50, 1, 1]",22,52947,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15429,0,0.595238,1,"[128, 1, 1]"
24,0,3.552,7,6939565314073.599,5,7,kernel,2.380952,"[50, 1, 1]",43,52976,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15431,0,0.595238,1,"[32, 4, 1]"
26624,0,19.617,7,6939565314221.054,13,7,kernel,6.095238,"[32, 2, 1]",82,52996,X,ampere_sgemm_64x32_sliced1x4_tn,0,15443,0,0.761905,1,"[256, 1, 1]"
0,0,1.632,7,6939565314283.231,20,7,kernel,9.523809,"[200, 1, 1]",18,53007,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,15446,0,2.380952,1,"[128, 1, 1]"
0,0,3.073,7,6939565314399.262,79,7,kernel,38.095238,"[400, 1, 1]",47,53040,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15448,0,4.761905,1,"[256, 1, 1]"
26624,0,17.344,7,6939565314519.646,16,7,kernel,7.619048,"[8, 2, 5]",82,53059,X,ampere_sgemm_64x32_sliced1x4_tn,0,15459,0,0.952381,1,"[256, 1, 1]"
0,0,2.176,7,6939565314537.822,25,7,kernel,12.190476,"[16, 4, 1]",44,53061,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15459,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565314654.27,20,7,kernel,9.523809,"[100, 1, 1]",47,53092,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15462,0,1.190476,1,"[256, 1, 1]"
0,0,1.824,7,6939565314699.646,5,7,kernel,2.380952,"[50, 1, 1]",22,53102,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15467,0,0.595238,1,"[128, 1, 1]"
24,0,3.489,7,6939565314817.565,5,7,kernel,2.380952,"[50, 1, 1]",43,53131,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15469,0,0.595238,1,"[32, 4, 1]"
26624,0,15.2,7,6939565315046.718,29,7,kernel,13.714286,"[24, 2, 3]",82,53151,X,ampere_sgemm_64x32_sliced1x4_tn,0,15481,0,1.714286,1,"[256, 1, 1]"
0,0,2.944,7,6939565315062.685,67,7,kernel,36.57143,"[48, 4, 1]",44,53153,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15481,0,2.285714,1,"[32, 16, 1]"
0,0,2.336,7,6939565315220.253,30,7,kernel,14.285714,"[300, 1, 1]",16,53167,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15495,0,3.571429,1,"[128, 1, 1]"
36352,0,14.464,7,6939565315532.605,1,7,kernel,0.380952,"[1, 8, 1]",168,53220,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15522,0,0.095238,1,"[32, 4, 1]"
26624,0,7.777,7,6939565315667.132,13,7,kernel,6.095238,"[8, 2, 4]",82,53244,X,ampere_sgemm_64x32_sliced1x4_tn,0,15536,0,0.761905,1,"[256, 1, 1]"
0,0,2.112,7,6939565315679.9,25,7,kernel,12.190476,"[16, 4, 1]",44,53246,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15536,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565315831.292,20,7,kernel,9.523809,"[100, 1, 1]",47,53277,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15539,0,1.190476,1,"[256, 1, 1]"
0,0,1.472,7,6939565315882.3,5,7,kernel,2.380952,"[50, 1, 1]",22,53287,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15544,0,0.595238,1,"[128, 1, 1]"
24,0,4.032,7,6939565315999.068,5,7,kernel,2.380952,"[50, 1, 1]",43,53316,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15546,0,0.595238,1,"[32, 4, 1]"
26624,0,8.16,7,6939565316263.579,13,7,kernel,6.095238,"[8, 2, 4]",82,53336,X,ampere_sgemm_64x32_sliced1x4_tn,0,15564,0,0.761905,1,"[256, 1, 1]"
0,0,2.015,7,6939565316275.74,25,7,kernel,12.190476,"[16, 4, 1]",44,53338,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15564,0,0.761905,1,"[32, 16, 1]"
26624,0,11.232,7,6939565316365.883,25,7,kernel,12.190476,"[16, 2, 4]",82,53358,X,ampere_sgemm_64x32_sliced1x4_tn,0,15572,0,1.52381,1,"[256, 1, 1]"
0,0,2.432,7,6939565316377.915,51,7,kernel,24.380953,"[32, 4, 1]",44,53360,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15572,0,1.52381,1,"[32, 16, 1]"
0,0,2.208,7,6939565316526.747,20,7,kernel,9.523809,"[200, 1, 1]",16,53374,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15586,0,2.380952,1,"[128, 1, 1]"
36352,0,14.4,7,6939565316831.099,1,7,kernel,0.380952,"[1, 8, 1]",168,53424,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15611,0,0.095238,1,"[32, 4, 1]"
26624,0,8.256,7,6939565316961.306,13,7,kernel,6.095238,"[8, 2, 4]",82,53448,X,ampere_sgemm_64x32_sliced1x4_tn,0,15625,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565316972.731,25,7,kernel,12.190476,"[16, 4, 1]",44,53450,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15625,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565317122.522,20,7,kernel,9.523809,"[100, 1, 1]",47,53481,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15628,0,1.190476,1,"[256, 1, 1]"
0,0,1.408,7,6939565317172.538,5,7,kernel,2.380952,"[50, 1, 1]",22,53491,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15633,0,0.595238,1,"[128, 1, 1]"
24,0,3.52,7,6939565317295.29,5,7,kernel,2.380952,"[50, 1, 1]",43,53520,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15635,0,0.595238,1,"[32, 4, 1]"
26624,0,19.487,7,6939565317437.786,13,7,kernel,6.095238,"[32, 2, 1]",82,53540,X,ampere_sgemm_64x32_sliced1x4_tn,0,15647,0,0.761905,1,"[256, 1, 1]"
0,0,1.633,7,6939565317503.257,20,7,kernel,9.523809,"[200, 1, 1]",18,53551,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,15650,0,2.380952,1,"[128, 1, 1]"
0,0,3.04,7,6939565317634.777,79,7,kernel,38.095238,"[400, 1, 1]",47,53584,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15652,0,4.761905,1,"[256, 1, 1]"
26624,0,17.216,7,6939565317761.337,16,7,kernel,7.619048,"[8, 2, 5]",82,53603,X,ampere_sgemm_64x32_sliced1x4_tn,0,15663,0,0.952381,1,"[256, 1, 1]"
0,0,2.176,7,6939565317779.289,25,7,kernel,12.190476,"[16, 4, 1]",44,53605,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15663,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565317899.865,20,7,kernel,9.523809,"[100, 1, 1]",47,53636,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15666,0,1.190476,1,"[256, 1, 1]"
0,0,1.664,7,6939565317946.457,5,7,kernel,2.380952,"[50, 1, 1]",22,53646,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15671,0,0.595238,1,"[128, 1, 1]"
24,0,3.488,7,6939565318066.265,5,7,kernel,2.380952,"[50, 1, 1]",43,53675,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15673,0,0.595238,1,"[32, 4, 1]"
26624,0,15.328,7,6939565318301.496,29,7,kernel,13.714286,"[24, 2, 3]",82,53695,X,ampere_sgemm_64x32_sliced1x4_tn,0,15685,0,1.714286,1,"[256, 1, 1]"
0,0,2.944,7,6939565318317.592,67,7,kernel,36.57143,"[48, 4, 1]",44,53697,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15685,0,2.285714,1,"[32, 16, 1]"
0,0,2.368,7,6939565318476.856,30,7,kernel,14.285714,"[300, 1, 1]",16,53711,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15699,0,3.571429,1,"[128, 1, 1]"
36352,0,14.495,7,6939565318726.968,1,7,kernel,0.380952,"[1, 8, 1]",168,53764,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15726,0,0.095238,1,"[32, 4, 1]"
26624,0,7.841,7,6939565318860.535,13,7,kernel,6.095238,"[8, 2, 4]",82,53788,X,ampere_sgemm_64x32_sliced1x4_tn,0,15740,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565318873.496,25,7,kernel,12.190476,"[16, 4, 1]",44,53790,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15740,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565319037.463,20,7,kernel,9.523809,"[100, 1, 1]",47,53821,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15743,0,1.190476,1,"[256, 1, 1]"
0,0,1.6,7,6939565319091.447,5,7,kernel,2.380952,"[50, 1, 1]",22,53831,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15748,0,0.595238,1,"[128, 1, 1]"
24,0,3.584,7,6939565319211.831,5,7,kernel,2.380952,"[50, 1, 1]",43,53860,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15750,0,0.595238,1,"[32, 4, 1]"
26624,0,8.064,7,6939565319479.831,13,7,kernel,6.095238,"[8, 2, 4]",82,53880,X,ampere_sgemm_64x32_sliced1x4_tn,0,15768,0,0.761905,1,"[256, 1, 1]"
0,0,2.016,7,6939565319492.215,25,7,kernel,12.190476,"[16, 4, 1]",44,53882,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15768,0,0.761905,1,"[32, 16, 1]"
26624,0,11.296,7,6939565319583.575,25,7,kernel,12.190476,"[16, 2, 4]",82,53902,X,ampere_sgemm_64x32_sliced1x4_tn,0,15776,0,1.52381,1,"[256, 1, 1]"
0,0,2.4,7,6939565319595.703,51,7,kernel,24.380953,"[32, 4, 1]",44,53904,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15776,0,1.52381,1,"[32, 16, 1]"
0,0,2.144,7,6939565319758.263,20,7,kernel,9.523809,"[200, 1, 1]",16,53918,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15790,0,2.380952,1,"[128, 1, 1]"
36352,0,14.656,7,6939565320044.566,1,7,kernel,0.380952,"[1, 8, 1]",168,53968,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15815,0,0.095238,1,"[32, 4, 1]"
26624,0,8.192,7,6939565320176.822,13,7,kernel,6.095238,"[8, 2, 4]",82,53992,X,ampere_sgemm_64x32_sliced1x4_tn,0,15829,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565320188.15,25,7,kernel,12.190476,"[16, 4, 1]",44,53994,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15829,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565320337.174,20,7,kernel,9.523809,"[100, 1, 1]",47,54025,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15832,0,1.190476,1,"[256, 1, 1]"
0,0,1.536,7,6939565320390.357,5,7,kernel,2.380952,"[50, 1, 1]",22,54035,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15837,0,0.595238,1,"[128, 1, 1]"
24,0,3.584,7,6939565320508.277,5,7,kernel,2.380952,"[50, 1, 1]",43,54064,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15839,0,0.595238,1,"[32, 4, 1]"
26624,0,19.776,7,6939565320653.589,13,7,kernel,6.095238,"[32, 2, 1]",82,54084,X,ampere_sgemm_64x32_sliced1x4_tn,0,15851,0,0.761905,1,"[256, 1, 1]"
0,0,1.664,7,6939565320715.669,20,7,kernel,9.523809,"[200, 1, 1]",18,54095,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,15854,0,2.380952,1,"[128, 1, 1]"
0,0,3.104,7,6939565320824.309,79,7,kernel,38.095238,"[400, 1, 1]",47,54128,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15856,0,4.761905,1,"[256, 1, 1]"
26624,0,17.183,7,6939565320950.229,16,7,kernel,7.619048,"[8, 2, 5]",82,54147,X,ampere_sgemm_64x32_sliced1x4_tn,0,15867,0,0.952381,1,"[256, 1, 1]"
0,0,2.143,7,6939565320968.213,25,7,kernel,12.190476,"[16, 4, 1]",44,54149,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15867,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565321147.124,20,7,kernel,9.523809,"[100, 1, 1]",47,54180,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15870,0,1.190476,1,"[256, 1, 1]"
0,0,1.76,7,6939565321195.988,5,7,kernel,2.380952,"[50, 1, 1]",22,54190,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15875,0,0.595238,1,"[128, 1, 1]"
24,0,3.744,7,6939565321307.828,5,7,kernel,2.380952,"[50, 1, 1]",43,54219,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15877,0,0.595238,1,"[32, 4, 1]"
26624,0,15.008,7,6939565321539.731,29,7,kernel,13.714286,"[24, 2, 3]",82,54239,X,ampere_sgemm_64x32_sliced1x4_tn,0,15889,0,1.714286,1,"[256, 1, 1]"
0,0,2.944,7,6939565321555.508,67,7,kernel,36.57143,"[48, 4, 1]",44,54241,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15889,0,2.285714,1,"[32, 16, 1]"
0,0,2.432,7,6939565321748.883,30,7,kernel,14.285714,"[300, 1, 1]",16,54255,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15903,0,3.571429,1,"[128, 1, 1]"
36352,0,14.496,7,6939565322058.195,1,7,kernel,0.380952,"[1, 8, 1]",168,54308,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,15930,0,0.095238,1,"[32, 4, 1]"
26624,0,7.841,7,6939565322197.234,13,7,kernel,6.095238,"[8, 2, 4]",82,54332,X,ampere_sgemm_64x32_sliced1x4_tn,0,15944,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565322208.691,25,7,kernel,12.190476,"[16, 4, 1]",44,54334,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15944,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565322363.474,20,7,kernel,9.523809,"[100, 1, 1]",47,54365,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,15947,0,1.190476,1,"[256, 1, 1]"
0,0,1.472,7,6939565322415.154,5,7,kernel,2.380952,"[50, 1, 1]",22,54375,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,15952,0,0.595238,1,"[128, 1, 1]"
24,0,3.584,7,6939565322532.658,5,7,kernel,2.380952,"[50, 1, 1]",43,54404,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,15954,0,0.595238,1,"[32, 4, 1]"
26624,0,8.192,7,6939565322806.482,13,7,kernel,6.095238,"[8, 2, 4]",82,54424,X,ampere_sgemm_64x32_sliced1x4_tn,0,15972,0,0.761905,1,"[256, 1, 1]"
0,0,2.047,7,6939565322818.226,25,7,kernel,12.190476,"[16, 4, 1]",44,54426,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15972,0,0.761905,1,"[32, 16, 1]"
26624,0,11.2,7,6939565322911.218,25,7,kernel,12.190476,"[16, 2, 4]",82,54446,X,ampere_sgemm_64x32_sliced1x4_tn,0,15980,0,1.52381,1,"[256, 1, 1]"
0,0,2.433,7,6939565322923.153,51,7,kernel,24.380953,"[32, 4, 1]",44,54448,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,15980,0,1.52381,1,"[32, 16, 1]"
0,0,2.176,7,6939565323080.241,20,7,kernel,9.523809,"[200, 1, 1]",16,54462,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,15994,0,2.380952,1,"[128, 1, 1]"
36352,0,14.496,7,6939565323378.961,1,7,kernel,0.380952,"[1, 8, 1]",168,54512,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,16019,0,0.095238,1,"[32, 4, 1]"
26624,0,8.0,7,6939565323511.569,13,7,kernel,6.095238,"[8, 2, 4]",82,54536,X,ampere_sgemm_64x32_sliced1x4_tn,0,16033,0,0.761905,1,"[256, 1, 1]"
0,0,2.175,7,6939565323522.801,25,7,kernel,12.190476,"[16, 4, 1]",44,54538,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16033,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565323675.76,20,7,kernel,9.523809,"[100, 1, 1]",47,54569,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,16036,0,1.190476,1,"[256, 1, 1]"
0,0,1.472,7,6939565323728.112,5,7,kernel,2.380952,"[50, 1, 1]",22,54579,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,16041,0,0.595238,1,"[128, 1, 1]"
24,0,4.064,7,6939565323844.784,5,7,kernel,2.380952,"[50, 1, 1]",43,54608,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,16043,0,0.595238,1,"[32, 4, 1]"
26624,0,19.648,7,6939565323987.472,13,7,kernel,6.095238,"[32, 2, 1]",82,54628,X,ampere_sgemm_64x32_sliced1x4_tn,0,16055,0,0.761905,1,"[256, 1, 1]"
0,0,1.664,7,6939565324050.096,20,7,kernel,9.523809,"[200, 1, 1]",18,54639,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,16058,0,2.380952,1,"[128, 1, 1]"
0,0,3.04,7,6939565324165.135,79,7,kernel,38.095238,"[400, 1, 1]",47,54672,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,16060,0,4.761905,1,"[256, 1, 1]"
26624,0,17.12,7,6939565324280.944,16,7,kernel,7.619048,"[8, 2, 5]",82,54691,X,ampere_sgemm_64x32_sliced1x4_tn,0,16071,0,0.952381,1,"[256, 1, 1]"
0,0,2.144,7,6939565324298.799,25,7,kernel,12.190476,"[16, 4, 1]",44,54693,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16071,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565324402.095,20,7,kernel,9.523809,"[100, 1, 1]",47,54724,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,16074,0,1.190476,1,"[256, 1, 1]"
0,0,1.6,7,6939565324448.367,5,7,kernel,2.380952,"[50, 1, 1]",22,54734,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,16079,0,0.595238,1,"[128, 1, 1]"
24,0,3.552,7,6939565324554.351,5,7,kernel,2.380952,"[50, 1, 1]",43,54763,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,16081,0,0.595238,1,"[32, 4, 1]"
26624,0,15.232,7,6939565324778.639,29,7,kernel,13.714286,"[24, 2, 3]",82,54783,X,ampere_sgemm_64x32_sliced1x4_tn,0,16093,0,1.714286,1,"[256, 1, 1]"
0,0,2.944,7,6939565324794.638,67,7,kernel,36.57143,"[48, 4, 1]",44,54785,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16093,0,2.285714,1,"[32, 16, 1]"
0,0,2.304,7,6939565324965.742,30,7,kernel,14.285714,"[300, 1, 1]",16,54799,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,16107,0,3.571429,1,"[128, 1, 1]"
36352,0,14.56,7,6939565325297.998,1,7,kernel,0.380952,"[1, 8, 1]",168,54852,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,16134,0,0.095238,1,"[32, 4, 1]"
26624,0,7.841,7,6939565325437.325,13,7,kernel,6.095238,"[8, 2, 4]",82,54876,X,ampere_sgemm_64x32_sliced1x4_tn,0,16148,0,0.761905,1,"[256, 1, 1]"
0,0,2.112,7,6939565325449.933,25,7,kernel,12.190476,"[16, 4, 1]",44,54878,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16148,0,0.761905,1,"[32, 16, 1]"
0,0,2.368,7,6939565325624.557,20,7,kernel,9.523809,"[100, 1, 1]",47,54909,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,16151,0,1.190476,1,"[256, 1, 1]"
0,0,1.472,7,6939565325683.629,5,7,kernel,2.380952,"[50, 1, 1]",22,54919,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,16156,0,0.595238,1,"[128, 1, 1]"
24,0,3.584,7,6939565325808.237,5,7,kernel,2.380952,"[50, 1, 1]",43,54948,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,16158,0,0.595238,1,"[32, 4, 1]"
26624,0,8.033,7,6939565326091.82,13,7,kernel,6.095238,"[8, 2, 4]",82,54968,X,ampere_sgemm_64x32_sliced1x4_tn,0,16176,0,0.761905,1,"[256, 1, 1]"
0,0,2.015,7,6939565326104.045,25,7,kernel,12.190476,"[16, 4, 1]",44,54970,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16176,0,0.761905,1,"[32, 16, 1]"
26624,0,11.296,7,6939565326204.236,25,7,kernel,12.190476,"[16, 2, 4]",82,54990,X,ampere_sgemm_64x32_sliced1x4_tn,0,16184,0,1.52381,1,"[256, 1, 1]"
0,0,2.401,7,6939565326216.332,51,7,kernel,24.380953,"[32, 4, 1]",44,54992,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16184,0,1.52381,1,"[32, 16, 1]"
0,0,2.176,7,6939565326371.468,20,7,kernel,9.523809,"[200, 1, 1]",16,55006,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,16198,0,2.380952,1,"[128, 1, 1]"
36352,0,14.401,7,6939565326660.939,1,7,kernel,0.380952,"[1, 8, 1]",168,55056,X,"fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::arch::Sm80, true, 64, 64, 64, true, true>::Params)",0,16223,0,0.095238,1,"[32, 4, 1]"
26624,0,8.193,7,6939565326793.579,13,7,kernel,6.095238,"[8, 2, 4]",82,55080,X,ampere_sgemm_64x32_sliced1x4_tn,0,16237,0,0.761905,1,"[256, 1, 1]"
0,0,2.144,7,6939565326804.748,25,7,kernel,12.190476,"[16, 4, 1]",44,55082,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16237,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565326956.427,20,7,kernel,9.523809,"[100, 1, 1]",47,55113,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,16240,0,1.190476,1,"[256, 1, 1]"
0,0,1.664,7,6939565327008.843,5,7,kernel,2.380952,"[50, 1, 1]",22,55123,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,16245,0,0.595238,1,"[128, 1, 1]"
24,0,3.552,7,6939565327127.787,5,7,kernel,2.380952,"[50, 1, 1]",43,55152,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,16247,0,0.595238,1,"[32, 4, 1]"
26624,0,19.745,7,6939565327315.818,13,7,kernel,6.095238,"[32, 2, 1]",82,55172,X,ampere_sgemm_64x32_sliced1x4_tn,0,16259,0,0.761905,1,"[256, 1, 1]"
0,0,1.631,7,6939565327384.171,20,7,kernel,9.523809,"[200, 1, 1]",18,55183,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,16262,0,2.380952,1,"[128, 1, 1]"
0,0,3.008,7,6939565327487.178,79,7,kernel,38.095238,"[400, 1, 1]",47,55216,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,16264,0,4.761905,1,"[256, 1, 1]"
26624,0,17.056,7,6939565327588.746,16,7,kernel,7.619048,"[8, 2, 5]",82,55235,X,ampere_sgemm_64x32_sliced1x4_tn,0,16275,0,0.952381,1,"[256, 1, 1]"
0,0,2.176,7,6939565327606.538,25,7,kernel,12.190476,"[16, 4, 1]",44,55237,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, true, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16275,0,0.761905,1,"[32, 16, 1]"
0,0,2.336,7,6939565327698.442,20,7,kernel,9.523809,"[100, 1, 1]",47,55268,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,16278,0,1.190476,1,"[256, 1, 1]"
0,0,1.792,7,6939565327741.962,5,7,kernel,2.380952,"[50, 1, 1]",22,55278,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,16283,0,0.595238,1,"[128, 1, 1]"
24,0,3.52,7,6939565327840.234,5,7,kernel,2.380952,"[50, 1, 1]",43,55307,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,16285,0,0.595238,1,"[32, 4, 1]"
24,0,3.519,7,6939565327977.29,5,7,kernel,2.380952,"[50, 1, 1]",43,55336,X,"void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*)",0,16292,0,0.595238,1,"[32, 4, 1]"
16384,0,57.792,7,6939565328169.161,31,7,kernel,15.047619,"[79, 2, 1]",57,55356,X,ampere_sgemm_128x32_tn,0,16306,0,1.880952,1,"[256, 1, 1]"
