Ev Idx,tid,cat,name,pid,dur,Python parent id,Python module id,Python id,ts,ph
1971,0,python_function,threading.py(973): _bootstrap,0,40098.618,,,1,6939565290255.878,X
1972,0,python_function,threading.py(1016): _bootstrap_inner,0,40097.422,1,,2,6939565290257.074,X
1973,0,python_function,decorator.py(232): fun,0,40093.747,2,,3,6939565290260.749,X
1974,0,python_function,IPython/core/history.py(61): only_when_enabled,0,40092.309,3,,4,6939565290262.187,X
1975,0,python_function,IPython/core/history.py(903): run,0,40087.427,4,,5,6939565290267.069,X
1976,0,python_function,threading.py(607): wait,0,40086.229,5,,6,6939565290268.267,X
1977,0,python_function,threading.py(320): wait,0,40084.767,6,,7,6939565290269.729,X
1978,0,python_function,threading.py(973): _bootstrap,0,40078.299,7,,8,6939565290276.197,X
1979,0,python_function,threading.py(1016): _bootstrap_inner,0,40077.855,8,,9,6939565290276.641,X
1980,0,python_function,ipykernel/control.py(23): run,0,40075.888,9,,10,6939565290278.608,X
1981,0,python_function,tornado/platform/asyncio.py(205): start,0,40071.733,10,,11,6939565290282.763,X
1982,0,python_function,asyncio/base_events.py(603): run_forever,0,40069.182,11,,12,6939565290285.314,X
1983,0,python_function,asyncio/base_events.py(1871): _run_once,0,40067.96,12,,13,6939565290286.536,X
1984,0,python_function,selectors.py(469): select,0,40066.493,13,,14,6939565290288.003,X
1985,0,python_function,threading.py(973): _bootstrap,0,40061.822,14,,15,6939565290292.674,X
1986,0,python_function,threading.py(1016): _bootstrap_inner,0,40061.057,15,,16,6939565290293.439,X
1987,0,python_function,threading.py(953): run,0,40059.962,16,,17,6939565290294.534,X
1988,0,python_function,ipykernel/iostream.py(387): _watch_pipe_fd,0,40058.393,17,,18,6939565290296.103,X
1989,0,python_function,threading.py(973): _bootstrap,0,40040.448,18,,19,6939565290314.048,X
1990,0,python_function,threading.py(1016): _bootstrap_inner,0,40039.829,19,,20,6939565290314.667,X
1991,0,python_function,threading.py(953): run,0,40038.144,20,,21,6939565290316.352,X
1992,0,python_function,ipykernel/iostream.py(387): _watch_pipe_fd,0,40037.768,21,,22,6939565290316.728,X
1993,0,python_function,threading.py(973): _bootstrap,0,40024.913,22,,23,6939565290329.583,X
1994,0,python_function,threading.py(1016): _bootstrap_inner,0,40024.514,23,,24,6939565290329.982,X
1995,0,python_function,ipykernel/heartbeat.py(106): run,0,40022.119,24,,25,6939565290332.377,X
1996,0,python_function,threading.py(973): _bootstrap,0,40009.358,25,,26,6939565290345.138,X
1997,0,python_function,threading.py(1016): _bootstrap_inner,0,40009.056,26,,27,6939565290345.44,X
1998,0,python_function,threading.py(953): run,0,40007.818,27,,28,6939565290346.678,X
1999,0,python_function,ipykernel/iostream.py(92): _thread_main,0,40006.74,28,,29,6939565290347.756,X
2000,0,python_function,tornado/platform/asyncio.py(205): start,0,40005.497,29,,30,6939565290348.999,X
2001,0,python_function,asyncio/base_events.py(603): run_forever,0,40004.854,30,,31,6939565290349.642,X
2002,0,python_function,asyncio/base_events.py(1871): _run_once,0,40004.345,31,,32,6939565290350.151,X
2003,0,python_function,selectors.py(469): select,0,40003.817,32,,33,6939565290350.679,X
2004,122891,python_function,runpy.py(196): _run_module_as_main,122891,39987.336,,,34,6939565290367.16,X
2005,122891,python_function,runpy.py(86): _run_code,122891,39985.82,34,,35,6939565290368.676,X
2006,122891,python_function,ipykernel_launcher.py(18): <module>,122891,39984.138,35,,36,6939565290370.358,X
2007,122891,python_function,traitlets/config/application.py(1075): launch_instance,122891,39982.035,36,,37,6939565290372.461,X
2008,122891,python_function,ipykernel/kernelapp.py(739): start,122891,39977.174,37,,38,6939565290377.322,X
2009,122891,python_function,tornado/platform/asyncio.py(205): start,122891,39976.714,38,,39,6939565290377.782,X
2010,122891,python_function,asyncio/base_events.py(603): run_forever,122891,39976.241,39,,40,6939565290378.255,X
2011,122891,python_function,asyncio/base_events.py(1909): _run_once,122891,39975.204,40,,41,6939565290379.292,X
2012,122891,python_function,asyncio/events.py(80): _run,122891,39971.426,41,,42,6939565290383.07,X
2013,122891,python_function,ipykernel/kernelbase.py(545): dispatch_queue,122891,39970.414,42,,43,6939565290384.082,X
2014,122891,python_function,ipykernel/kernelbase.py(534): process_one,122891,39969.503,43,,44,6939565290384.993,X
2015,122891,python_function,ipykernel/kernelbase.py(437): dispatch_shell,122891,39967.892,44,,45,6939565290386.604,X
2016,122891,python_function,ipykernel/ipkernel.py(362): execute_request,122891,39966.3,45,,46,6939565290388.196,X
2017,122891,python_function,ipykernel/kernelbase.py(778): execute_request,122891,39965.182,46,,47,6939565290389.314,X
2018,122891,python_function,ipykernel/ipkernel.py(449): do_execute,122891,39963.436,47,,48,6939565290391.06,X
2019,122891,python_function,ipykernel/zmqshell.py(549): run_cell,122891,39962.094,48,,49,6939565290392.402,X
2020,122891,python_function,IPython/core/interactiveshell.py(3075): run_cell,122891,39955.851,49,,50,6939565290398.645,X
2021,122891,python_function,IPython/core/interactiveshell.py(3130): _run_cell,122891,39954.89,50,,51,6939565290399.606,X
2022,122891,python_function,IPython/core/async_helpers.py(128): _pseudo_sync_runner,122891,39943.775,51,,52,6939565290410.721,X
2023,122891,python_function,IPython/core/interactiveshell.py(3334): run_cell_async,122891,39941.953,52,,53,6939565290412.543,X
2024,122891,python_function,IPython/core/interactiveshell.py(3517): run_ast_nodes,122891,39940.29,53,,54,6939565290414.206,X
2025,122891,python_function,IPython/core/interactiveshell.py(3577): run_code,122891,39938.946,54,,55,6939565290415.55,X
2026,122891,python_function,/tmp/ipykernel_122891/1902610750.py(25): <module>,122891,39938.107,55,,56,6939565290416.389,X
2027,122891,python_function,pytorch_tracing.py(31): py_tracing_forward,122891,39936.626,56,,57,6939565290417.87,X
2028,122891,python_function,torch/profiler/profiler.py(776): step,122891,450.497,57,,58,6939565290418.79,X
2029,122891,python_function,torch/profiler/profiler.py(793): _transit_action,122891,170.458,58,,59,6939565290419.947,X
2030,122891,python_function,torch/profiler/profiler.py(174): start_trace,122891,168.361,59,,60,6939565290421.012,X
2031,122891,python_function,torch/autograd/profiler.py(339): _start_trace,122891,49.486,60,,61,6939565290421.984,X
2032,122891,python_function,<built-in function perf_counter_ns>,122891,3.093,61,,62,6939565290463.08,X
2033,122891,python_function,torch/profiler/profiler.py(304): add_metadata_json,122891,19.052,60,,63,6939565290477.234,X
2034,122891,python_function,<built-in method _add_metadata_json of PyCapsule object at 0x7fb81c7850e0>,122891,6.138,63,,64,6939565290489.912,X
2035,122891,python_function,torch/profiler/profiler.py(304): add_metadata_json,122891,2.044,60,,65,6939565290497.529,X
2036,122891,python_function,<built-in method _add_metadata_json of PyCapsule object at 0x7fb81c7850e0>,122891,1.453,65,,66,6939565290498.032,X
2037,122891,python_function,<built-in method kineto_available of PyCapsule object at 0x7fb81c785140>,122891,1.151,60,,67,6939565290504.661,X
2038,122891,python_function,torch/profiler/profiler.py(319): _get_distributed_info,122891,39.477,60,,68,6939565290508.646,X
2039,122891,python_function,torch/distributed/__init__.py(14): is_available,122891,4.583,68,,69,6939565290521.932,X
2040,122891,python_function,<built-in function hasattr>,122891,1.183,69,,70,6939565290525.188,X
2041,122891,python_function,torch/distributed/distributed_c10d.py(1121): is_initialized,122891,12.883,68,,71,6939565290534.975,X
2042,122891,python_function,torch/distributed/distributed_c10d.py(705): WORLD,122891,5.642,71,,72,6939565290541.911,X
2043,122891,python_function,torch/distributed/distributed_c10d.py(576): default_pg,122891,0.915,72,,73,6939565290546.373,X
2044,122891,python_function,<built-in function hasattr>,122891,1.225,60,,74,6939565290550.385,X
2045,122891,python_function,torch/utils/_config_module.py(379): __getattr__,122891,9.961,60,,75,6939565290570.268,X
2046,122891,python_function,torch/utils/_config_module.py(148): __getattr__,122891,4.031,75,,76,6939565290575.826,X
2047,122891,python_function,<built-in method items of dict object at 0x7fb80d470140>,122891,0.302,60,,77,6939565290587.838,X
2048,122891,python_function,torch/autograd/profiler.py(1141): increment_step,122891,31.92,58,,78,6939565290613.633,X
2049,122891,python_function,<built-in method values of collections.defaultdict object at 0x7fb81c7b0ea0>,122891,0.266,78,,79,6939565290622.806,X
2050,122891,python_function,<built-in function max>,122891,1.812,78,,80,6939565290626.025,X
2051,122891,python_function,<built-in method _kineto_step of PyCapsule object at 0x7fb81c785110>,122891,4.309,78,,81,6939565290638.273,X
2052,122891,python_function,torch/autograd/profiler.py(721): __init__,122891,16.438,58,,82,6939565290651.099,X
2053,122891,python_function,typing.py(306): inner,122891,5.803,82,,83,6939565290658.705,X
2054,122891,python_function,torch/jit/__init__.py(128): annotate,122891,0.567,82,,84,6939565290666.467,X
2055,122891,python_function,torch/autograd/profiler.py(732): __enter__,122891,190.221,58,,85,6939565290678.634,X
2056,122891,python_function,torch/_ops.py(1105): __call__,122891,182.823,85,,86,6939565290685.159,X
2057,122891,python_function,<built-in method _record_function_enter_new of PyCapsule object at 0x7fb81c0beca0>,122891,174.484,86,,87,6939565290693.222,X
2058,122891,python_function,nn.Module: Transformer_0,122891,37303.795,57,0,88,6939565290902.112,X
2059,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,37298.689,88,,89,6939565290905.538,X
2060,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.564,89,,90,6939565290909.42,X
2061,122891,python_function,transformer.py(39): forward,122891,37284.683,89,,91,6939565290915.689,X
2062,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,3.289,91,,92,6939565290919.858,X
2063,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.532,91,,93,6939565290924.899,X
2064,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.517,91,,94,6939565290928.362,X
2065,122891,python_function,nn.Module: Embedding_0,122891,3408.63,91,0,95,6939565290941.303,X
2066,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,3403.982,95,,96,6939565290943.839,X
2067,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.693,96,,97,6939565290944.334,X
2068,122891,python_function,torch/nn/modules/sparse.py(189): forward,122891,3397.139,96,,98,6939565290949.043,X
2069,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.741,98,,99,6939565290952.892,X
2070,122891,python_function,torch/nn/functional.py(2437): embedding,122891,3387.671,98,,100,6939565290956.431,X
2071,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.483,100,,101,6939565290959.578,X
2072,122891,python_function,<built-in method embedding of type object at 0x7fb97ee5f1c0>,122891,3377.773,100,,102,6939565290965.244,X
2073,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.729,91,,103,6939565294362.278,X
2074,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.178,91,,104,6939565294579.429,X
2075,122891,python_function,nn.Module: Embedding_1,122891,133.603,91,1,105,6939565294599.147,X
2076,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,130.404,105,,106,6939565294601.541,X
2077,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,4.5,106,,107,6939565294603.95,X
2078,122891,python_function,torch/nn/modules/sparse.py(189): forward,122891,119.188,106,,108,6939565294612.301,X
2079,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.9,108,,109,6939565294615.417,X
2080,122891,python_function,torch/nn/functional.py(2437): embedding,122891,112.741,108,,110,6939565294618.363,X
2081,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.594,110,,111,6939565294619.377,X
2082,122891,python_function,<built-in method embedding of type object at 0x7fb97ee5f1c0>,122891,109.046,110,,112,6939565294621.841,X
2083,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.955,91,,113,6939565294735.513,X
2084,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,36.149,91,,114,6939565294859.74,X
2085,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.578,91,,115,6939565294903.173,X
2086,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.69,91,,116,6939565294921.105,X
2087,122891,python_function,nn.Module: Transformer_0,122891,33061.409,91,0,117,6939565294936.63,X
2088,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,33057.999,117,,118,6939565294938.973,X
2089,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.68,118,,119,6939565294940.12,X
2090,122891,python_function,torch/nn/modules/transformer.py(172): forward,122891,33049.262,118,,120,6939565294947.034,X
2091,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.812,120,,121,6939565294950.431,X
2092,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.478,120,,122,6939565294952.759,X
2093,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.268,120,,123,6939565294954.583,X
2094,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.741,120,,124,6939565294955.931,X
2095,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.492,120,,125,6939565294957.546,X
2096,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.887,120,,126,6939565294960.206,X
2097,122891,python_function,nn.Module: TransformerEncoder_0,122891,12760.528,120,0,127,6939565294972.003,X
2098,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,12756.069,127,,128,6939565294974.718,X
2099,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.505,128,,129,6939565294975.076,X
2100,122891,python_function,torch/nn/modules/transformer.py(384): forward,122891,12750.129,128,,130,6939565294979.738,X
2101,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.685,130,,131,6939565294982.869,X
2102,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.611,130,,132,6939565294986.629,X
2103,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.334,130,,133,6939565294989.682,X
2104,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.972,130,,134,6939565294993.159,X
2105,122891,python_function,torch/nn/modules/container.py(329): __getitem__,122891,23.841,130,,135,6939565294999.372,X
2106,122891,python_function,<built-in function isinstance>,122891,0.39,135,,136,6939565295003.49,X
2107,122891,python_function,torch/nn/modules/container.py(312): _get_abs_string_index,122891,15.515,135,,137,6939565295006.94,X
2108,122891,python_function,<built-in function index>,122891,0.23,137,,138,6939565295013.968,X
2109,122891,python_function,<built-in function len>,122891,3.071,137,,139,6939565295016.032,X
2110,122891,python_function,torch/nn/modules/container.py(350): __len__,122891,1.405,139,,140,6939565295017.399,X
2111,122891,python_function,<built-in function len>,122891,0.226,140,,141,6939565295018.427,X
2112,122891,python_function,<built-in function len>,122891,0.799,137,,142,6939565295020.035,X
2113,122891,python_function,torch/nn/modules/container.py(350): __len__,122891,0.39,142,,143,6939565295020.375,X
2114,122891,python_function,<built-in function len>,122891,0.088,143,,144,6939565295020.619,X
2115,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.218,130,,145,6939565295025.516,X
2116,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.731,130,,146,6939565295031.492,X
2117,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.215,146,,147,6939565295033.635,X
2118,122891,python_function,<built-in function hasattr>,122891,0.745,130,,148,6939565295035.435,X
2119,122891,python_function,torch/nn/modules/transformer.py(47): _get_seq_len,122891,38.731,130,,149,6939565295051.93,X
2120,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,2.091,149,,150,6939565295086.447,X
2121,122891,python_function,<built-in function len>,122891,0.49,149,,151,6939565295089.238,X
2122,122891,python_function,torch/nn/modules/transformer.py(1158): _detect_is_causal_mask,122891,0.631,130,,152,6939565295092.69,X
2123,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.049,130,,153,6939565295095.167,X
2124,122891,python_function,torch/nn/modules/container.py(354): __iter__,122891,4.678,130,,154,6939565295097.921,X
2125,122891,python_function,<built-in method values of collections.defaultdict object at 0x7fb81c7b0ea0>,122891,0.343,154,,155,6939565295099.34,X
2126,122891,python_function,<built-in function iter>,122891,0.404,154,,156,6939565295101.984,X
2127,122891,python_function,nn.Module: TransformerEncoderLayer_0,122891,2587.518,130,0,157,6939565295110.443,X
2128,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,2584.367,157,,158,6939565295112.387,X
2129,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.808,158,,159,6939565295112.904,X
2130,122891,python_function,torch/nn/modules/transformer.py(750): forward,122891,2579.134,158,,160,6939565295116.688,X
2131,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.335,160,,161,6939565295117.908,X
2132,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.207,160,,162,6939565295118.986,X
2133,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.137,160,,163,6939565295120.087,X
2134,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,0.758,160,,164,6939565295121.041,X
2135,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.091,164,,165,6939565295121.557,X
2136,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.188,160,,166,6939565295122.584,X
2137,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.252,160,,167,6939565295125.927,X
2138,122891,python_function,torch/nn/modules/transformer.py(911): _sa_block,122891,1398.679,160,,168,6939565295128.811,X
2139,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.328,168,,169,6939565295129.992,X
2140,122891,python_function,nn.Module: MultiheadAttention_0,122891,1191.538,168,0,170,6939565295146.665,X
2141,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,1186.025,170,,171,6939565295150.396,X
2142,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.714,171,,172,6939565295151.008,X
2143,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,1178.628,171,,173,6939565295155.946,X
2144,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.307,173,,174,6939565295157.388,X
2145,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.259,173,,175,6939565295159.253,X
2146,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.35,173,,176,6939565295160.861,X
2147,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.392,173,,177,6939565295163.006,X
2148,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,0.734,173,,178,6939565295164.583,X
2149,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.138,178,,179,6939565295165.063,X
2150,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.625,173,,180,6939565295167.336,X
2151,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.365,173,,181,6939565295169.36,X
2152,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.395,173,,182,6939565295171.523,X
2153,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.358,173,,183,6939565295173.325,X
2154,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.409,173,,184,6939565295177.504,X
2155,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.393,173,,185,6939565295179.404,X
2156,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.338,173,,186,6939565295182.209,X
2157,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.801,173,,187,6939565295186.013,X
2158,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.903,173,,188,6939565295188.443,X
2159,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.661,173,,189,6939565295190.799,X
2160,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,1137.4,173,,190,6939565295193.536,X
2161,122891,python_function,<built-in function _has_torch_function>,122891,0.633,190,,191,6939565295196.909,X
2162,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,6.453,190,,192,6939565295198.73,X
2163,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.14,192,,193,6939565295203.578,X
2164,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.096,192,,194,6939565295204.254,X
2165,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.118,192,,195,6939565295204.66,X
2166,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.117,190,,196,6939565295208.898,X
2167,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.227,190,,197,6939565295210.212,X
2168,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.169,190,,198,6939565295211.617,X
2169,122891,python_function,<built-in function isinstance>,122891,0.734,190,,199,6939565295213.206,X
2170,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,608.393,190,,200,6939565295218.86,X
2171,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.194,200,,201,6939565295219.773,X
2172,122891,python_function,<built-in function linear>,122891,254.788,200,,202,6939565295225.612,X
2173,122891,python_function,torch/_tensor.py(1356): unflatten,122891,66.537,200,,203,6939565295483.193,X
2174,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.785,203,,204,6939565295486.931,X
2175,122891,python_function,<built-in function isinstance>,122891,0.965,203,,205,6939565295488.983,X
2176,122891,python_function,<built-in function isinstance>,122891,0.143,203,,206,6939565295491.314,X
2177,122891,python_function,<built-in function isinstance>,122891,0.373,203,,207,6939565295492.455,X
2178,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,47.454,203,,208,6939565295501.856,X
2179,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,23.842,200,,209,6939565295552.829,X
2180,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.472,200,,210,6939565295580.509,X
2181,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,31.097,200,,211,6939565295596.797,X
2182,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,125.973,200,,212,6939565295631.341,X
2183,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,15.232,190,,213,6939565295835.275,X
2184,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.477,190,,214,6939565295851.259,X
2185,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.775,190,,215,6939565295868.379,X
2186,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,7.851,190,,216,6939565295876.665,X
2187,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.609,190,,217,6939565295886.578,X
2188,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,9.596,190,,218,6939565295894.654,X
2189,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.152,190,,219,6939565295906.215,X
2190,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.612,190,,220,6939565295908.738,X
2191,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.708,190,,221,6939565295921.777,X
2192,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,6.736,190,,222,6939565295930.45,X
2193,122891,python_function,<built-in function scaled_dot_product_attention>,122891,199.248,190,,223,6939565295940.704,X
2194,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,28.537,190,,224,6939565296145.451,X
2195,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.581,190,,225,6939565296174.693,X
2196,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.306,190,,226,6939565296176.106,X
2197,122891,python_function,<built-in function linear>,122891,125.099,190,,227,6939565296188.521,X
2198,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.804,190,,228,6939565296315.277,X
2199,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,12.287,190,,229,6939565296317.484,X
2200,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.391,168,,230,6939565296347.334,X
2201,122891,python_function,nn.Module: Dropout_0,122891,166.304,168,0,231,6939565296360.244,X
2202,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,163.243,231,,232,6939565296362.32,X
2203,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.487,232,,233,6939565296364.136,X
2204,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,152.866,232,,234,6939565296372.014,X
2205,122891,python_function,torch/nn/functional.py(1401): dropout,122891,149.324,234,,235,6939565296375.204,X
2206,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.398,235,,236,6939565296376.706,X
2207,122891,python_function,torch/_VF.py(27): __getattr__,122891,3.849,235,,237,6939565296386.165,X
2208,122891,python_function,<built-in function getattr>,122891,0.688,237,,238,6939565296389.174,X
2209,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,119.032,235,,239,6939565296405.077,X
2210,122891,python_function,nn.Module: LayerNorm_0,122891,185.345,160,0,240,6939565296604.897,X
2211,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,181.722,240,,241,6939565296607.842,X
2212,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.221,241,,242,6939565296608.834,X
2213,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,173.575,241,,243,6939565296615.34,X
2214,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.284,243,,244,6939565296619.312,X
2215,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.404,243,,245,6939565296621.695,X
2216,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,165.034,243,,246,6939565296623.52,X
2217,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.398,246,,247,6939565296624.5,X
2218,122891,python_function,torch/backends/__init__.py(38): __get__,122891,4.498,246,,248,6939565296629.187,X
2219,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.559,248,,249,6939565296632.948,X
2220,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,141.986,246,,250,6939565296646.3,X
2221,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.672,160,,251,6939565296795.808,X
2222,122891,python_function,torch/nn/modules/transformer.py(930): _ff_block,122891,670.935,160,,252,6939565296800.35,X
2223,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.488,252,,253,6939565296802.348,X
2224,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.33,252,,254,6939565296805.109,X
2225,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.377,252,,255,6939565296807.97,X
2226,122891,python_function,nn.Module: Linear_0,122891,160.83,252,0,256,6939565296825.015,X
2227,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,158.651,256,,257,6939565296826.551,X
2228,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.121,257,,258,6939565296827.132,X
2229,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,152.075,257,,259,6939565296832.704,X
2230,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.582,259,,260,6939565296835.459,X
2231,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.529,259,,261,6939565296837.227,X
2232,122891,python_function,<built-in function linear>,122891,146.14,259,,262,6939565296838.35,X
2233,122891,python_function,torch/nn/functional.py(1693): relu,122891,93.121,252,,263,6939565296987.942,X
2234,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.438,263,,264,6939565296989.384,X
2235,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,84.597,263,,265,6939565296996.127,X
2236,122891,python_function,nn.Module: Dropout_1,122891,116.644,252,1,266,6939565297091.032,X
2237,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,114.275,266,,267,6939565297092.823,X
2238,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.863,267,,268,6939565297093.521,X
2239,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,108.029,267,,269,6939565297098.666,X
2240,122891,python_function,torch/nn/functional.py(1401): dropout,122891,105.779,269,,270,6939565297100.596,X
2241,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.333,270,,271,6939565297101.171,X
2242,122891,python_function,torch/_VF.py(27): __getattr__,122891,2.643,270,,272,6939565297108.462,X
2243,122891,python_function,<built-in function getattr>,122891,0.704,272,,273,6939565297110.239,X
2244,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,94.577,270,,274,6939565297111.611,X
2245,122891,python_function,nn.Module: Linear_1,122891,145.994,252,1,275,6939565297218.758,X
2246,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,143.88,275,,276,6939565297220.249,X
2247,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.084,276,,277,6939565297220.783,X
2248,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,138.915,276,,278,6939565297224.887,X
2249,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.389,278,,279,6939565297227.329,X
2250,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.496,278,,280,6939565297229.926,X
2251,122891,python_function,<built-in function linear>,122891,132.78,278,,281,6939565297230.754,X
2252,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.326,252,,282,6939565297369.145,X
2253,122891,python_function,nn.Module: Dropout_2,122891,95.62,252,2,283,6939565297375.145,X
2254,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,93.996,283,,284,6939565297376.281,X
2255,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.182,284,,285,6939565297376.765,X
2256,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,88.98,284,,286,6939565297380.988,X
2257,122891,python_function,torch/nn/functional.py(1401): dropout,122891,87.59,286,,287,6939565297382.134,X
2258,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.281,287,,288,6939565297382.435,X
2259,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.008,287,,289,6939565297386.745,X
2260,122891,python_function,<built-in function getattr>,122891,0.181,289,,290,6939565297387.522,X
2261,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,81.396,287,,291,6939565297388.105,X
2262,122891,python_function,nn.Module: LayerNorm_1,122891,163.405,160,1,292,6939565297530.812,X
2263,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,161.427,292,,293,6939565297532.263,X
2264,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.144,293,,294,6939565297532.844,X
2265,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,156.467,293,,295,6939565297536.774,X
2266,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.911,295,,296,6939565297539.962,X
2267,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.496,295,,297,6939565297542.123,X
2268,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,149.244,295,,298,6939565297543.637,X
2269,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.467,298,,299,6939565297544.314,X
2270,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.551,298,,300,6939565297548.762,X
2271,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.717,300,,301,6939565297549.451,X
2272,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,141.667,298,,302,6939565297551.028,X
2273,122891,python_function,nn.Module: TransformerEncoderLayer_1,122891,2006.151,130,1,303,6939565297706.977,X
2274,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,2003.067,303,,304,6939565297708.895,X
2275,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.324,304,,305,6939565297709.466,X
2276,122891,python_function,torch/nn/modules/transformer.py(750): forward,122891,1996.872,304,,306,6939565297714.086,X
2277,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.509,306,,307,6939565297716.197,X
2278,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.479,306,,308,6939565297719.062,X
2279,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.406,306,,309,6939565297720.867,X
2280,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.236,306,,310,6939565297723.317,X
2281,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.29,310,,311,6939565297724.986,X
2282,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.77,306,,312,6939565297726.558,X
2283,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.569,306,,313,6939565297731.376,X
2284,122891,python_function,torch/nn/modules/transformer.py(911): _sa_block,122891,1045.188,306,,314,6939565297735.812,X
2285,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.224,314,,315,6939565297737.166,X
2286,122891,python_function,nn.Module: MultiheadAttention_1,122891,881.642,314,1,316,6939565297749.743,X
2287,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,877.556,316,,317,6939565297752.241,X
2288,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.669,317,,318,6939565297752.632,X
2289,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,871.365,317,,319,6939565297756.979,X
2290,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.239,319,,320,6939565297758.083,X
2291,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.308,319,,321,6939565297759.56,X
2292,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.245,319,,322,6939565297760.829,X
2293,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.338,319,,323,6939565297762.265,X
2294,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,0.916,319,,324,6939565297763.516,X
2295,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.137,324,,325,6939565297764.103,X
2296,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.791,319,,326,6939565297766.429,X
2297,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.58,319,,327,6939565297768.966,X
2298,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.619,319,,328,6939565297771.707,X
2299,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.718,319,,329,6939565297774.081,X
2300,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.549,319,,330,6939565297779.676,X
2301,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.592,319,,331,6939565297782.14,X
2302,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.066,319,,332,6939565297785.369,X
2303,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.43,319,,333,6939565297789.923,X
2304,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.239,319,,334,6939565297793.18,X
2305,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.567,319,,335,6939565297796.213,X
2306,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,825.905,319,,336,6939565297799.18,X
2307,122891,python_function,<built-in function _has_torch_function>,122891,0.978,336,,337,6939565297800.632,X
2308,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.845,336,,338,6939565297802.664,X
2309,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.271,338,,339,6939565297803.225,X
2310,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.232,338,,340,6939565297804.029,X
2311,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.353,338,,341,6939565297804.663,X
2312,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.139,336,,342,6939565297809.849,X
2313,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.197,336,,343,6939565297811.12,X
2314,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.188,336,,344,6939565297813.049,X
2315,122891,python_function,<built-in function isinstance>,122891,0.88,336,,345,6939565297814.811,X
2316,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,372.661,336,,346,6939565297820.31,X
2317,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,2.143,346,,347,6939565297821.123,X
2318,122891,python_function,<built-in function linear>,122891,142.382,346,,348,6939565297824.059,X
2319,122891,python_function,torch/_tensor.py(1356): unflatten,122891,29.903,346,,349,6939565297968.466,X
2320,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.492,349,,350,6939565297969.49,X
2321,122891,python_function,<built-in function isinstance>,122891,0.356,349,,351,6939565297970.994,X
2322,122891,python_function,<built-in function isinstance>,122891,0.09,349,,352,6939565297972.217,X
2323,122891,python_function,<built-in function isinstance>,122891,0.407,349,,353,6939565297973.079,X
2324,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,22.166,349,,354,6939565297975.856,X
2325,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,16.902,346,,355,6939565297999.429,X
2326,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.141,346,,356,6939565298018.842,X
2327,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,15.824,346,,357,6939565298034.313,X
2328,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,84.846,346,,358,6939565298051.494,X
2329,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.898,336,,359,6939565298196.244,X
2330,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.049,336,,360,6939565298208.869,X
2331,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.23,336,,361,6939565298224.381,X
2332,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.568,336,,362,6939565298236.059,X
2333,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.976,336,,363,6939565298248.945,X
2334,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.937,336,,364,6939565298257.569,X
2335,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.063,336,,365,6939565298270.231,X
2336,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.213,336,,366,6939565298272.258,X
2337,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.581,336,,367,6939565298281.695,X
2338,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.113,336,,368,6939565298292.313,X
2339,122891,python_function,<built-in function scaled_dot_product_attention>,122891,166.077,336,,369,6939565298301.78,X
2340,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,17.855,336,,370,6939565298468.852,X
2341,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.613,336,,371,6939565298487.237,X
2342,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.349,336,,372,6939565298488.506,X
2343,122891,python_function,<built-in function linear>,122891,107.256,336,,373,6939565298501.646,X
2344,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.331,336,,374,6939565298610.225,X
2345,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,12.224,336,,375,6939565298611.897,X
2346,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.794,314,,376,6939565298639.187,X
2347,122891,python_function,nn.Module: Dropout_3,122891,124.482,314,3,377,6939565298655.843,X
2348,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,121.474,377,,378,6939565298658.101,X
2349,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.194,378,,379,6939565298659.547,X
2350,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,112.439,378,,380,6939565298666.687,X
2351,122891,python_function,torch/nn/functional.py(1401): dropout,122891,109.802,380,,381,6939565298668.972,X
2352,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.381,381,,382,6939565298669.554,X
2353,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.985,381,,383,6939565298677.018,X
2354,122891,python_function,<built-in function getattr>,122891,0.777,383,,384,6939565298678.114,X
2355,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,98.914,381,,385,6939565298679.549,X
2356,122891,python_function,nn.Module: LayerNorm_2,122891,120.319,306,2,386,6939565298846.217,X
2357,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,118.25,386,,387,6939565298847.811,X
2358,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.718,387,,388,6939565298848.378,X
2359,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,112.839,387,,389,6939565298852.861,X
2360,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.046,389,,390,6939565298856.151,X
2361,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.559,389,,391,6939565298858.413,X
2362,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,104.986,389,,392,6939565298860.374,X
2363,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.46,392,,393,6939565298860.908,X
2364,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.372,392,,394,6939565298864.945,X
2365,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.642,394,,395,6939565298865.558,X
2366,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,98.172,392,,396,6939565298866.981,X
2367,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.234,306,,397,6939565298970.392,X
2368,122891,python_function,torch/nn/modules/transformer.py(930): _ff_block,122891,563.92,306,,398,6939565298974.102,X
2369,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.426,398,,399,6939565298975.748,X
2370,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.323,398,,400,6939565298978.344,X
2371,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.243,398,,401,6939565298981.293,X
2372,122891,python_function,nn.Module: Linear_2,122891,138.768,398,2,402,6939565298993.049,X
2373,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,136.814,402,,403,6939565298994.464,X
2374,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.036,403,,404,6939565298994.851,X
2375,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,132.475,403,,405,6939565298998.436,X
2376,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.578,405,,406,6939565299000.303,X
2377,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.489,405,,407,6939565299002.004,X
2378,122891,python_function,<built-in function linear>,122891,127.898,405,,408,6939565299002.74,X
2379,122891,python_function,torch/nn/functional.py(1693): relu,122891,56.828,398,,409,6939565299133.487,X
2380,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.365,409,,410,6939565299134.158,X
2381,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,54.122,409,,411,6939565299135.843,X
2382,122891,python_function,nn.Module: Dropout_4,122891,102.959,398,4,412,6939565299197.553,X
2383,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,101.025,412,,413,6939565299198.98,X
2384,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.31,413,,414,6939565299199.495,X
2385,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,96.152,413,,415,6939565299203.476,X
2386,122891,python_function,torch/nn/functional.py(1401): dropout,122891,94.294,415,,416,6939565299205.014,X
2387,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.326,416,,417,6939565299205.361,X
2388,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.765,416,,418,6939565299211.301,X
2389,122891,python_function,<built-in function getattr>,122891,0.481,418,,419,6939565299212.47,X
2390,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,85.45,416,,420,6939565299213.515,X
2391,122891,python_function,nn.Module: Linear_3,122891,125.346,398,3,421,6939565299311.99,X
2392,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,123.555,421,,422,6939565299313.296,X
2393,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.167,422,,423,6939565299313.794,X
2394,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,119.265,422,,424,6939565299317.244,X
2395,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.314,424,,425,6939565299319.11,X
2396,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.485,424,,426,6939565299321.585,X
2397,122891,python_function,<built-in function linear>,122891,114.016,424,,427,6939565299322.269,X
2398,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.09,398,,428,6939565299440.458,X
2399,122891,python_function,nn.Module: Dropout_5,122891,90.888,398,5,429,6939565299446.605,X
2400,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,89.057,429,,430,6939565299447.961,X
2401,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.131,430,,431,6939565299448.452,X
2402,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,85.29,430,,432,6939565299451.397,X
2403,122891,python_function,torch/nn/functional.py(1401): dropout,122891,84.169,432,,433,6939565299452.269,X
2404,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.255,433,,434,6939565299452.508,X
2405,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.053,433,,435,6939565299456.367,X
2406,122891,python_function,<built-in function getattr>,122891,0.211,435,,436,6939565299457.108,X
2407,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,78.519,433,,437,6939565299457.755,X
2408,122891,python_function,nn.Module: LayerNorm_3,122891,115.304,306,3,438,6939565299594.255,X
2409,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,113.198,438,,439,6939565299595.901,X
2410,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.124,439,,440,6939565299596.307,X
2411,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,108.869,439,,441,6939565299599.912,X
2412,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.807,441,,442,6939565299602.553,X
2413,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.547,441,,443,6939565299604.558,X
2414,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,102.673,441,,444,6939565299605.837,X
2415,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.455,444,,445,6939565299606.203,X
2416,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.104,444,,446,6939565299609.427,X
2417,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.405,446,,447,6939565299609.981,X
2418,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,97.4,444,,448,6939565299610.939,X
2419,122891,python_function,nn.Module: TransformerEncoderLayer_2,122891,1931.0,130,2,449,6939565299721.052,X
2420,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,1927.539,449,,450,6939565299723.231,X
2421,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.001,450,,451,6939565299723.693,X
2422,122891,python_function,torch/nn/modules/transformer.py(750): forward,122891,1921.943,450,,452,6939565299727.754,X
2423,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.522,452,,453,6939565299729.507,X
2424,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.466,452,,454,6939565299732.259,X
2425,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.255,452,,455,6939565299733.962,X
2426,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.454,452,,456,6939565299736.033,X
2427,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.392,456,,457,6939565299737.713,X
2428,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.672,452,,458,6939565299739.282,X
2429,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.062,452,,459,6939565299743.361,X
2430,122891,python_function,torch/nn/modules/transformer.py(911): _sa_block,122891,986.764,452,,460,6939565299747.028,X
2431,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.613,460,,461,6939565299748.244,X
2432,122891,python_function,nn.Module: MultiheadAttention_2,122891,844.865,460,2,462,6939565299759.976,X
2433,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,840.589,462,,463,6939565299762.544,X
2434,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.622,463,,464,6939565299762.992,X
2435,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,834.431,463,,465,6939565299767.163,X
2436,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.293,465,,466,6939565299767.901,X
2437,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.222,465,,467,6939565299769.419,X
2438,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.302,465,,468,6939565299770.526,X
2439,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.224,465,,469,6939565299771.922,X
2440,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,0.874,465,,470,6939565299773.086,X
2441,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.114,470,,471,6939565299773.663,X
2442,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.71,465,,472,6939565299776.17,X
2443,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.745,465,,473,6939565299778.611,X
2444,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.66,465,,474,6939565299781.557,X
2445,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.571,465,,475,6939565299783.886,X
2446,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.517,465,,476,6939565299789.091,X
2447,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.712,465,,477,6939565299791.378,X
2448,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.897,465,,478,6939565299794.809,X
2449,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.931,465,,479,6939565299799.816,X
2450,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.163,465,,480,6939565299802.652,X
2451,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.013,465,,481,6939565299805.649,X
2452,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,789.122,465,,482,6939565299809.0,X
2453,122891,python_function,<built-in function _has_torch_function>,122891,0.831,482,,483,6939565299810.14,X
2454,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.779,482,,484,6939565299812.05,X
2455,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.445,484,,485,6939565299812.52,X
2456,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.261,484,,486,6939565299813.523,X
2457,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.186,484,,487,6939565299814.163,X
2458,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.193,482,,488,6939565299819.058,X
2459,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.401,482,,489,6939565299820.333,X
2460,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.19,482,,490,6939565299822.297,X
2461,122891,python_function,<built-in function isinstance>,122891,0.688,482,,491,6939565299823.893,X
2462,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,345.643,482,,492,6939565299828.623,X
2463,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,2.152,492,,493,6939565299829.351,X
2464,122891,python_function,<built-in function linear>,122891,127.309,492,,494,6939565299832.058,X
2465,122891,python_function,torch/_tensor.py(1356): unflatten,122891,27.773,492,,495,6939565299960.839,X
2466,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.349,495,,496,6939565299961.58,X
2467,122891,python_function,<built-in function isinstance>,122891,0.344,495,,497,6939565299962.753,X
2468,122891,python_function,<built-in function isinstance>,122891,0.113,495,,498,6939565299963.953,X
2469,122891,python_function,<built-in function isinstance>,122891,0.219,495,,499,6939565299964.809,X
2470,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,21.033,495,,500,6939565299967.258,X
2471,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,17.611,492,,501,6939565299989.334,X
2472,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.241,492,,502,6939565300009.299,X
2473,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,16.231,492,,503,6939565300023.923,X
2474,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,75.968,492,,504,6939565300041.441,X
2475,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.773,482,,505,6939565300177.565,X
2476,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.762,482,,506,6939565300189.852,X
2477,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.646,482,,507,6939565300206.138,X
2478,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.629,482,,508,6939565300214.269,X
2479,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.268,482,,509,6939565300229.216,X
2480,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.667,482,,510,6939565300239.002,X
2481,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.035,482,,511,6939565300252.395,X
2482,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.538,482,,512,6939565300254.33,X
2483,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.158,482,,513,6939565300263.988,X
2484,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.109,482,,514,6939565300274.193,X
2485,122891,python_function,<built-in function scaled_dot_product_attention>,122891,167.725,482,,515,6939565300284.601,X
2486,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,16.724,482,,516,6939565300453.311,X
2487,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.744,482,,517,6939565300470.507,X
2488,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.078,482,,518,6939565300471.793,X
2489,122891,python_function,<built-in function linear>,122891,98.956,482,,519,6939565300483.855,X
2490,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.233,482,,520,6939565300584.053,X
2491,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.604,482,,521,6939565300585.629,X
2492,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.463,460,,522,6939565300612.038,X
2493,122891,python_function,nn.Module: Dropout_6,122891,111.031,460,6,523,6939565300622.027,X
2494,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,108.367,523,,524,6939565300624.133,X
2495,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.31,524,,525,6939565300625.269,X
2496,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,100.488,524,,526,6939565300631.607,X
2497,122891,python_function,torch/nn/functional.py(1401): dropout,122891,98.213,526,,527,6939565300633.572,X
2498,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.277,527,,528,6939565300634.096,X
2499,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.941,527,,529,6939565300640.701,X
2500,122891,python_function,<built-in function getattr>,122891,0.687,529,,530,6939565300641.834,X
2501,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,88.502,527,,531,6939565300643.053,X
2502,122891,python_function,nn.Module: LayerNorm_4,122891,118.264,452,4,532,6939565300795.966,X
2503,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,116.312,532,,533,6939565300797.457,X
2504,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.704,533,,534,6939565300798.171,X
2505,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,111.064,533,,535,6939565300802.286,X
2506,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.018,535,,536,6939565300805.232,X
2507,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.679,535,,537,6939565300807.575,X
2508,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,103.768,535,,538,6939565300809.275,X
2509,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.459,538,,539,6939565300809.725,X
2510,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.53,538,,540,6939565300813.499,X
2511,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.604,540,,541,6939565300814.306,X
2512,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,97.398,538,,542,6939565300815.463,X
2513,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.027,452,,543,6939565300917.597,X
2514,122891,python_function,torch/nn/modules/transformer.py(930): _ff_block,122891,547.638,452,,544,6939565300921.025,X
2515,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.543,544,,545,6939565300922.558,X
2516,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.21,544,,546,6939565300925.294,X
2517,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.218,544,,547,6939565300928.002,X
2518,122891,python_function,nn.Module: Linear_4,122891,123.517,544,4,548,6939565300938.6,X
2519,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,121.684,548,,549,6939565300939.928,X
2520,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.084,549,,550,6939565300940.581,X
2521,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,116.569,549,,551,6939565300944.455,X
2522,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.627,551,,552,6939565300946.471,X
2523,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.719,551,,553,6939565300948.173,X
2524,122891,python_function,<built-in function linear>,122891,111.726,551,,554,6939565300949.096,X
2525,122891,python_function,torch/nn/functional.py(1693): relu,122891,54.362,544,,555,6939565301063.474,X
2526,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.348,555,,556,6939565301063.959,X
2527,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,51.994,555,,557,6939565301065.575,X
2528,122891,python_function,nn.Module: Dropout_7,122891,103.087,544,7,558,6939565301125.088,X
2529,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,101.073,558,,559,6939565301126.618,X
2530,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.503,559,,560,6939565301127.111,X
2531,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,95.658,559,,561,6939565301131.715,X
2532,122891,python_function,torch/nn/functional.py(1401): dropout,122891,94.169,561,,562,6939565301132.92,X
2533,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.263,562,,563,6939565301133.148,X
2534,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.647,562,,564,6939565301138.466,X
2535,122891,python_function,<built-in function getattr>,122891,0.593,564,,565,6939565301139.451,X
2536,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,86.432,562,,566,6939565301140.485,X
2537,122891,python_function,nn.Module: Linear_5,122891,126.621,544,5,567,6939565301238.895,X
2538,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,124.697,567,,568,6939565301240.342,X
2539,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.067,568,,569,6939565301240.967,X
2540,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,120.115,568,,570,6939565301244.485,X
2541,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.875,570,,571,6939565301246.57,X
2542,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.528,570,,572,6939565301248.596,X
2543,122891,python_function,<built-in function linear>,122891,115.078,570,,573,6939565301249.321,X
2544,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,4.977,544,,574,6939565301368.616,X
2545,122891,python_function,nn.Module: Dropout_8,122891,91.181,544,8,575,6939565301377.026,X
2546,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,89.716,575,,576,6939565301378.039,X
2547,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.031,576,,577,6939565301378.55,X
2548,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,85.871,576,,578,6939565301381.572,X
2549,122891,python_function,torch/nn/functional.py(1401): dropout,122891,84.828,578,,579,6939565301382.361,X
2550,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.242,579,,580,6939565301382.592,X
2551,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.107,579,,581,6939565301386.691,X
2552,122891,python_function,<built-in function getattr>,122891,0.209,581,,582,6939565301387.361,X
2553,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,78.85,579,,583,6939565301388.162,X
2554,122891,python_function,nn.Module: LayerNorm_5,122891,128.592,452,5,584,6939565301519.695,X
2555,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,126.733,584,,585,6939565301521.075,X
2556,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.072,585,,586,6939565301521.769,X
2557,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,122.029,585,,587,6939565301525.445,X
2558,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.812,587,,588,6939565301528.052,X
2559,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.479,587,,589,6939565301530.111,X
2560,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,115.92,587,,590,6939565301531.27,X
2561,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.467,590,,591,6939565301531.672,X
2562,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.192,590,,592,6939565301534.997,X
2563,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.567,592,,593,6939565301535.511,X
2564,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,110.25,590,,594,6939565301536.709,X
2565,122891,python_function,nn.Module: TransformerEncoderLayer_3,122891,1923.962,130,3,595,6939565301673.352,X
2566,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,1920.709,595,,596,6939565301675.329,X
2567,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.109,596,,597,6939565301675.896,X
2568,122891,python_function,torch/nn/modules/transformer.py(750): forward,122891,1914.418,596,,598,6939565301680.825,X
2569,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.575,598,,599,6939565301682.52,X
2570,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.472,598,,600,6939565301685.043,X
2571,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.271,598,,601,6939565301686.629,X
2572,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.365,598,,602,6939565301688.558,X
2573,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.282,602,,603,6939565301690.26,X
2574,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.649,598,,604,6939565301691.704,X
2575,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.218,598,,605,6939565301695.912,X
2576,122891,python_function,torch/nn/modules/transformer.py(911): _sa_block,122891,992.094,598,,606,6939565301699.58,X
2577,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.445,606,,607,6939565301701.101,X
2578,122891,python_function,nn.Module: MultiheadAttention_3,122891,845.77,606,3,608,6939565301713.483,X
2579,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,842.236,608,,609,6939565301715.456,X
2580,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.644,609,,610,6939565301715.96,X
2581,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,837.079,609,,611,6939565301719.419,X
2582,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.302,611,,612,6939565301720.498,X
2583,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.193,611,,613,6939565301722.053,X
2584,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.281,611,,614,6939565301723.052,X
2585,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.219,611,,615,6939565301724.45,X
2586,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,0.947,611,,616,6939565301725.594,X
2587,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.127,616,,617,6939565301726.212,X
2588,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.625,611,,618,6939565301728.496,X
2589,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.532,611,,619,6939565301730.829,X
2590,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.641,611,,620,6939565301733.423,X
2591,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.568,611,,621,6939565301735.777,X
2592,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.485,611,,622,6939565301740.922,X
2593,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.557,611,,623,6939565301743.22,X
2594,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.179,611,,624,6939565301746.161,X
2595,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.972,611,,625,6939565301751.072,X
2596,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.302,611,,626,6939565301753.8,X
2597,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.588,611,,627,6939565301756.87,X
2598,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,793.557,611,,628,6939565301759.562,X
2599,122891,python_function,<built-in function _has_torch_function>,122891,0.837,628,,629,6939565301760.536,X
2600,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.471,628,,630,6939565301762.393,X
2601,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.282,630,,631,6939565301762.773,X
2602,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.228,630,,632,6939565301763.587,X
2603,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.24,630,,633,6939565301764.183,X
2604,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.136,628,,634,6939565301769.183,X
2605,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.194,628,,635,6939565301770.504,X
2606,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.223,628,,636,6939565301772.094,X
2607,122891,python_function,<built-in function isinstance>,122891,0.704,628,,637,6939565301773.665,X
2608,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,350.12,628,,638,6939565301778.462,X
2609,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,2.112,638,,639,6939565301779.025,X
2610,122891,python_function,<built-in function linear>,122891,134.698,638,,640,6939565301781.721,X
2611,122891,python_function,torch/_tensor.py(1356): unflatten,122891,27.471,638,,641,6939565301917.983,X
2612,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.351,641,,642,6939565301918.667,X
2613,122891,python_function,<built-in function isinstance>,122891,0.321,641,,643,6939565301919.961,X
2614,122891,python_function,<built-in function isinstance>,122891,0.065,641,,644,6939565301921.037,X
2615,122891,python_function,<built-in function isinstance>,122891,0.229,641,,645,6939565301921.717,X
2616,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,21.192,641,,646,6939565301923.969,X
2617,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,15.568,638,,647,6939565301946.33,X
2618,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.099,638,,648,6939565301964.284,X
2619,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,14.407,638,,649,6939565301979.697,X
2620,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,76.883,638,,650,6939565301995.434,X
2621,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.311,628,,651,6939565302131.533,X
2622,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,17.368,628,,652,6939565302142.364,X
2623,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.069,628,,653,6939565302163.48,X
2624,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.899,628,,654,6939565302172.954,X
2625,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.823,628,,655,6939565302186.104,X
2626,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.248,628,,656,6939565302195.336,X
2627,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.063,628,,657,6939565302207.325,X
2628,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.747,628,,658,6939565302209.364,X
2629,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.108,628,,659,6939565302219.288,X
2630,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.229,628,,660,6939565302231.474,X
2631,122891,python_function,<built-in function scaled_dot_product_attention>,122891,164.942,628,,661,6939565302242.0,X
2632,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,16.489,628,,662,6939565302408.025,X
2633,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.578,628,,663,6939565302425.03,X
2634,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.757,628,,664,6939565302426.153,X
2635,122891,python_function,<built-in function linear>,122891,101.601,628,,665,6939565302436.864,X
2636,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.307,628,,666,6939565302539.694,X
2637,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.86,628,,667,6939565302541.371,X
2638,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.432,606,,668,6939565302566.068,X
2639,122891,python_function,nn.Module: Dropout_9,122891,114.619,606,9,669,6939565302576.468,X
2640,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,112.02,669,,670,6939565302578.5,X
2641,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.761,670,,671,6939565302579.746,X
2642,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,103.572,670,,672,6939565302586.579,X
2643,122891,python_function,torch/nn/functional.py(1401): dropout,122891,101.378,672,,673,6939565302588.317,X
2644,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.309,673,,674,6939565302588.821,X
2645,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.966,673,,675,6939565302595.723,X
2646,122891,python_function,<built-in function getattr>,122891,0.685,675,,676,6939565302596.812,X
2647,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,91.397,673,,677,6939565302598.069,X
2648,122891,python_function,nn.Module: LayerNorm_6,122891,114.39,598,6,678,6939565302753.593,X
2649,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,112.316,678,,679,6939565302755.175,X
2650,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.563,679,,680,6939565302755.725,X
2651,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,106.955,679,,681,6939565302760.186,X
2652,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.9,681,,682,6939565302762.887,X
2653,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.531,681,,683,6939565302765.043,X
2654,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,100.292,681,,684,6939565302766.535,X
2655,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.484,684,,685,6939565302766.944,X
2656,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.161,684,,686,6939565302770.871,X
2657,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.56,686,,687,6939565302771.381,X
2658,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,94.073,684,,688,6939565302772.567,X
2659,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.98,598,,689,6939565302871.413,X
2660,122891,python_function,torch/nn/modules/transformer.py(930): _ff_block,122891,547.531,598,,690,6939565302874.816,X
2661,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.401,690,,691,6939565302876.216,X
2662,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.326,690,,692,6939565302878.855,X
2663,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.281,690,,693,6939565302881.575,X
2664,122891,python_function,nn.Module: Linear_6,122891,121.989,690,6,694,6939565302891.477,X
2665,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,120.233,694,,695,6939565302892.806,X
2666,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.918,695,,696,6939565302893.261,X
2667,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,115.976,695,,697,6939565302896.659,X
2668,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.631,697,,698,6939565302898.643,X
2669,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.471,697,,699,6939565302900.42,X
2670,122891,python_function,<built-in function linear>,122891,111.35,697,,700,6939565302901.09,X
2671,122891,python_function,torch/nn/functional.py(1693): relu,122891,53.877,690,,701,6939565303014.788,X
2672,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.365,701,,702,6939565303015.372,X
2673,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,51.585,701,,703,6939565303016.839,X
2674,122891,python_function,nn.Module: Dropout_10,122891,101.945,690,10,704,6939565303075.561,X
2675,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,100.208,704,,705,6939565303076.814,X
2676,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.283,705,,706,6939565303077.349,X
2677,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,95.63,705,,707,6939565303081.076,X
2678,122891,python_function,torch/nn/functional.py(1401): dropout,122891,94.145,707,,708,6939565303082.243,X
2679,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.198,708,,709,6939565303082.481,X
2680,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.492,708,,710,6939565303087.767,X
2681,122891,python_function,<built-in function getattr>,122891,0.452,710,,711,6939565303088.689,X
2682,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,86.493,708,,712,6939565303089.704,X
2683,122891,python_function,nn.Module: Linear_7,122891,127.893,690,7,713,6939565303188.55,X
2684,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,126.124,713,,714,6939565303189.79,X
2685,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.065,714,,715,6939565303190.224,X
2686,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,122.213,714,,716,6939565303193.355,X
2687,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.075,716,,717,6939565303195.342,X
2688,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.454,716,,718,6939565303197.623,X
2689,122891,python_function,<built-in function linear>,122891,117.036,716,,719,6939565303198.289,X
2690,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.061,690,,720,6939565303319.887,X
2691,122891,python_function,nn.Module: Dropout_11,122891,96.078,690,11,721,6939565303325.784,X
2692,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,94.539,721,,722,6939565303326.814,X
2693,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.142,722,,723,6939565303327.45,X
2694,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,90.204,722,,724,6939565303330.794,X
2695,122891,python_function,torch/nn/functional.py(1401): dropout,122891,89.036,724,,725,6939565303331.718,X
2696,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.324,725,,726,6939565303331.962,X
2697,122891,python_function,torch/_VF.py(27): __getattr__,122891,0.994,725,,727,6939565303336.154,X
2698,122891,python_function,<built-in function getattr>,122891,0.26,727,,728,6939565303336.811,X
2699,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,83.062,725,,729,6939565303337.465,X
2700,122891,python_function,nn.Module: LayerNorm_7,122891,117.299,598,7,730,6939565303476.522,X
2701,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,115.432,730,,731,6939565303477.912,X
2702,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.01,731,,732,6939565303478.395,X
2703,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,111.414,731,,733,6939565303481.597,X
2704,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.952,733,,734,6939565303484.148,X
2705,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.519,733,,735,6939565303486.354,X
2706,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,105.181,733,,736,6939565303487.582,X
2707,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.434,736,,737,6939565303487.985,X
2708,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.325,736,,738,6939565303491.056,X
2709,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.355,738,,739,6939565303491.872,X
2710,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,99.535,736,,740,6939565303493.042,X
2711,122891,python_function,nn.Module: TransformerEncoderLayer_4,122891,2011.513,130,4,741,6939565303604.593,X
2712,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,2008.629,741,,742,6939565303606.493,X
2713,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.966,742,,743,6939565303606.932,X
2714,122891,python_function,torch/nn/modules/transformer.py(750): forward,122891,2003.757,742,,744,6939565303610.559,X
2715,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.483,744,,745,6939565303612.183,X
2716,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.366,744,,746,6939565303614.467,X
2717,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.222,744,,747,6939565303615.922,X
2718,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.248,744,,748,6939565303617.539,X
2719,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.286,748,,749,6939565303619.226,X
2720,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.556,744,,750,6939565303620.846,X
2721,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.585,744,,751,6939565303624.936,X
2722,122891,python_function,torch/nn/modules/transformer.py(911): _sa_block,122891,1078.646,744,,752,6939565303629.156,X
2723,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.449,752,,753,6939565303630.622,X
2724,122891,python_function,nn.Module: MultiheadAttention_4,122891,916.662,752,4,754,6939565303642.415,X
2725,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,913.405,754,,755,6939565303644.218,X
2726,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.677,755,,756,6939565303644.632,X
2727,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,908.074,755,,757,6939565303648.251,X
2728,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.302,757,,758,6939565303649.286,X
2729,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.19,757,,759,6939565303650.68,X
2730,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.267,757,,760,6939565303651.857,X
2731,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.337,757,,761,6939565303653.135,X
2732,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,0.851,757,,762,6939565303654.369,X
2733,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.131,762,,763,6939565303654.913,X
2734,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.657,757,,764,6939565303657.237,X
2735,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.557,757,,765,6939565303659.564,X
2736,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.634,757,,766,6939565303662.172,X
2737,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.619,757,,767,6939565303664.425,X
2738,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.528,757,,768,6939565303669.816,X
2739,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.695,757,,769,6939565303672.115,X
2740,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.952,757,,770,6939565303675.181,X
2741,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.335,757,,771,6939565303679.645,X
2742,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.243,757,,772,6939565303682.764,X
2743,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.704,757,,773,6939565303685.755,X
2744,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,864.686,757,,774,6939565303688.453,X
2745,122891,python_function,<built-in function _has_torch_function>,122891,0.741,774,,775,6939565303689.336,X
2746,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.429,774,,776,6939565303691.074,X
2747,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.283,776,,777,6939565303691.472,X
2748,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.212,776,,778,6939565303692.249,X
2749,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.255,776,,779,6939565303692.819,X
2750,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.152,774,,780,6939565303697.423,X
2751,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.205,774,,781,6939565303698.733,X
2752,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.193,774,,782,6939565303700.297,X
2753,122891,python_function,<built-in function isinstance>,122891,0.613,774,,783,6939565303701.929,X
2754,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,430.807,774,,784,6939565303706.412,X
2755,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.96,784,,785,6939565303706.972,X
2756,122891,python_function,<built-in function linear>,122891,126.768,784,,786,6939565303709.547,X
2757,122891,python_function,torch/_tensor.py(1356): unflatten,122891,27.405,784,,787,6939565303837.571,X
2758,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.387,787,,788,6939565303838.259,X
2759,122891,python_function,<built-in function isinstance>,122891,0.456,787,,789,6939565303839.535,X
2760,122891,python_function,<built-in function isinstance>,122891,0.064,787,,790,6939565303840.855,X
2761,122891,python_function,<built-in function isinstance>,122891,0.255,787,,791,6939565303841.539,X
2762,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,20.468,787,,792,6939565303844.222,X
2763,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,18.169,784,,793,6939565303865.967,X
2764,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.275,784,,794,6939565303886.383,X
2765,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,15.207,784,,795,6939565303902.028,X
2766,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,160.831,784,,796,6939565303918.467,X
2767,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,12.465,774,,797,6939565304140.427,X
2768,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.273,774,,798,6939565304153.491,X
2769,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.336,774,,799,6939565304170.501,X
2770,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.095,774,,800,6939565304179.28,X
2771,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.044,774,,801,6939565304191.802,X
2772,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.555,774,,802,6939565304200.294,X
2773,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.009,774,,803,6939565304213.624,X
2774,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.949,774,,804,6939565304215.51,X
2775,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.864,774,,805,6939565304224.588,X
2776,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.197,774,,806,6939565304233.5,X
2777,122891,python_function,<built-in function scaled_dot_product_attention>,122891,159.108,774,,807,6939565304244.033,X
2778,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,17.28,774,,808,6939565304404.022,X
2779,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.612,774,,809,6939565304421.907,X
2780,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.079,774,,810,6939565304423.157,X
2781,122891,python_function,<built-in function linear>,122891,104.663,774,,811,6939565304433.993,X
2782,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.282,774,,812,6939565304539.977,X
2783,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.535,774,,813,6939565304541.666,X
2784,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.636,752,,814,6939565304567.176,X
2785,122891,python_function,nn.Module: Dropout_12,122891,129.304,752,12,815,6939565304577.862,X
2786,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,126.657,815,,816,6939565304579.984,X
2787,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.847,816,,817,6939565304581.457,X
2788,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,118.198,816,,818,6939565304587.944,X
2789,122891,python_function,torch/nn/functional.py(1401): dropout,122891,116.085,818,,819,6939565304589.737,X
2790,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.405,819,,820,6939565304590.31,X
2791,122891,python_function,torch/_VF.py(27): __getattr__,122891,2.039,819,,821,6939565304597.244,X
2792,122891,python_function,<built-in function getattr>,122891,0.753,821,,822,6939565304598.4,X
2793,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,105.873,819,,823,6939565304599.69,X
2794,122891,python_function,nn.Module: LayerNorm_8,122891,118.556,744,8,824,6939565304772.815,X
2795,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,116.478,824,,825,6939565304774.409,X
2796,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.622,825,,826,6939565304775.232,X
2797,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,110.794,825,,827,6939565304779.746,X
2798,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.936,827,,828,6939565304782.733,X
2799,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.545,827,,829,6939565304785.126,X
2800,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,103.348,827,,830,6939565304786.945,X
2801,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.433,830,,831,6939565304787.455,X
2802,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.524,830,,832,6939565304791.467,X
2803,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.72,832,,833,6939565304791.994,X
2804,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,96.614,830,,834,6939565304793.499,X
2805,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.086,744,,835,6939565304894.929,X
2806,122891,python_function,torch/nn/modules/transformer.py(930): _ff_block,122891,536.021,744,,836,6939565304898.549,X
2807,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.735,836,,837,6939565304900.027,X
2808,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.112,836,,838,6939565304903.211,X
2809,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.373,836,,839,6939565304905.852,X
2810,122891,python_function,nn.Module: Linear_8,122891,123.395,836,8,840,6939565304916.984,X
2811,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,121.329,840,,841,6939565304918.567,X
2812,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.092,841,,842,6939565304919.019,X
2813,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,117.259,841,,843,6939565304922.281,X
2814,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.58,843,,844,6939565304924.261,X
2815,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.512,843,,845,6939565304925.932,X
2816,122891,python_function,<built-in function linear>,122891,112.6,843,,846,6939565304926.644,X
2817,122891,python_function,torch/nn/functional.py(1693): relu,122891,54.527,836,,847,6939565305041.647,X
2818,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.395,847,,848,6939565305042.179,X
2819,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,52.291,847,,849,6939565305043.639,X
2820,122891,python_function,nn.Module: Dropout_13,122891,102.132,836,13,850,6939565305103.366,X
2821,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,100.254,850,,851,6939565305104.727,X
2822,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.229,851,,852,6939565305105.19,X
2823,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,95.576,851,,853,6939565305109.089,X
2824,122891,python_function,torch/nn/functional.py(1401): dropout,122891,93.899,853,,854,6939565305110.48,X
2825,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.216,854,,855,6939565305110.743,X
2826,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.97,854,,856,6939565305116.301,X
2827,122891,python_function,<built-in function getattr>,122891,0.569,856,,857,6939565305117.402,X
2828,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,85.474,854,,858,6939565305118.693,X
2829,122891,python_function,nn.Module: Linear_9,122891,127.49,836,9,859,6939565305215.767,X
2830,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,125.502,859,,860,6939565305217.259,X
2831,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.346,860,,861,6939565305217.762,X
2832,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,120.685,860,,862,6939565305221.647,X
2833,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.252,862,,863,6939565305223.981,X
2834,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.53,862,,864,6939565305226.53,X
2835,122891,python_function,<built-in function linear>,122891,114.796,862,,865,6939565305227.3,X
2836,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.801,836,,866,6939565305347.133,X
2837,122891,python_function,nn.Module: Dropout_14,122891,81.25,836,14,867,6939565305352.347,X
2838,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,79.561,867,,868,6939565305353.429,X
2839,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.06,868,,869,6939565305353.916,X
2840,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,75.712,868,,870,6939565305356.991,X
2841,122891,python_function,torch/nn/functional.py(1401): dropout,122891,74.498,870,,871,6939565305357.971,X
2842,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.365,871,,872,6939565305358.175,X
2843,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.125,871,,873,6939565305362.026,X
2844,122891,python_function,<built-in function getattr>,122891,0.434,873,,874,6939565305362.618,X
2845,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,68.868,871,,875,6939565305363.395,X
2846,122891,python_function,nn.Module: LayerNorm_9,122891,127.681,744,9,876,6939565305485.168,X
2847,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,125.956,876,,877,6939565305486.43,X
2848,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.112,877,,878,6939565305487.089,X
2849,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,120.935,877,,879,6939565305491.085,X
2850,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.792,879,,880,6939565305493.367,X
2851,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.35,879,,881,6939565305495.075,X
2852,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,115.478,879,,882,6939565305496.265,X
2853,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.307,882,,883,6939565305496.645,X
2854,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.484,882,,884,6939565305500.152,X
2855,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.719,884,,885,6939565305500.822,X
2856,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,109.519,882,,886,6939565305502.062,X
2857,122891,python_function,nn.Module: TransformerEncoderLayer_5,122891,1975.494,130,5,887,6939565305623.51,X
2858,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,1972.554,887,,888,6939565305625.14,X
2859,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.163,888,,889,6939565305625.553,X
2860,122891,python_function,torch/nn/modules/transformer.py(750): forward,122891,1967.825,888,,890,6939565305628.974,X
2861,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.318,890,,891,6939565305630.318,X
2862,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.348,890,,892,6939565305632.451,X
2863,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.356,890,,893,6939565305633.577,X
2864,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,1.839,890,,894,6939565305635.195,X
2865,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.188,894,,895,6939565305636.64,X
2866,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.512,890,,896,6939565305637.815,X
2867,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.4,890,,897,6939565305641.387,X
2868,122891,python_function,torch/nn/modules/transformer.py(911): _sa_block,122891,1030.395,890,,898,6939565305645.444,X
2869,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.581,898,,899,6939565305647.582,X
2870,122891,python_function,nn.Module: MultiheadAttention_5,122891,883.264,898,5,900,6939565305659.215,X
2871,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,879.743,900,,901,6939565305661.288,X
2872,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.623,901,,902,6939565305661.705,X
2873,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,874.181,901,,903,6939565305665.79,X
2874,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.321,903,,904,6939565305666.638,X
2875,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.361,903,,905,6939565305667.973,X
2876,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.234,903,,906,6939565305669.158,X
2877,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.257,903,,907,6939565305670.47,X
2878,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,1.014,903,,908,6939565305671.646,X
2879,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.346,908,,909,6939565305672.141,X
2880,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.694,903,,910,6939565305674.798,X
2881,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.519,903,,911,6939565305677.376,X
2882,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.611,903,,912,6939565305679.97,X
2883,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.793,903,,913,6939565305682.153,X
2884,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.511,903,,914,6939565305687.569,X
2885,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.578,903,,915,6939565305689.892,X
2886,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.913,903,,916,6939565305693.067,X
2887,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.425,903,,917,6939565305698.026,X
2888,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.193,903,,918,6939565305701.289,X
2889,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.629,903,,919,6939565305704.464,X
2890,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,829.595,903,,920,6939565305707.239,X
2891,122891,python_function,<built-in function _has_torch_function>,122891,0.814,920,,921,6939565305708.157,X
2892,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.595,920,,922,6939565305710.071,X
2893,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.263,922,,923,6939565305710.449,X
2894,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.255,922,,924,6939565305711.299,X
2895,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.374,922,,925,6939565305711.886,X
2896,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.179,920,,926,6939565305716.829,X
2897,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.194,920,,927,6939565305718.112,X
2898,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.188,920,,928,6939565305719.697,X
2899,122891,python_function,<built-in function isinstance>,122891,0.742,920,,929,6939565305721.255,X
2900,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,380.959,920,,930,6939565305725.939,X
2901,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.982,930,,931,6939565305726.513,X
2902,122891,python_function,<built-in function linear>,122891,158.999,930,,932,6939565305729.159,X
2903,122891,python_function,torch/_tensor.py(1356): unflatten,122891,29.374,930,,933,6939565305889.883,X
2904,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.467,933,,934,6939565305890.604,X
2905,122891,python_function,<built-in function isinstance>,122891,0.404,933,,935,6939565305892.03,X
2906,122891,python_function,<built-in function isinstance>,122891,0.065,933,,936,6939565305893.394,X
2907,122891,python_function,<built-in function isinstance>,122891,0.363,933,,937,6939565305894.149,X
2908,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,22.237,933,,938,6939565305896.706,X
2909,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,16.179,930,,939,6939565305919.873,X
2910,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.731,930,,940,6939565305938.271,X
2911,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,15.549,930,,941,6939565305953.221,X
2912,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,80.575,930,,942,6939565305969.906,X
2913,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.622,920,,943,6939565306110.117,X
2914,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.142,920,,944,6939565306121.249,X
2915,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.208,920,,945,6939565306138.216,X
2916,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.046,920,,946,6939565306147.905,X
2917,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.91,920,,947,6939565306161.273,X
2918,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.678,920,,948,6939565306169.593,X
2919,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.148,920,,949,6939565306181.879,X
2920,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.999,920,,950,6939565306183.945,X
2921,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,13.015,920,,951,6939565306194.085,X
2922,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.404,920,,952,6939565306208.214,X
2923,122891,python_function,<built-in function scaled_dot_product_attention>,122891,165.689,920,,953,6939565306217.971,X
2924,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,18.217,920,,954,6939565306384.744,X
2925,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.604,920,,955,6939565306403.47,X
2926,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.515,920,,956,6939565306404.55,X
2927,122891,python_function,<built-in function linear>,122891,104.543,920,,957,6939565306417.013,X
2928,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.451,920,,958,6939565306522.898,X
2929,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.344,920,,959,6939565306524.618,X
2930,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.29,898,,960,6939565306549.527,X
2931,122891,python_function,nn.Module: Dropout_15,122891,115.891,898,15,961,6939565306559.409,X
2932,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,113.302,961,,962,6939565306561.356,X
2933,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.89,962,,963,6939565306562.718,X
2934,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,105.344,962,,964,6939565306568.94,X
2935,122891,python_function,torch/nn/functional.py(1401): dropout,122891,103.079,964,,965,6939565306570.901,X
2936,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.457,965,,966,6939565306571.459,X
2937,122891,python_function,torch/_VF.py(27): __getattr__,122891,2.076,965,,967,6939565306578.516,X
2938,122891,python_function,<built-in function getattr>,122891,0.784,967,,968,6939565306579.626,X
2939,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,92.773,965,,969,6939565306580.999,X
2940,122891,python_function,nn.Module: LayerNorm_10,122891,118.053,890,10,970,6939565306740.921,X
2941,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,115.931,970,,971,6939565306742.447,X
2942,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.472,971,,972,6939565306743.01,X
2943,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,110.857,971,,973,6939565306747.154,X
2944,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.023,973,,974,6939565306750.209,X
2945,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.52,973,,975,6939565306752.397,X
2946,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,103.914,973,,976,6939565306753.851,X
2947,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.536,976,,977,6939565306754.416,X
2948,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.41,976,,978,6939565306758.604,X
2949,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.666,978,,979,6939565306759.265,X
2950,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,96.863,976,,980,6939565306760.602,X
2951,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.894,890,,981,6939565306862.407,X
2952,122891,python_function,torch/nn/modules/transformer.py(930): _ff_block,122891,559.863,890,,982,6939565306865.735,X
2953,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.378,982,,983,6939565306867.121,X
2954,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.171,982,,984,6939565306869.698,X
2955,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.649,982,,985,6939565306872.307,X
2956,122891,python_function,nn.Module: Linear_10,122891,126.16,982,10,986,6939565306882.139,X
2957,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,124.331,986,,987,6939565306883.42,X
2958,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.052,987,,988,6939565306883.953,X
2959,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,119.86,987,,989,6939565306887.478,X
2960,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.633,989,,990,6939565306889.327,X
2961,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.484,989,,991,6939565306891.021,X
2962,122891,python_function,<built-in function linear>,122891,115.418,989,,992,6939565306891.716,X
2963,122891,python_function,torch/nn/functional.py(1693): relu,122891,57.256,982,,993,6939565307009.682,X
2964,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.328,993,,994,6939565307010.36,X
2965,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,54.991,993,,995,6939565307011.692,X
2966,122891,python_function,nn.Module: Dropout_16,122891,102.919,982,16,996,6939565307073.856,X
2967,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,100.934,996,,997,6939565307075.308,X
2968,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.245,997,,998,6939565307075.746,X
2969,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,96.495,997,,999,6939565307079.462,X
2970,122891,python_function,torch/nn/functional.py(1401): dropout,122891,94.917,999,,1000,6939565307080.768,X
2971,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.386,1000,,1001,6939565307080.982,X
2972,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.49,1000,,1002,6939565307086.765,X
2973,122891,python_function,<built-in function getattr>,122891,0.499,1002,,1003,6939565307087.665,X
2974,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,86.807,1000,,1004,6939565307088.701,X
2975,122891,python_function,nn.Module: Linear_11,122891,135.437,982,11,1005,6939565307188.828,X
2976,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,133.324,1005,,1006,6939565307190.372,X
2977,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.003,1006,,1007,6939565307190.845,X
2978,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,129.197,1006,,1008,6939565307194.185,X
2979,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.88,1008,,1009,6939565307196.003,X
2980,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.505,1008,,1010,6939565307198.061,X
2981,122891,python_function,<built-in function linear>,122891,124.351,1008,,1011,6939565307198.782,X
2982,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.061,982,,1012,6939565307327.327,X
2983,122891,python_function,nn.Module: Dropout_17,122891,91.977,982,17,1013,6939565307333.114,X
2984,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,90.076,1013,,1014,6939565307334.48,X
2985,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.169,1014,,1015,6939565307334.92,X
2986,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,86.053,1014,,1016,6939565307338.202,X
2987,122891,python_function,torch/nn/functional.py(1401): dropout,122891,84.675,1016,,1017,6939565307339.192,X
2988,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.29,1017,,1018,6939565307339.479,X
2989,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.069,1017,,1019,6939565307343.566,X
2990,122891,python_function,<built-in function getattr>,122891,0.278,1019,,1020,6939565307344.258,X
2991,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,78.676,1017,,1021,6939565307344.981,X
2992,122891,python_function,nn.Module: LayerNorm_11,122891,113.557,890,11,1022,6939565307481.852,X
2993,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,111.277,1022,,1023,6939565307483.694,X
2994,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.098,1023,,1024,6939565307484.376,X
2995,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,106.671,1023,,1025,6939565307487.995,X
2996,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.921,1025,,1026,6939565307490.504,X
2997,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.551,1025,,1027,6939565307492.633,X
2998,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,100.373,1025,,1028,6939565307493.842,X
2999,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.452,1028,,1029,6939565307494.257,X
3000,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.053,1028,,1030,6939565307497.168,X
3001,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.399,1030,,1031,6939565307497.712,X
3002,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,95.302,1028,,1032,6939565307498.731,X
3003,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.346,130,,1033,6939565307606.174,X
3004,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.349,130,,1034,6939565307611.946,X
3005,122891,python_function,nn.Module: LayerNorm_12,122891,104.529,130,12,1035,6939565307624.251,X
3006,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,102.733,1035,,1036,6939565307625.641,X
3007,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.95,1036,,1037,6939565307626.317,X
3008,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,98.278,1036,,1038,6939565307629.777,X
3009,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.601,1038,,1039,6939565307631.504,X
3010,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.518,1038,,1040,6939565307633.176,X
3011,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,93.559,1038,,1041,6939565307634.265,X
3012,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.354,1041,,1042,6939565307634.554,X
3013,122891,python_function,torch/backends/__init__.py(38): __get__,122891,0.5,1041,,1043,6939565307636.314,X
3014,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.224,1043,,1044,6939565307636.531,X
3015,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,90.362,1041,,1045,6939565307637.295,X
3016,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.249,120,,1046,6939565307738.213,X
3017,122891,python_function,nn.Module: TransformerDecoder_0,122891,20243.073,120,0,1047,6939565307751.918,X
3018,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,20237.586,1047,,1048,6939565307755.567,X
3019,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.937,1048,,1049,6939565307756.246,X
3020,122891,python_function,torch/nn/modules/transformer.py(557): forward,122891,20230.18,1048,,1050,6939565307762.338,X
3021,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.7,1050,,1051,6939565307765.226,X
3022,122891,python_function,torch/nn/modules/container.py(329): __getitem__,122891,16.219,1050,,1052,6939565307768.826,X
3023,122891,python_function,<built-in function isinstance>,122891,0.387,1052,,1053,6939565307770.297,X
3024,122891,python_function,torch/nn/modules/container.py(312): _get_abs_string_index,122891,10.145,1052,,1054,6939565307774.103,X
3025,122891,python_function,<built-in function index>,122891,0.29,1054,,1055,6939565307775.72,X
3026,122891,python_function,<built-in function len>,122891,2.729,1054,,1056,6939565307776.458,X
3027,122891,python_function,torch/nn/modules/container.py(350): __len__,122891,0.996,1056,,1057,6939565307777.849,X
3028,122891,python_function,<built-in function len>,122891,0.276,1057,,1058,6939565307778.47,X
3029,122891,python_function,<built-in function len>,122891,1.13,1054,,1059,6939565307780.908,X
3030,122891,python_function,torch/nn/modules/container.py(350): __len__,122891,0.613,1059,,1060,6939565307781.256,X
3031,122891,python_function,<built-in function len>,122891,0.112,1060,,1061,6939565307781.661,X
3032,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.425,1050,,1062,6939565307787.109,X
3033,122891,python_function,torch/nn/modules/transformer.py(47): _get_seq_len,122891,5.724,1050,,1063,6939565307791.153,X
3034,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,2.112,1063,,1064,6939565307792.82,X
3035,122891,python_function,<built-in function len>,122891,0.543,1063,,1065,6939565307795.484,X
3036,122891,python_function,torch/nn/modules/transformer.py(1158): _detect_is_causal_mask,122891,0.646,1050,,1066,6939565307798.983,X
3037,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.278,1050,,1067,6939565307801.526,X
3038,122891,python_function,torch/nn/modules/container.py(354): __iter__,122891,2.769,1050,,1068,6939565307804.44,X
3039,122891,python_function,<built-in method values of collections.defaultdict object at 0x7fb81c7b0ea0>,122891,0.599,1068,,1069,6939565307805.637,X
3040,122891,python_function,<built-in function iter>,122891,0.459,1068,,1070,6939565307806.61,X
3041,122891,python_function,nn.Module: TransformerDecoderLayer_0,122891,3649.215,1050,0,1071,6939565307814.176,X
3042,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,3645.2,1071,,1072,6939565307816.633,X
3043,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.71,1072,,1073,6939565307817.131,X
3044,122891,python_function,torch/nn/modules/transformer.py(1031): forward,122891,3639.299,1072,,1074,6939565307821.542,X
3045,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.691,1074,,1075,6939565307824.638,X
3046,122891,python_function,torch/nn/modules/transformer.py(1100): _sa_block,122891,1019.968,1074,,1076,6939565307827.842,X
3047,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.489,1076,,1077,6939565307829.576,X
3048,122891,python_function,nn.Module: MultiheadAttention_6,122891,873.301,1076,6,1078,6939565307841.422,X
3049,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,869.869,1078,,1079,6939565307843.454,X
3050,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.586,1079,,1080,6939565307843.811,X
3051,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,863.704,1079,,1081,6939565307848.352,X
3052,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.512,1081,,1082,6939565307849.382,X
3053,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.447,1081,,1083,6939565307852.053,X
3054,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.309,1081,,1084,6939565307854.611,X
3055,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.238,1081,,1085,6939565307856.04,X
3056,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,4.753,1081,,1086,6939565307858.049,X
3057,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.2,1086,,1087,6939565307862.332,X
3058,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.68,1081,,1088,6939565307865.008,X
3059,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.546,1081,,1089,6939565307867.589,X
3060,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.554,1081,,1090,6939565307870.28,X
3061,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.583,1081,,1091,6939565307872.555,X
3062,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.556,1081,,1092,6939565307877.697,X
3063,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.551,1081,,1093,6939565307880.026,X
3064,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.734,1081,,1094,6939565307883.097,X
3065,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.337,1081,,1095,6939565307887.594,X
3066,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.294,1081,,1096,6939565307890.734,X
3067,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.817,1081,,1097,6939565307893.842,X
3068,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,812.449,1081,,1098,6939565307896.824,X
3069,122891,python_function,<built-in function _has_torch_function>,122891,0.777,1098,,1099,6939565307897.791,X
3070,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.745,1098,,1100,6939565307899.661,X
3071,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.272,1100,,1101,6939565307900.039,X
3072,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.225,1100,,1102,6939565307900.911,X
3073,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.191,1100,,1103,6939565307901.568,X
3074,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.157,1098,,1104,6939565307905.958,X
3075,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.21,1098,,1105,6939565307907.325,X
3076,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.194,1098,,1106,6939565307908.834,X
3077,122891,python_function,<built-in function isinstance>,122891,0.577,1098,,1107,6939565307910.233,X
3078,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,365.641,1098,,1108,6939565307915.046,X
3079,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.382,1108,,1109,6939565307915.579,X
3080,122891,python_function,<built-in function linear>,122891,143.921,1108,,1110,6939565307917.57,X
3081,122891,python_function,torch/_tensor.py(1356): unflatten,122891,27.715,1108,,1111,6939565308063.368,X
3082,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.402,1111,,1112,6939565308064.182,X
3083,122891,python_function,<built-in function isinstance>,122891,0.405,1111,,1113,6939565308065.451,X
3084,122891,python_function,<built-in function isinstance>,122891,0.085,1111,,1114,6939565308066.518,X
3085,122891,python_function,<built-in function isinstance>,122891,0.21,1111,,1115,6939565308067.202,X
3086,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,21.244,1111,,1116,6939565308069.538,X
3087,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,16.495,1108,,1117,6939565308091.762,X
3088,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,17.141,1108,,1118,6939565308110.7,X
3089,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,14.808,1108,,1119,6939565308129.131,X
3090,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,78.851,1108,,1120,6939565308145.39,X
3091,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.974,1098,,1121,6939565308284.091,X
3092,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.16,1098,,1122,6939565308296.694,X
3093,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.77,1098,,1123,6939565308313.244,X
3094,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.485,1098,,1124,6939565308323.49,X
3095,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.987,1098,,1125,6939565308336.494,X
3096,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.135,1098,,1126,6939565308345.998,X
3097,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.963,1098,,1127,6939565308357.827,X
3098,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.261,1098,,1128,6939565308359.724,X
3099,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.712,1098,,1129,6939565308371.269,X
3100,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.741,1098,,1130,6939565308380.12,X
3101,122891,python_function,<built-in function scaled_dot_product_attention>,122891,172.795,1098,,1131,6939565308389.174,X
3102,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,16.903,1098,,1132,6939565308563.056,X
3103,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.597,1098,,1133,6939565308580.519,X
3104,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.312,1098,,1134,6939565308581.637,X
3105,122891,python_function,<built-in function linear>,122891,101.617,1098,,1135,6939565308592.711,X
3106,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.41,1098,,1136,6939565308695.434,X
3107,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.209,1098,,1137,6939565308697.15,X
3108,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.714,1076,,1138,6939565308723.145,X
3109,122891,python_function,nn.Module: Dropout_18,122891,113.495,1076,18,1139,6939565308733.748,X
3110,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,110.962,1139,,1140,6939565308735.679,X
3111,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.86,1140,,1141,6939565308736.984,X
3112,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,102.878,1140,,1142,6939565308743.37,X
3113,122891,python_function,torch/nn/functional.py(1401): dropout,122891,100.824,1142,,1143,6939565308745.155,X
3114,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.295,1143,,1144,6939565308745.612,X
3115,122891,python_function,torch/_VF.py(27): __getattr__,122891,2.165,1143,,1145,6939565308752.855,X
3116,122891,python_function,<built-in function getattr>,122891,0.637,1145,,1146,6939565308754.029,X
3117,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,90.308,1143,,1147,6939565308755.451,X
3118,122891,python_function,nn.Module: LayerNorm_13,122891,128.349,1074,13,1148,6939565308939.144,X
3119,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,125.825,1148,,1149,6939565308941.016,X
3120,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.833,1149,,1150,6939565308941.761,X
3121,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,120.13,1149,,1151,6939565308946.34,X
3122,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.996,1151,,1152,6939565308949.655,X
3123,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.531,1151,,1153,6939565308951.954,X
3124,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,112.721,1151,,1154,6939565308953.473,X
3125,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.445,1154,,1155,6939565308954.025,X
3126,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.524,1154,,1156,6939565308958.161,X
3127,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.642,1156,,1157,6939565308958.953,X
3128,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,105.756,1154,,1158,6939565308960.26,X
3129,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.755,1074,,1159,6939565309071.253,X
3130,122891,python_function,torch/nn/modules/transformer.py(1119): _mha_block,122891,1325.722,1074,,1160,6939565309076.199,X
3131,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.491,1160,,1161,6939565309078.429,X
3132,122891,python_function,nn.Module: MultiheadAttention_7,122891,1169.508,1160,7,1162,6939565309091.0,X
3133,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,1165.374,1162,,1163,6939565309093.559,X
3134,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.074,1163,,1164,6939565309094.17,X
3135,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,1158.221,1163,,1165,6939565309099.284,X
3136,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.566,1165,,1166,6939565309100.134,X
3137,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.559,1165,,1167,6939565309102.464,X
3138,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.358,1165,,1168,6939565309104.82,X
3139,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.221,1165,,1169,6939565309106.418,X
3140,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.222,1165,,1170,6939565309108.02,X
3141,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.228,1170,,1171,6939565309109.656,X
3142,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.657,1165,,1172,6939565309115.784,X
3143,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.62,1165,,1173,6939565309118.264,X
3144,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.327,1165,,1174,6939565309121.306,X
3145,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.134,1165,,1175,6939565309126.318,X
3146,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.16,1165,,1176,6939565309129.313,X
3147,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.732,1165,,1177,6939565309132.389,X
3148,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,1118.777,1165,,1178,6939565309135.101,X
3149,122891,python_function,<built-in function _has_torch_function>,122891,0.753,1178,,1179,6939565309136.02,X
3150,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.568,1178,,1180,6939565309137.793,X
3151,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.274,1180,,1181,6939565309138.264,X
3152,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.352,1180,,1182,6939565309138.999,X
3153,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.229,1180,,1183,6939565309139.719,X
3154,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.163,1178,,1184,6939565309144.066,X
3155,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.248,1178,,1185,6939565309145.193,X
3156,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.218,1178,,1186,6939565309146.805,X
3157,122891,python_function,<built-in function isinstance>,122891,0.567,1178,,1187,6939565309148.437,X
3158,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,664.217,1178,,1188,6939565309152.474,X
3159,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.567,1188,,1189,6939565309152.971,X
3160,122891,python_function,torch/_tensor.py(968): split,122891,79.491,1188,,1190,6939565309186.524,X
3161,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.281,1190,,1191,6939565309187.97,X
3162,122891,python_function,<built-in function isinstance>,122891,0.554,1190,,1192,6939565309189.166,X
3163,122891,python_function,<built-in function isinstance>,122891,0.164,1190,,1193,6939565309191.129,X
3164,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.943,1190,,1194,6939565309197.461,X
3165,122891,python_function,<built-in function getattr>,122891,0.616,1194,,1195,6939565309198.687,X
3166,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,57.401,1190,,1196,6939565309208.402,X
3167,122891,python_function,torch/_tensor.py(968): split,122891,32.062,1188,,1197,6939565309268.156,X
3168,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.333,1197,,1198,6939565309268.471,X
3169,122891,python_function,<built-in function isinstance>,122891,0.313,1197,,1199,6939565309269.204,X
3170,122891,python_function,<built-in function isinstance>,122891,0.219,1197,,1200,6939565309270.192,X
3171,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.123,1197,,1201,6939565309274.231,X
3172,122891,python_function,<built-in function getattr>,122891,0.336,1201,,1202,6939565309274.927,X
3173,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,24.402,1197,,1203,6939565309275.692,X
3174,122891,python_function,<built-in function linear>,122891,147.513,1188,,1204,6939565309301.435,X
3175,122891,python_function,<built-in function linear>,122891,143.179,1188,,1205,6939565309450.05,X
3176,122891,python_function,torch/_tensor.py(1356): unflatten,122891,30.345,1188,,1206,6939565309595.728,X
3177,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.408,1206,,1207,6939565309596.527,X
3178,122891,python_function,<built-in function isinstance>,122891,0.367,1206,,1208,6939565309597.919,X
3179,122891,python_function,<built-in function isinstance>,122891,0.087,1206,,1209,6939565309599.16,X
3180,122891,python_function,<built-in function isinstance>,122891,0.258,1206,,1210,6939565309599.985,X
3181,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,22.648,1206,,1211,6939565309602.875,X
3182,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,16.984,1188,,1212,6939565309627.356,X
3183,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.861,1188,,1213,6939565309648.024,X
3184,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,15.201,1188,,1214,6939565309664.529,X
3185,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,90.041,1188,,1215,6939565309681.623,X
3186,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,12.422,1178,,1216,6939565309822.422,X
3187,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.719,1178,,1217,6939565309835.382,X
3188,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.145,1178,,1218,6939565309852.178,X
3189,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.861,1178,,1219,6939565309860.795,X
3190,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.7,1178,,1220,6939565309874.326,X
3191,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.417,1178,,1221,6939565309884.443,X
3192,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.078,1178,,1222,6939565309897.865,X
3193,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.565,1178,,1223,6939565309899.847,X
3194,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.247,1178,,1224,6939565309911.687,X
3195,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.469,1178,,1225,6939565309920.979,X
3196,122891,python_function,<built-in function scaled_dot_product_attention>,122891,171.989,1178,,1226,6939565309931.979,X
3197,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,18.269,1178,,1227,6939565310105.05,X
3198,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.615,1178,,1228,6939565310123.793,X
3199,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.298,1178,,1229,6939565310125.059,X
3200,122891,python_function,<built-in function linear>,122891,101.63,1178,,1230,6939565310136.664,X
3201,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.262,1178,,1231,6939565310239.548,X
3202,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.743,1178,,1232,6939565310241.12,X
3203,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.592,1160,,1233,6939565310269.341,X
3204,122891,python_function,nn.Module: Dropout_19,122891,121.366,1160,19,1234,6939565310279.952,X
3205,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,118.688,1234,,1235,6939565310282.009,X
3206,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.173,1235,,1236,6939565310283.367,X
3207,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,109.686,1235,,1237,6939565310290.602,X
3208,122891,python_function,torch/nn/functional.py(1401): dropout,122891,107.131,1237,,1238,6939565310292.844,X
3209,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.316,1238,,1239,6939565310293.498,X
3210,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.918,1238,,1240,6939565310299.976,X
3211,122891,python_function,<built-in function getattr>,122891,0.607,1240,,1241,6939565310301.177,X
3212,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,97.341,1238,,1242,6939565310302.394,X
3213,122891,python_function,nn.Module: LayerNorm_14,122891,122.253,1074,14,1243,6939565310468.294,X
3214,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,120.069,1243,,1244,6939565310469.92,X
3215,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.991,1244,,1245,6939565310470.501,X
3216,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,114.264,1244,,1246,6939565310475.332,X
3217,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.984,1246,,1247,6939565310478.5,X
3218,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.492,1246,,1248,6939565310480.803,X
3219,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,107.073,1246,,1249,6939565310482.196,X
3220,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.502,1249,,1250,6939565310482.674,X
3221,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.564,1249,,1251,6939565310486.883,X
3222,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.568,1251,,1252,6939565310487.776,X
3223,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,99.986,1249,,1253,6939565310489.063,X
3224,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.975,1074,,1254,6939565310594.666,X
3225,122891,python_function,torch/nn/modules/transformer.py(1139): _ff_block,122891,682.448,1074,,1255,6939565310598.664,X
3226,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.659,1255,,1256,6939565310670.887,X
3227,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.78,1255,,1257,6939565310674.276,X
3228,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.39,1255,,1258,6939565310677.617,X
3229,122891,python_function,nn.Module: Linear_12,122891,162.423,1255,12,1259,6939565310691.037,X
3230,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,160.572,1259,,1260,6939565310692.355,X
3231,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.388,1260,,1261,6939565310692.86,X
3232,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,155.523,1260,,1262,6939565310697.01,X
3233,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.592,1262,,1263,6939565310699.255,X
3234,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.522,1262,,1264,6939565310701.065,X
3235,122891,python_function,<built-in function linear>,122891,150.413,1262,,1265,6939565310701.833,X
3236,122891,python_function,torch/nn/functional.py(1693): relu,122891,61.702,1255,,1266,6939565310855.563,X
3237,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.452,1266,,1267,6939565310856.139,X
3238,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,58.765,1266,,1268,6939565310858.191,X
3239,122891,python_function,nn.Module: Dropout_20,122891,107.177,1255,20,1269,6939565310924.834,X
3240,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,105.04,1269,,1270,6939565310926.375,X
3241,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.524,1270,,1271,6939565310926.837,X
3242,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,100.103,1270,,1272,6939565310931.008,X
3243,122891,python_function,torch/nn/functional.py(1401): dropout,122891,97.853,1272,,1273,6939565310932.933,X
3244,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.297,1273,,1274,6939565310933.193,X
3245,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.728,1273,,1275,6939565310939.819,X
3246,122891,python_function,<built-in function getattr>,122891,0.628,1275,,1276,6939565310940.822,X
3247,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,88.616,1273,,1277,6939565310941.948,X
3248,122891,python_function,nn.Module: Linear_13,122891,132.934,1255,13,1278,6939565311042.518,X
3249,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,131.221,1278,,1279,6939565311043.666,X
3250,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.075,1279,,1280,6939565311044.121,X
3251,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,127.049,1279,,1281,6939565311047.406,X
3252,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.228,1281,,1282,6939565311049.577,X
3253,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.53,1281,,1283,6939565311051.982,X
3254,122891,python_function,<built-in function linear>,122891,121.308,1281,,1284,6939565311052.897,X
3255,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.198,1255,,1285,6939565311179.217,X
3256,122891,python_function,nn.Module: Dropout_21,122891,95.468,1255,21,1286,6939565311185.107,X
3257,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,93.967,1286,,1287,6939565311186.042,X
3258,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.177,1287,,1288,6939565311186.421,X
3259,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,89.806,1287,,1289,6939565311189.927,X
3260,122891,python_function,torch/nn/functional.py(1401): dropout,122891,88.64,1289,,1290,6939565311190.866,X
3261,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.268,1290,,1291,6939565311191.134,X
3262,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.289,1290,,1292,6939565311195.223,X
3263,122891,python_function,<built-in function getattr>,122891,0.307,1292,,1293,6939565311195.865,X
3264,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,82.36,1290,,1294,6939565311196.926,X
3265,122891,python_function,nn.Module: LayerNorm_15,122891,122.405,1074,15,1295,6939565311337.238,X
3266,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,120.45,1295,,1296,6939565311338.63,X
3267,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.184,1296,,1297,6939565311339.166,X
3268,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,115.865,1296,,1298,6939565311342.912,X
3269,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.752,1298,,1299,6939565311345.639,X
3270,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.516,1298,,1300,6939565311347.679,X
3271,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,109.403,1298,,1301,6939565311349.092,X
3272,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.366,1301,,1302,6939565311349.626,X
3273,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.434,1301,,1303,6939565311353.173,X
3274,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.561,1303,,1304,6939565311353.944,X
3275,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,103.218,1301,,1305,6939565311355.104,X
3276,122891,python_function,nn.Module: TransformerDecoderLayer_1,122891,3368.107,1050,1,1306,6939565311472.355,X
3277,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,3364.082,1306,,1307,6939565311475.029,X
3278,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.074,1307,,1308,6939565311475.461,X
3279,122891,python_function,torch/nn/modules/transformer.py(1031): forward,122891,3358.491,1307,,1309,6939565311479.668,X
3280,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.426,1309,,1310,6939565311482.382,X
3281,122891,python_function,torch/nn/modules/transformer.py(1100): _sa_block,122891,1009.015,1309,,1311,6939565311485.938,X
3282,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.604,1311,,1312,6939565311487.626,X
3283,122891,python_function,nn.Module: MultiheadAttention_8,122891,858.988,1311,8,1313,6939565311500.164,X
3284,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,854.722,1313,,1314,6939565311503.005,X
3285,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.595,1314,,1315,6939565311503.385,X
3286,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,849.256,1314,,1316,6939565311507.348,X
3287,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.73,1316,,1317,6939565311508.644,X
3288,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.516,1316,,1318,6939565311511.289,X
3289,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.371,1316,,1319,6939565311513.886,X
3290,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.253,1316,,1320,6939565311515.715,X
3291,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.025,1316,,1321,6939565311517.702,X
3292,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.291,1321,,1322,6939565311519.137,X
3293,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.721,1316,,1323,6939565311521.891,X
3294,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.503,1316,,1324,6939565311524.37,X
3295,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.625,1316,,1325,6939565311527.125,X
3296,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.559,1316,,1326,6939565311529.577,X
3297,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.586,1316,,1327,6939565311534.874,X
3298,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.579,1316,,1328,6939565311537.567,X
3299,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.847,1316,,1329,6939565311540.689,X
3300,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.1,1316,,1330,6939565311545.51,X
3301,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.275,1316,,1331,6939565311548.468,X
3302,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.627,1316,,1332,6939565311551.544,X
3303,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,799.431,1316,,1333,6939565311554.325,X
3304,122891,python_function,<built-in function _has_torch_function>,122891,0.832,1333,,1334,6939565311555.277,X
3305,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.441,1333,,1335,6939565311557.404,X
3306,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.289,1335,,1336,6939565311557.771,X
3307,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.297,1335,,1337,6939565311558.584,X
3308,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.189,1335,,1338,6939565311559.222,X
3309,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.347,1333,,1339,6939565311564.004,X
3310,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.256,1333,,1340,6939565311565.395,X
3311,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.352,1333,,1341,6939565311567.363,X
3312,122891,python_function,<built-in function isinstance>,122891,0.714,1333,,1342,6939565311569.141,X
3313,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,348.172,1333,,1343,6939565311574.04,X
3314,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.99,1343,,1344,6939565311574.695,X
3315,122891,python_function,<built-in function linear>,122891,128.901,1343,,1345,6939565311577.393,X
3316,122891,python_function,torch/_tensor.py(1356): unflatten,122891,27.024,1343,,1346,6939565311707.742,X
3317,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.371,1346,,1347,6939565311708.494,X
3318,122891,python_function,<built-in function isinstance>,122891,0.308,1346,,1348,6939565311709.86,X
3319,122891,python_function,<built-in function isinstance>,122891,0.118,1346,,1349,6939565311710.963,X
3320,122891,python_function,<built-in function isinstance>,122891,0.201,1346,,1350,6939565311711.748,X
3321,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,20.4,1346,,1351,6939565311714.06,X
3322,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,17.523,1343,,1352,6939565311735.517,X
3323,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.238,1343,,1353,6939565311755.447,X
3324,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,14.304,1343,,1354,6939565311770.0,X
3325,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,79.505,1343,,1355,6939565311785.64,X
3326,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,12.296,1333,,1356,6939565311925.321,X
3327,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.862,1333,,1357,6939565311938.214,X
3328,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.63,1333,,1358,6939565311954.999,X
3329,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.304,1333,,1359,6939565311963.19,X
3330,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.32,1333,,1360,6939565311977.14,X
3331,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,15.388,1333,,1361,6939565311986.918,X
3332,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.98,1333,,1362,6939565312004.043,X
3333,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.953,1333,,1363,6939565312005.979,X
3334,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.883,1333,,1364,6939565312017.133,X
3335,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.506,1333,,1365,6939565312028.227,X
3336,122891,python_function,<built-in function scaled_dot_product_attention>,122891,162.435,1333,,1366,6939565312037.085,X
3337,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,16.336,1333,,1367,6939565312200.569,X
3338,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.599,1333,,1368,6939565312217.509,X
3339,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,16.888,1333,,1369,6939565312218.648,X
3340,122891,python_function,<built-in function linear>,122891,101.169,1333,,1370,6939565312237.362,X
3341,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.231,1333,,1371,6939565312339.664,X
3342,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.45,1333,,1372,6939565312341.329,X
3343,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.512,1311,,1373,6939565312366.845,X
3344,122891,python_function,nn.Module: Dropout_22,122891,116.627,1311,22,1374,6939565312377.711,X
3345,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,113.94,1374,,1375,6939565312379.634,X
3346,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.07,1375,,1376,6939565312380.749,X
3347,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,105.164,1375,,1377,6939565312388.004,X
3348,122891,python_function,torch/nn/functional.py(1401): dropout,122891,102.743,1377,,1378,6939565312390.159,X
3349,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.35,1378,,1379,6939565312390.669,X
3350,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.913,1378,,1380,6939565312397.574,X
3351,122891,python_function,<built-in function getattr>,122891,0.766,1380,,1381,6939565312398.602,X
3352,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,92.73,1378,,1382,6939565312399.923,X
3353,122891,python_function,nn.Module: LayerNorm_16,122891,120.819,1309,16,1383,6939565312557.427,X
3354,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,118.641,1383,,1384,6939565312558.956,X
3355,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.705,1384,,1385,6939565312559.555,X
3356,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,112.713,1384,,1386,6939565312564.458,X
3357,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.002,1386,,1387,6939565312567.49,X
3358,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.512,1386,,1388,6939565312569.761,X
3359,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,105.662,1386,,1389,6939565312571.216,X
3360,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.539,1389,,1390,6939565312571.697,X
3361,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.647,1389,,1391,6939565312575.698,X
3362,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.655,1391,,1392,6939565312576.399,X
3363,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,98.814,1389,,1393,6939565312577.895,X
3364,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.993,1309,,1394,6939565312681.582,X
3365,122891,python_function,torch/nn/modules/transformer.py(1119): _mha_block,122891,1214.87,1309,,1395,6939565312684.916,X
3366,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.528,1395,,1396,6939565312686.674,X
3367,122891,python_function,nn.Module: MultiheadAttention_9,122891,1064.543,1395,9,1397,6939565312699.059,X
3368,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,1060.143,1397,,1398,6939565312702.034,X
3369,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.077,1398,,1399,6939565312702.528,X
3370,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,1053.275,1398,,1400,6939565312707.591,X
3371,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.523,1400,,1401,6939565312708.636,X
3372,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.631,1400,,1402,6939565312710.864,X
3373,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.403,1400,,1403,6939565312713.329,X
3374,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.252,1400,,1404,6939565312714.825,X
3375,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,1.948,1400,,1405,6939565312716.332,X
3376,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.276,1405,,1406,6939565312717.734,X
3377,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.736,1400,,1407,6939565312723.702,X
3378,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.631,1400,,1408,6939565312726.341,X
3379,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.77,1400,,1409,6939565312729.558,X
3380,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.198,1400,,1410,6939565312733.645,X
3381,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.296,1400,,1411,6939565312736.655,X
3382,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.66,1400,,1412,6939565312739.724,X
3383,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,1015.745,1400,,1413,6939565312742.041,X
3384,122891,python_function,<built-in function _has_torch_function>,122891,0.812,1413,,1414,6939565312742.956,X
3385,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.454,1413,,1415,6939565312744.752,X
3386,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.271,1415,,1416,6939565312745.211,X
3387,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.252,1415,,1417,6939565312746.006,X
3388,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.199,1415,,1418,6939565312746.622,X
3389,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.223,1413,,1419,6939565312750.5,X
3390,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.23,1413,,1420,6939565312751.781,X
3391,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.209,1413,,1421,6939565312753.25,X
3392,122891,python_function,<built-in function isinstance>,122891,0.572,1413,,1422,6939565312754.796,X
3393,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,566.47,1413,,1423,6939565312759.025,X
3394,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.538,1423,,1424,6939565312759.513,X
3395,122891,python_function,torch/_tensor.py(968): split,122891,40.049,1423,,1425,6939565312762.93,X
3396,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.357,1425,,1426,6939565312763.624,X
3397,122891,python_function,<built-in function isinstance>,122891,0.647,1425,,1427,6939565312764.463,X
3398,122891,python_function,<built-in function isinstance>,122891,0.2,1425,,1428,6939565312766.058,X
3399,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.415,1425,,1429,6939565312770.688,X
3400,122891,python_function,<built-in function getattr>,122891,0.352,1429,,1430,6939565312771.658,X
3401,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,30.293,1425,,1431,6939565312772.498,X
3402,122891,python_function,torch/_tensor.py(968): split,122891,27.442,1423,,1432,6939565312804.408,X
3403,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.27,1432,,1433,6939565312804.793,X
3404,122891,python_function,<built-in function isinstance>,122891,0.62,1432,,1434,6939565312805.393,X
3405,122891,python_function,<built-in function isinstance>,122891,0.19,1432,,1435,6939565312806.62,X
3406,122891,python_function,torch/_VF.py(27): __getattr__,122891,0.808,1432,,1436,6939565312809.758,X
3407,122891,python_function,<built-in function getattr>,122891,0.126,1436,,1437,6939565312810.327,X
3408,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,20.865,1432,,1438,6939565312810.821,X
3409,122891,python_function,<built-in function linear>,122891,196.889,1423,,1439,6939565312832.707,X
3410,122891,python_function,<built-in function linear>,122891,104.499,1423,,1440,6939565313030.318,X
3411,122891,python_function,torch/_tensor.py(1356): unflatten,122891,25.434,1423,,1441,6939565313136.563,X
3412,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.505,1441,,1442,6939565313137.127,X
3413,122891,python_function,<built-in function isinstance>,122891,0.378,1441,,1443,6939565313138.5,X
3414,122891,python_function,<built-in function isinstance>,122891,0.083,1441,,1444,6939565313139.476,X
3415,122891,python_function,<built-in function isinstance>,122891,0.163,1441,,1445,6939565313140.428,X
3416,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,18.959,1441,,1446,6939565313142.765,X
3417,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,14.519,1423,,1447,6939565313162.924,X
3418,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.444,1423,,1448,6939565313180.03,X
3419,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,14.782,1423,,1449,6939565313194.774,X
3420,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,74.305,1423,,1450,6939565313210.767,X
3421,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.114,1413,,1451,6939565313330.457,X
3422,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.257,1413,,1452,6939565313341.151,X
3423,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.136,1413,,1453,6939565313356.117,X
3424,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.456,1413,,1454,6939565313365.774,X
3425,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.047,1413,,1455,6939565313378.526,X
3426,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.248,1413,,1456,6939565313387.983,X
3427,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.0,1413,,1457,6939565313399.953,X
3428,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.754,1413,,1458,6939565313401.92,X
3429,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.78,1413,,1459,6939565313410.822,X
3430,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.647,1413,,1460,6939565313419.655,X
3431,122891,python_function,<built-in function scaled_dot_product_attention>,122891,182.292,1413,,1461,6939565313430.736,X
3432,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,18.223,1413,,1462,6939565313614.024,X
3433,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.67,1413,,1463,6939565313632.799,X
3434,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.141,1413,,1464,6939565313633.959,X
3435,122891,python_function,<built-in function linear>,122891,96.956,1413,,1465,6939565313645.921,X
3436,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.283,1413,,1466,6939565313744.02,X
3437,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.315,1413,,1467,6939565313745.596,X
3438,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.559,1395,,1468,6939565313771.934,X
3439,122891,python_function,nn.Module: Dropout_23,122891,116.567,1395,23,1469,6939565313782.687,X
3440,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,113.741,1469,,1470,6939565313784.819,X
3441,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.282,1470,,1471,6939565313786.202,X
3442,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,104.579,1470,,1472,6939565313793.625,X
3443,122891,python_function,torch/nn/functional.py(1401): dropout,122891,102.131,1472,,1473,6939565313795.785,X
3444,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.425,1473,,1474,6939565313796.555,X
3445,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.925,1473,,1475,6939565313802.832,X
3446,122891,python_function,<built-in function getattr>,122891,0.666,1475,,1476,6939565313803.866,X
3447,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,92.539,1473,,1477,6939565313805.151,X
3448,122891,python_function,nn.Module: LayerNorm_17,122891,126.912,1309,17,1478,6939565313964.684,X
3449,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,124.838,1478,,1479,6939565313966.262,X
3450,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.5,1479,,1480,6939565313966.835,X
3451,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,119.78,1479,,1481,6939565313971.0,X
3452,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.927,1481,,1482,6939565313974.193,X
3453,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.492,1481,,1483,6939565313976.284,X
3454,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,112.802,1481,,1484,6939565313977.703,X
3455,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.588,1484,,1485,6939565313978.215,X
3456,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.63,1484,,1486,6939565313983.737,X
3457,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.718,1486,,1487,6939565313984.54,X
3458,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,104.066,1484,,1488,6939565313986.184,X
3459,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.067,1309,,1489,6939565314094.819,X
3460,122891,python_function,torch/nn/modules/transformer.py(1139): _ff_block,122891,564.88,1309,,1490,6939565314098.111,X
3461,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.374,1490,,1491,6939565314099.71,X
3462,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.36,1490,,1492,6939565314102.296,X
3463,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.338,1490,,1493,6939565314105.132,X
3464,122891,python_function,nn.Module: Linear_14,122891,124.919,1490,14,1494,6939565314114.708,X
3465,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,122.854,1494,,1495,6939565314116.209,X
3466,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.187,1495,,1496,6939565314116.672,X
3467,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,117.971,1495,,1497,6939565314120.748,X
3468,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.703,1497,,1498,6939565314123.128,X
3469,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.496,1497,,1499,6939565314124.967,X
3470,122891,python_function,<built-in function linear>,122891,112.783,1497,,1500,6939565314125.689,X
3471,122891,python_function,torch/nn/functional.py(1693): relu,122891,54.896,1490,,1501,6939565314241.154,X
3472,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.411,1501,,1502,6939565314241.578,X
3473,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,52.686,1501,,1503,6939565314243.043,X
3474,122891,python_function,nn.Module: Dropout_24,122891,104.192,1490,24,1504,6939565314303.297,X
3475,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,102.329,1504,,1505,6939565314304.57,X
3476,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.421,1505,,1506,6939565314305.044,X
3477,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,97.701,1505,,1507,6939565314308.859,X
3478,122891,python_function,torch/nn/functional.py(1401): dropout,122891,95.97,1507,,1508,6939565314310.303,X
3479,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.165,1508,,1509,6939565314310.512,X
3480,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.422,1508,,1510,6939565314316.027,X
3481,122891,python_function,<built-in function getattr>,122891,0.427,1510,,1511,6939565314316.94,X
3482,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,88.222,1508,,1512,6939565314317.844,X
3483,122891,python_function,nn.Module: Linear_15,122891,137.019,1490,15,1513,6939565314418.384,X
3484,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,135.06,1513,,1514,6939565314419.751,X
3485,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.072,1514,,1515,6939565314420.179,X
3486,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,130.887,1514,,1516,6939565314423.566,X
3487,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.0,1516,,1517,6939565314425.578,X
3488,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.536,1516,,1518,6939565314427.742,X
3489,122891,python_function,<built-in function linear>,122891,125.733,1516,,1519,6939565314428.485,X
3490,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.198,1490,,1520,6939565314558.978,X
3491,122891,python_function,nn.Module: Dropout_25,122891,97.193,1490,25,1521,6939565314565.178,X
3492,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,95.331,1521,,1522,6939565314566.474,X
3493,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.224,1522,,1523,6939565314567.066,X
3494,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,90.838,1522,,1524,6939565314570.685,X
3495,122891,python_function,torch/nn/functional.py(1401): dropout,122891,89.666,1524,,1525,6939565314571.577,X
3496,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.276,1525,,1526,6939565314571.833,X
3497,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.07,1525,,1527,6939565314575.852,X
3498,122891,python_function,<built-in function getattr>,122891,0.288,1527,,1528,6939565314576.547,X
3499,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,83.722,1525,,1529,6939565314577.298,X
3500,122891,python_function,nn.Module: LayerNorm_18,122891,119.453,1309,18,1530,6939565314717.364,X
3501,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,117.46,1530,,1531,6939565314718.851,X
3502,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.156,1531,,1532,6939565314719.338,X
3503,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,112.336,1531,,1533,6939565314723.646,X
3504,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.991,1533,,1534,6939565314726.424,X
3505,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.513,1533,,1535,6939565314728.651,X
3506,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,105.782,1533,,1536,6939565314729.962,X
3507,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.433,1536,,1537,6939565314730.372,X
3508,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.43,1536,,1538,6939565314733.65,X
3509,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.578,1538,,1539,6939565314734.214,X
3510,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,100.018,1536,,1540,6939565314735.563,X
3511,122891,python_function,nn.Module: TransformerDecoderLayer_2,122891,3241.965,1050,2,1541,6939565314849.337,X
3512,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,3238.095,1541,,1542,6939565314851.858,X
3513,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.08,1542,,1543,6939565314852.32,X
3514,122891,python_function,torch/nn/modules/transformer.py(1031): forward,122891,3232.199,1542,,1544,6939565314856.468,X
3515,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.533,1544,,1545,6939565314858.938,X
3516,122891,python_function,torch/nn/modules/transformer.py(1100): _sa_block,122891,978.379,1544,,1546,6939565314862.425,X
3517,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.635,1546,,1547,6939565314864.13,X
3518,122891,python_function,nn.Module: MultiheadAttention_10,122891,834.938,1546,10,1548,6939565314875.73,X
3519,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,831.028,1548,,1549,6939565314878.153,X
3520,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.598,1549,,1550,6939565314878.502,X
3521,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,825.851,1549,,1551,6939565314882.034,X
3522,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.683,1551,,1552,6939565314883.274,X
3523,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.519,1551,,1553,6939565314886.014,X
3524,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.412,1551,,1554,6939565314888.218,X
3525,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.252,1551,,1555,6939565314889.739,X
3526,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.157,1551,,1556,6939565314891.667,X
3527,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.292,1556,,1557,6939565314893.274,X
3528,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.723,1551,,1558,6939565314896.049,X
3529,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.516,1551,,1559,6939565314898.505,X
3530,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.564,1551,,1560,6939565314901.075,X
3531,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.631,1551,,1561,6939565314903.301,X
3532,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.572,1551,,1562,6939565314908.852,X
3533,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.522,1551,,1563,6939565314911.279,X
3534,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.895,1551,,1564,6939565314914.353,X
3535,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.803,1551,,1565,6939565314918.974,X
3536,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.194,1551,,1566,6939565314921.546,X
3537,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.746,1551,,1567,6939565314924.508,X
3538,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,777.457,1551,,1568,6939565314927.267,X
3539,122891,python_function,<built-in function _has_torch_function>,122891,0.754,1568,,1569,6939565314928.336,X
3540,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.864,1568,,1570,6939565314930.327,X
3541,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.331,1570,,1571,6939565314930.805,X
3542,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.347,1570,,1572,6939565314931.689,X
3543,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.183,1570,,1573,6939565314932.388,X
3544,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.14,1568,,1574,6939565314937.721,X
3545,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.226,1568,,1575,6939565314938.852,X
3546,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.213,1568,,1576,6939565314940.389,X
3547,122891,python_function,<built-in function isinstance>,122891,0.739,1568,,1577,6939565314942.068,X
3548,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,338.508,1568,,1578,6939565314946.476,X
3549,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,2.207,1578,,1579,6939565314947.098,X
3550,122891,python_function,<built-in function linear>,122891,125.805,1578,,1580,6939565314949.944,X
3551,122891,python_function,torch/_tensor.py(1356): unflatten,122891,26.745,1578,,1581,6939565315077.39,X
3552,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.454,1581,,1582,6939565315078.178,X
3553,122891,python_function,<built-in function isinstance>,122891,0.306,1581,,1583,6939565315079.567,X
3554,122891,python_function,<built-in function isinstance>,122891,0.072,1581,,1584,6939565315080.551,X
3555,122891,python_function,<built-in function isinstance>,122891,0.171,1581,,1585,6939565315081.265,X
3556,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,20.228,1581,,1586,6939565315083.55,X
3557,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,19.47,1578,,1587,6939565315104.823,X
3558,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.923,1578,,1588,6939565315126.742,X
3559,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,13.902,1578,,1589,6939565315141.107,X
3560,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,75.268,1578,,1590,6939565315156.291,X
3561,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.898,1568,,1591,6939565315288.005,X
3562,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.086,1568,,1592,6939565315299.418,X
3563,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.43,1568,,1593,6939565315316.205,X
3564,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.431,1568,,1594,6939565315325.065,X
3565,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.474,1568,,1595,6939565315337.898,X
3566,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.585,1568,,1596,6939565315345.851,X
3567,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.899,1568,,1597,6939565315358.125,X
3568,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.456,1568,,1598,6939565315359.925,X
3569,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.778,1568,,1599,6939565315372.625,X
3570,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.399,1568,,1600,6939565315382.447,X
3571,122891,python_function,<built-in function scaled_dot_product_attention>,122891,160.842,1568,,1601,6939565315394.141,X
3572,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,17.024,1568,,1602,6939565315556.043,X
3573,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.561,1568,,1603,6939565315573.713,X
3574,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.012,1568,,1604,6939565315574.765,X
3575,122891,python_function,<built-in function linear>,122891,101.565,1568,,1605,6939565315585.645,X
3576,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.169,1568,,1606,6939565315688.345,X
3577,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,13.911,1568,,1607,6939565315689.934,X
3578,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.563,1546,,1608,6939565315717.466,X
3579,122891,python_function,nn.Module: Dropout_26,122891,112.399,1546,26,1609,6939565315727.825,X
3580,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,109.826,1609,,1610,6939565315729.763,X
3581,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.081,1610,,1611,6939565315730.9,X
3582,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,101.648,1610,,1612,6939565315737.582,X
3583,122891,python_function,torch/nn/functional.py(1401): dropout,122891,99.457,1612,,1613,6939565315739.473,X
3584,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.321,1613,,1614,6939565315740.123,X
3585,122891,python_function,torch/_VF.py(27): __getattr__,122891,2.09,1613,,1615,6939565315747.058,X
3586,122891,python_function,<built-in function getattr>,122891,0.769,1615,,1616,6939565315748.196,X
3587,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,88.97,1613,,1617,6939565315749.747,X
3588,122891,python_function,nn.Module: LayerNorm_19,122891,115.559,1544,19,1618,6939565315901.341,X
3589,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,113.645,1618,,1619,6939565315902.753,X
3590,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.662,1619,,1620,6939565315903.374,X
3591,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,108.243,1619,,1621,6939565315907.836,X
3592,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.034,1621,,1622,6939565315910.651,X
3593,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.452,1621,,1623,6939565315912.892,X
3594,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,101.584,1621,,1624,6939565315914.252,X
3595,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.471,1624,,1625,6939565315914.831,X
3596,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.501,1624,,1626,6939565315918.584,X
3597,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.699,1626,,1627,6939565315919.3,X
3598,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,94.9,1624,,1628,6939565315920.773,X
3599,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.365,1544,,1629,6939565316019.978,X
3600,122891,python_function,torch/nn/modules/transformer.py(1119): _mha_block,122891,1107.867,1544,,1630,6939565316023.568,X
3601,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.254,1630,,1631,6939565316025.157,X
3602,122891,python_function,nn.Module: MultiheadAttention_11,122891,962.164,1630,11,1632,6939565316036.696,X
3603,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,958.308,1632,,1633,6939565316039.117,X
3604,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.105,1633,,1634,6939565316039.697,X
3605,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,951.995,1633,,1635,6939565316044.247,X
3606,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.506,1635,,1636,6939565316045.044,X
3607,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.452,1635,,1637,6939565316047.21,X
3608,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.393,1635,,1638,6939565316049.455,X
3609,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.227,1635,,1639,6939565316051.223,X
3610,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.103,1635,,1640,6939565316052.76,X
3611,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.286,1640,,1641,6939565316054.28,X
3612,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.665,1635,,1642,6939565316059.782,X
3613,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.615,1635,,1643,6939565316062.219,X
3614,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.731,1635,,1644,6939565316065.204,X
3615,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.176,1635,,1645,6939565316069.14,X
3616,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.14,1635,,1646,6939565316072.083,X
3617,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.785,1635,,1647,6939565316075.202,X
3618,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,914.995,1635,,1648,6939565316078.002,X
3619,122891,python_function,<built-in function _has_torch_function>,122891,0.764,1648,,1649,6939565316078.864,X
3620,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.493,1648,,1650,6939565316080.702,X
3621,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.269,1650,,1651,6939565316081.092,X
3622,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.27,1650,,1652,6939565316081.911,X
3623,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.188,1650,,1653,6939565316082.568,X
3624,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.177,1648,,1654,6939565316086.797,X
3625,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.238,1648,,1655,6939565316087.949,X
3626,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.206,1648,,1656,6939565316089.461,X
3627,122891,python_function,<built-in function isinstance>,122891,0.637,1648,,1657,6939565316090.953,X
3628,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,484.495,1648,,1658,6939565316095.121,X
3629,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.377,1658,,1659,6939565316095.579,X
3630,122891,python_function,torch/_tensor.py(968): split,122891,39.374,1658,,1660,6939565316098.796,X
3631,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.352,1660,,1661,6939565316099.483,X
3632,122891,python_function,<built-in function isinstance>,122891,0.488,1660,,1662,6939565316100.171,X
3633,122891,python_function,<built-in function isinstance>,122891,0.189,1660,,1663,6939565316101.536,X
3634,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.4,1660,,1664,6939565316106.223,X
3635,122891,python_function,<built-in function getattr>,122891,0.446,1664,,1665,6939565316107.095,X
3636,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,29.979,1660,,1666,6939565316108.013,X
3637,122891,python_function,torch/_tensor.py(968): split,122891,25.465,1658,,1667,6939565316139.475,X
3638,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.317,1667,,1668,6939565316139.795,X
3639,122891,python_function,<built-in function isinstance>,122891,0.336,1667,,1669,6939565316140.373,X
3640,122891,python_function,<built-in function isinstance>,122891,0.177,1667,,1670,6939565316141.146,X
3641,122891,python_function,torch/_VF.py(27): __getattr__,122891,0.842,1667,,1671,6939565316144.362,X
3642,122891,python_function,<built-in function getattr>,122891,0.124,1671,,1672,6939565316144.962,X
3643,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,19.124,1667,,1673,6939565316145.675,X
3644,122891,python_function,<built-in function linear>,122891,127.45,1658,,1674,6939565316165.749,X
3645,122891,python_function,<built-in function linear>,122891,97.191,1658,,1675,6939565316293.841,X
3646,122891,python_function,torch/_tensor.py(1356): unflatten,122891,22.897,1658,,1676,6939565316392.65,X
3647,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.398,1676,,1677,6939565316393.264,X
3648,122891,python_function,<built-in function isinstance>,122891,0.363,1676,,1678,6939565316394.521,X
3649,122891,python_function,<built-in function isinstance>,122891,0.06,1676,,1679,6939565316395.695,X
3650,122891,python_function,<built-in function isinstance>,122891,0.16,1676,,1680,6939565316396.365,X
3651,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,16.983,1676,,1681,6939565316398.333,X
3652,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,16.397,1658,,1682,6939565316416.235,X
3653,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.818,1658,,1683,6939565316434.825,X
3654,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,13.547,1658,,1684,6939565316449.949,X
3655,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,73.301,1658,,1685,6939565316465.014,X
3656,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.995,1648,,1686,6939565316584.56,X
3657,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.922,1648,,1687,6939565316595.045,X
3658,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.51,1648,,1688,6939565316609.51,X
3659,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.445,1648,,1689,6939565316618.456,X
3660,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.757,1648,,1690,6939565316632.251,X
3661,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.997,1648,,1691,6939565316641.412,X
3662,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.984,1648,,1692,6939565316654.238,X
3663,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.988,1648,,1693,6939565316656.113,X
3664,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.77,1648,,1694,6939565316665.257,X
3665,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.874,1648,,1695,6939565316674.089,X
3666,122891,python_function,<built-in function scaled_dot_product_attention>,122891,170.395,1648,,1696,6939565316684.236,X
3667,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,15.626,1648,,1697,6939565316855.79,X
3668,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.625,1648,,1698,6939565316871.955,X
3669,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.605,1648,,1699,6939565316873.084,X
3670,122891,python_function,<built-in function linear>,122891,96.241,1648,,1700,6939565316883.685,X
3671,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.243,1648,,1701,6939565316981.045,X
3672,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.514,1648,,1702,6939565316982.583,X
3673,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.293,1630,,1703,6939565317005.461,X
3674,122891,python_function,nn.Module: Dropout_27,122891,115.735,1630,27,1704,6939565317015.17,X
3675,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,113.116,1704,,1705,6939565317017.204,X
3676,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.857,1705,,1706,6939565317018.547,X
3677,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,104.891,1705,,1707,6939565317025.039,X
3678,122891,python_function,torch/nn/functional.py(1401): dropout,122891,102.598,1707,,1708,6939565317027.034,X
3679,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.384,1708,,1709,6939565317027.637,X
3680,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.654,1708,,1710,6939565317033.555,X
3681,122891,python_function,<built-in function getattr>,122891,0.541,1710,,1711,6939565317034.486,X
3682,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,93.812,1708,,1712,6939565317035.634,X
3683,122891,python_function,nn.Module: LayerNorm_20,122891,122.188,1544,20,1713,6939565317191.985,X
3684,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,120.007,1713,,1714,6939565317193.638,X
3685,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.491,1714,,1715,6939565317194.302,X
3686,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,114.688,1714,,1716,6939565317198.514,X
3687,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.092,1716,,1717,6939565317201.563,X
3688,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.489,1716,,1718,6939565317203.888,X
3689,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,107.439,1716,,1719,6939565317205.468,X
3690,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.495,1719,,1720,6939565317206.002,X
3691,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.51,1719,,1721,6939565317209.913,X
3692,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.707,1721,,1722,6939565317210.611,X
3693,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,100.68,1719,,1723,6939565317212.048,X
3694,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.288,1544,,1724,6939565317317.526,X
3695,122891,python_function,torch/nn/modules/transformer.py(1139): _ff_block,122891,587.698,1544,,1725,6939565317320.802,X
3696,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.552,1725,,1726,6939565317322.258,X
3697,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.264,1725,,1727,6939565317325.036,X
3698,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.14,1725,,1728,6939565317328.023,X
3699,122891,python_function,nn.Module: Linear_16,122891,119.833,1725,16,1729,6939565317339.063,X
3700,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,117.992,1729,,1730,6939565317340.305,X
3701,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.12,1730,,1731,6939565317340.843,X
3702,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,112.81,1730,,1732,6939565317345.07,X
3703,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.635,1732,,1733,6939565317347.279,X
3704,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.492,1732,,1734,6939565317349.086,X
3705,122891,python_function,<built-in function linear>,122891,107.807,1732,,1735,6939565317349.789,X
3706,122891,python_function,torch/nn/functional.py(1693): relu,122891,55.827,1725,,1736,6939565317459.976,X
3707,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.381,1736,,1737,6939565317460.516,X
3708,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,53.45,1736,,1738,6939565317462.087,X
3709,122891,python_function,nn.Module: Dropout_28,122891,120.718,1725,28,1739,6939565317522.845,X
3710,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,118.767,1739,,1740,6939565317524.195,X
3711,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.217,1740,,1741,6939565317524.708,X
3712,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,113.841,1740,,1742,6939565317528.675,X
3713,122891,python_function,torch/nn/functional.py(1401): dropout,122891,112.341,1742,,1743,6939565317529.915,X
3714,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.253,1743,,1744,6939565317530.264,X
3715,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.494,1743,,1745,6939565317535.751,X
3716,122891,python_function,<built-in function getattr>,122891,0.502,1745,,1746,6939565317536.681,X
3717,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,104.215,1743,,1747,6939565317537.813,X
3718,122891,python_function,nn.Module: Linear_17,122891,134.787,1725,17,1748,6939565317659.094,X
3719,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,132.523,1748,,1749,6939565317660.625,X
3720,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.202,1749,,1750,6939565317661.118,X
3721,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,127.949,1749,,1751,6939565317664.789,X
3722,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.225,1751,,1752,6939565317667.147,X
3723,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.573,1751,,1753,6939565317669.601,X
3724,122891,python_function,<built-in function linear>,122891,122.146,1751,,1754,6939565317670.376,X
3725,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.807,1725,,1755,6939565317797.619,X
3726,122891,python_function,nn.Module: Dropout_29,122891,103.081,1725,29,1756,6939565317804.942,X
3727,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,101.194,1756,,1757,6939565317806.264,X
3728,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.127,1757,,1758,6939565317806.826,X
3729,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,96.438,1757,,1759,6939565317810.705,X
3730,122891,python_function,torch/nn/functional.py(1401): dropout,122891,95.081,1759,,1760,6939565317811.804,X
3731,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.413,1760,,1761,6939565317812.1,X
3732,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.358,1760,,1762,6939565317816.556,X
3733,122891,python_function,<built-in function getattr>,122891,0.409,1762,,1763,6939565317817.228,X
3734,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,88.36,1760,,1764,6939565317818.285,X
3735,122891,python_function,nn.Module: LayerNorm_21,122891,122.941,1544,21,1765,6939565317964.551,X
3736,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,120.754,1765,,1766,6939565317966.146,X
3737,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.122,1766,,1767,6939565317966.666,X
3738,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,116.146,1766,,1768,6939565317970.44,X
3739,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.915,1768,,1769,6939565317973.366,X
3740,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.514,1768,,1770,6939565317975.529,X
3741,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,109.361,1768,,1771,6939565317976.951,X
3742,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.425,1771,,1772,6939565317977.363,X
3743,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.394,1771,,1773,6939565317980.774,X
3744,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.705,1773,,1774,6939565317981.351,X
3745,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,103.506,1771,,1775,6939565317982.654,X
3746,122891,python_function,nn.Module: TransformerDecoderLayer_3,122891,3229.406,1050,3,1776,6939565318100.148,X
3747,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,3225.316,1776,,1777,6939565318102.932,X
3748,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.923,1777,,1778,6939565318103.418,X
3749,122891,python_function,torch/nn/modules/transformer.py(1031): forward,122891,3219.824,1777,,1779,6939565318107.477,X
3750,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.254,1779,,1780,6939565318110.096,X
3751,122891,python_function,torch/nn/modules/transformer.py(1100): _sa_block,122891,934.788,1779,,1781,6939565318113.369,X
3752,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.5,1781,,1782,6939565318114.735,X
3753,122891,python_function,nn.Module: MultiheadAttention_12,122891,780.767,1781,12,1783,6939565318125.644,X
3754,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,776.567,1783,,1784,6939565318128.282,X
3755,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.613,1784,,1785,6939565318128.7,X
3756,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,770.873,1784,,1786,6939565318132.438,X
3757,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.719,1786,,1787,6939565318133.72,X
3758,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.491,1786,,1788,6939565318136.68,X
3759,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.41,1786,,1789,6939565318139.07,X
3760,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.245,1786,,1790,6939565318140.694,X
3761,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.365,1786,,1791,6939565318142.467,X
3762,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.278,1791,,1792,6939565318144.202,X
3763,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.675,1786,,1793,6939565318146.677,X
3764,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.518,1786,,1794,6939565318149.055,X
3765,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.589,1786,,1795,6939565318152.316,X
3766,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.587,1786,,1796,6939565318154.642,X
3767,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.581,1786,,1797,6939565318160.231,X
3768,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.515,1786,,1798,6939565318162.703,X
3769,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.056,1786,,1799,6939565318165.629,X
3770,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.183,1786,,1800,6939565318170.381,X
3771,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.222,1786,,1801,6939565318173.418,X
3772,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.626,1786,,1802,6939565318176.408,X
3773,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,720.294,1786,,1803,6939565318179.118,X
3774,122891,python_function,<built-in function _has_torch_function>,122891,0.79,1803,,1804,6939565318180.176,X
3775,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.564,1803,,1805,6939565318182.055,X
3776,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.249,1805,,1806,6939565318182.561,X
3777,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.213,1805,,1807,6939565318183.392,X
3778,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.182,1805,,1808,6939565318183.997,X
3779,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.21,1803,,1809,6939565318188.458,X
3780,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.246,1803,,1810,6939565318189.704,X
3781,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.238,1803,,1811,6939565318191.187,X
3782,122891,python_function,<built-in function isinstance>,122891,0.723,1803,,1812,6939565318192.736,X
3783,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,334.52,1803,,1813,6939565318197.629,X
3784,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,2.204,1813,,1814,6939565318198.161,X
3785,122891,python_function,<built-in function linear>,122891,133.066,1813,,1815,6939565318200.99,X
3786,122891,python_function,torch/_tensor.py(1356): unflatten,122891,25.773,1813,,1816,6939565318335.367,X
3787,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.491,1816,,1817,6939565318336.075,X
3788,122891,python_function,<built-in function isinstance>,122891,0.428,1816,,1818,6939565318337.459,X
3789,122891,python_function,<built-in function isinstance>,122891,0.076,1816,,1819,6939565318338.626,X
3790,122891,python_function,<built-in function isinstance>,122891,0.194,1816,,1820,6939565318339.411,X
3791,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,19.227,1816,,1821,6939565318341.648,X
3792,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,15.803,1813,,1822,6939565318361.984,X
3793,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.056,1813,,1823,6939565318379.94,X
3794,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,14.589,1813,,1824,6939565318395.445,X
3795,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,75.766,1813,,1825,6939565318411.256,X
3796,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.183,1803,,1826,6939565318534.773,X
3797,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,9.974,1803,,1827,6939565318544.376,X
3798,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.183,1803,,1828,6939565318557.218,X
3799,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,7.544,1803,,1829,6939565318564.781,X
3800,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,6.077,1803,,1830,6939565318574.028,X
3801,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,9.274,1803,,1831,6939565318580.419,X
3802,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.742,1803,,1832,6939565318590.903,X
3803,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,6.227,1803,,1833,6939565318592.459,X
3804,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.214,1803,,1834,6939565318599.547,X
3805,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,5.983,1803,,1835,6939565318607.53,X
3806,122891,python_function,<built-in function scaled_dot_product_attention>,122891,131.418,1803,,1836,6939565318614.48,X
3807,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,13.199,1803,,1837,6939565318746.981,X
3808,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.51,1803,,1838,6939565318760.628,X
3809,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.441,1803,,1839,6939565318761.509,X
3810,122891,python_function,<built-in function linear>,122891,110.379,1803,,1840,6939565318771.587,X
3811,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.647,1803,,1841,6939565318883.413,X
3812,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,12.807,1803,,1842,6939565318885.462,X
3813,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.921,1781,,1843,6939565318915.016,X
3814,122891,python_function,nn.Module: Dropout_30,122891,121.316,1781,30,1844,6939565318926.256,X
3815,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,118.667,1844,,1845,6939565318928.3,X
3816,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.482,1845,,1846,6939565318930.026,X
3817,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,109.176,1845,,1847,6939565318937.436,X
3818,122891,python_function,torch/nn/functional.py(1401): dropout,122891,106.636,1847,,1848,6939565318939.513,X
3819,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.495,1848,,1849,6939565318940.221,X
3820,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.994,1848,,1850,6939565318947.778,X
3821,122891,python_function,<built-in function getattr>,122891,0.725,1850,,1851,6939565318948.936,X
3822,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,95.606,1848,,1852,6939565318950.189,X
3823,122891,python_function,nn.Module: LayerNorm_22,122891,119.218,1779,22,1853,6939565319110.95,X
3824,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,116.89,1853,,1854,6939565319112.774,X
3825,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.643,1854,,1855,6939565319113.339,X
3826,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,111.464,1854,,1856,6939565319117.87,X
3827,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.062,1856,,1857,6939565319120.842,X
3828,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.52,1856,,1858,6939565319123.129,X
3829,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,104.358,1856,,1859,6939565319124.556,X
3830,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.458,1859,,1860,6939565319125.139,X
3831,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.661,1859,,1861,6939565319129.141,X
3832,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.675,1861,,1862,6939565319129.884,X
3833,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,97.305,1859,,1863,6939565319131.391,X
3834,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.146,1779,,1864,6939565319233.188,X
3835,122891,python_function,torch/nn/modules/transformer.py(1119): _mha_block,122891,1109.812,1779,,1865,6939565319236.57,X
3836,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.287,1865,,1866,6939565319238.109,X
3837,122891,python_function,nn.Module: MultiheadAttention_13,122891,966.343,1865,13,1867,6939565319250.142,X
3838,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,962.682,1867,,1868,6939565319252.479,X
3839,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.112,1868,,1869,6939565319253.009,X
3840,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,956.37,1868,,1870,6939565319257.879,X
3841,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.514,1870,,1871,6939565319258.735,X
3842,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.538,1870,,1872,6939565319261.079,X
3843,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.41,1870,,1873,6939565319263.394,X
3844,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.266,1870,,1874,6939565319264.914,X
3845,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.328,1870,,1875,6939565319266.688,X
3846,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.296,1875,,1876,6939565319268.417,X
3847,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.673,1870,,1877,6939565319274.17,X
3848,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.602,1870,,1878,6939565319276.753,X
3849,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.938,1870,,1879,6939565319279.603,X
3850,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.112,1870,,1880,6939565319284.17,X
3851,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.224,1870,,1881,6939565319287.071,X
3852,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.588,1870,,1882,6939565319290.091,X
3853,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,918.564,1870,,1883,6939565319292.68,X
3854,122891,python_function,<built-in function _has_torch_function>,122891,0.744,1883,,1884,6939565319293.633,X
3855,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.734,1883,,1885,6939565319295.428,X
3856,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.253,1885,,1886,6939565319295.88,X
3857,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.331,1885,,1887,6939565319296.835,X
3858,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.24,1885,,1888,6939565319297.546,X
3859,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.218,1883,,1889,6939565319302.19,X
3860,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.261,1883,,1890,6939565319303.349,X
3861,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.226,1883,,1891,6939565319304.889,X
3862,122891,python_function,<built-in function isinstance>,122891,0.646,1883,,1892,6939565319306.508,X
3863,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,498.393,1883,,1893,6939565319310.978,X
3864,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.356,1893,,1894,6939565319311.611,X
3865,122891,python_function,torch/_tensor.py(968): split,122891,39.898,1893,,1895,6939565319314.79,X
3866,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.301,1895,,1896,6939565319315.664,X
3867,122891,python_function,<built-in function isinstance>,122891,0.823,1895,,1897,6939565319316.424,X
3868,122891,python_function,<built-in function isinstance>,122891,0.17,1895,,1898,6939565319318.196,X
3869,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.307,1895,,1899,6939565319322.735,X
3870,122891,python_function,<built-in function getattr>,122891,0.382,1899,,1900,6939565319323.561,X
3871,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,30.037,1895,,1901,6939565319324.486,X
3872,122891,python_function,torch/_tensor.py(968): split,122891,25.422,1893,,1902,6939565319356.025,X
3873,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.291,1902,,1903,6939565319356.34,X
3874,122891,python_function,<built-in function isinstance>,122891,0.446,1902,,1904,6939565319356.954,X
3875,122891,python_function,<built-in function isinstance>,122891,0.191,1902,,1905,6939565319358.046,X
3876,122891,python_function,torch/_VF.py(27): __getattr__,122891,0.815,1902,,1906,6939565319361.24,X
3877,122891,python_function,<built-in function getattr>,122891,0.115,1906,,1907,6939565319361.869,X
3878,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,18.927,1902,,1908,6939565319362.378,X
3879,122891,python_function,<built-in function linear>,122891,127.089,1893,,1909,6939565319382.298,X
3880,122891,python_function,<built-in function linear>,122891,97.766,1893,,1910,6939565319509.981,X
3881,122891,python_function,torch/_tensor.py(1356): unflatten,122891,23.805,1893,,1911,6939565319609.218,X
3882,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.357,1911,,1912,6939565319609.922,X
3883,122891,python_function,<built-in function isinstance>,122891,0.267,1911,,1913,6939565319611.121,X
3884,122891,python_function,<built-in function isinstance>,122891,0.08,1911,,1914,6939565319611.941,X
3885,122891,python_function,<built-in function isinstance>,122891,0.159,1911,,1915,6939565319612.646,X
3886,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,17.886,1911,,1916,6939565319614.87,X
3887,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,15.39,1893,,1917,6939565319633.772,X
3888,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,24.929,1893,,1918,6939565319651.498,X
3889,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,14.79,1893,,1919,6939565319677.744,X
3890,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,75.733,1893,,1920,6939565319693.678,X
3891,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.488,1883,,1921,6939565319814.661,X
3892,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.584,1883,,1922,6939565319825.749,X
3893,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.719,1883,,1923,6939565319842.387,X
3894,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.056,1883,,1924,6939565319850.571,X
3895,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.556,1883,,1925,6939565319863.075,X
3896,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,9.798,1883,,1926,6939565319871.101,X
3897,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.049,1883,,1927,6939565319882.578,X
3898,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.007,1883,,1928,6939565319884.507,X
3899,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.328,1883,,1929,6939565319893.817,X
3900,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.917,1883,,1930,6939565319904.271,X
3901,122891,python_function,<built-in function scaled_dot_product_attention>,122891,154.136,1883,,1931,6939565319913.641,X
3902,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,17.172,1883,,1932,6939565320069.116,X
3903,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.667,1883,,1933,6939565320086.782,X
3904,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.131,1883,,1934,6939565320087.893,X
3905,122891,python_function,<built-in function linear>,122891,95.178,1883,,1935,6939565320100.921,X
3906,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.329,1883,,1936,6939565320197.26,X
3907,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.39,1883,,1937,6939565320198.996,X
3908,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.155,1865,,1938,6939565320223.977,X
3909,122891,python_function,nn.Module: Dropout_31,122891,111.593,1865,31,1939,6939565320234.215,X
3910,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,109.079,1939,,1940,6939565320236.174,X
3911,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.271,1940,,1941,6939565320237.355,X
3912,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,100.233,1940,,1942,6939565320244.615,X
3913,122891,python_function,torch/nn/functional.py(1401): dropout,122891,98.186,1942,,1943,6939565320246.376,X
3914,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.269,1943,,1944,6939565320246.956,X
3915,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.756,1943,,1945,6939565320252.698,X
3916,122891,python_function,<built-in function getattr>,122891,0.62,1945,,1946,6939565320253.643,X
3917,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,89.311,1943,,1947,6939565320255.031,X
3918,122891,python_function,nn.Module: LayerNorm_23,122891,118.604,1779,23,1948,6939565320409.77,X
3919,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,116.609,1948,,1949,6939565320411.333,X
3920,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.498,1949,,1950,6939565320411.986,X
3921,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,111.233,1949,,1951,6939565320416.302,X
3922,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.999,1951,,1952,6939565320419.139,X
3923,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.525,1951,,1953,6939565320421.414,X
3924,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,104.418,1951,,1954,6939565320422.849,X
3925,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.44,1954,,1955,6939565320423.375,X
3926,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.54,1954,,1956,6939565320426.982,X
3927,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.69,1956,,1957,6939565320427.741,X
3928,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,97.783,1954,,1958,6939565320429.279,X
3929,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.088,1779,,1959,6939565320531.586,X
3930,122891,python_function,torch/nn/modules/transformer.py(1139): _ff_block,122891,621.287,1779,,1960,6939565320534.642,X
3931,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.52,1960,,1961,6939565320536.3,X
3932,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.395,1960,,1962,6939565320539.027,X
3933,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.277,1960,,1963,6939565320541.964,X
3934,122891,python_function,nn.Module: Linear_18,122891,118.918,1960,18,1964,6939565320551.862,X
3935,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,117.161,1964,,1965,6939565320553.069,X
3936,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.047,1965,,1966,6939565320553.482,X
3937,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,112.375,1965,,1967,6939565320557.435,X
3938,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.599,1967,,1968,6939565320559.688,X
3939,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.567,1967,,1969,6939565320561.419,X
3940,122891,python_function,<built-in function linear>,122891,107.402,1967,,1970,6939565320562.185,X
3941,122891,python_function,torch/nn/functional.py(1693): relu,122891,55.902,1960,,1971,6939565320671.941,X
3942,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.385,1971,,1972,6939565320672.375,X
3943,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,53.708,1971,,1973,6939565320673.852,X
3944,122891,python_function,nn.Module: Dropout_32,122891,97.313,1960,32,1974,6939565320734.926,X
3945,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,95.485,1974,,1975,6939565320736.241,X
3946,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.644,1975,,1976,6939565320736.71,X
3947,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,90.592,1975,,1977,6939565320740.84,X
3948,122891,python_function,torch/nn/functional.py(1401): dropout,122891,88.895,1977,,1978,6939565320742.112,X
3949,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.153,1978,,1979,6939565320742.399,X
3950,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.346,1978,,1980,6939565320747.991,X
3951,122891,python_function,<built-in function getattr>,122891,0.408,1980,,1981,6939565320748.851,X
3952,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,81.045,1978,,1982,6939565320749.753,X
3953,122891,python_function,nn.Module: Linear_19,122891,123.962,1960,19,1983,6939565320853.316,X
3954,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,122.014,1983,,1984,6939565320854.523,X
3955,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.111,1984,,1985,6939565320854.932,X
3956,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,117.581,1984,,1986,6939565320858.615,X
3957,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.097,1986,,1987,6939565320860.638,X
3958,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.494,1986,,1988,6939565320862.903,X
3959,122891,python_function,<built-in function linear>,122891,112.364,1986,,1989,6939565320863.605,X
3960,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.268,1960,,1990,6939565320980.651,X
3961,122891,python_function,nn.Module: Dropout_33,122891,168.762,1960,33,1991,6939565320986.667,X
3962,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,167.215,1991,,1992,6939565320987.605,X
3963,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.218,1992,,1993,6939565320988.066,X
3964,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,163.166,1992,,1994,6939565320991.362,X
3965,122891,python_function,torch/nn/functional.py(1401): dropout,122891,161.999,1994,,1995,6939565320992.229,X
3966,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.211,1995,,1996,6939565320992.477,X
3967,122891,python_function,torch/_VF.py(27): __getattr__,122891,0.863,1995,,1997,6939565320996.607,X
3968,122891,python_function,<built-in function getattr>,122891,0.167,1997,,1998,6939565320997.223,X
3969,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,156.123,1995,,1999,6939565320997.857,X
3970,122891,python_function,nn.Module: LayerNorm_24,122891,112.791,1779,24,2000,6939565321213.357,X
3971,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,110.83,2000,,2001,6939565321214.795,X
3972,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.228,2001,,2002,6939565321215.418,X
3973,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,105.919,2001,,2003,6939565321219.335,X
3974,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.965,2003,,2004,6939565321222.5,X
3975,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.466,2003,,2005,6939565321224.728,X
3976,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,99.038,2003,,2006,6939565321225.924,X
3977,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.459,2006,,2007,6939565321226.343,X
3978,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.475,2006,,2008,6939565321230.016,X
3979,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.579,2008,,2009,6939565321230.619,X
3980,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,92.747,2006,,2010,6939565321232.047,X
3981,122891,python_function,nn.Module: TransformerDecoderLayer_4,122891,3239.059,1050,4,2011,6939565321338.673,X
3982,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,3235.238,2011,,2012,6939565321341.21,X
3983,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.009,2012,,2013,6939565321341.65,X
3984,122891,python_function,torch/nn/modules/transformer.py(1031): forward,122891,3229.778,2012,,2014,6939565321345.546,X
3985,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.399,2014,,2015,6939565321348.167,X
3986,122891,python_function,torch/nn/modules/transformer.py(1100): _sa_block,122891,1021.314,2014,,2016,6939565321351.547,X
3987,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.461,2016,,2017,6939565321352.926,X
3988,122891,python_function,nn.Module: MultiheadAttention_14,122891,871.982,2016,14,2018,6939565321364.657,X
3989,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,867.429,2018,,2019,6939565321367.224,X
3990,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.69,2019,,2020,6939565321367.624,X
3991,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,861.855,2019,,2021,6939565321371.475,X
3992,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.654,2021,,2022,6939565321372.505,X
3993,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.557,2021,,2023,6939565321375.18,X
3994,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.412,2021,,2024,6939565321377.484,X
3995,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.255,2021,,2025,6939565321379.006,X
3996,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.228,2021,,2026,6939565321380.632,X
3997,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.283,2026,,2027,6939565321382.281,X
3998,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.676,2021,,2028,6939565321384.948,X
3999,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.539,2021,,2029,6939565321387.361,X
4000,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.661,2021,,2030,6939565321390.008,X
4001,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.626,2021,,2031,6939565321392.326,X
4002,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.613,2021,,2032,6939565321397.721,X
4003,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.561,2021,,2033,6939565321400.156,X
4004,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.804,2021,,2034,6939565321403.172,X
4005,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.114,2021,,2035,6939565321407.529,X
4006,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.18,2021,,2036,6939565321410.359,X
4007,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.628,2021,,2037,6939565321413.297,X
4008,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,814.342,2021,,2038,6939565321415.85,X
4009,122891,python_function,<built-in function _has_torch_function>,122891,0.767,2038,,2039,6939565321416.78,X
4010,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.572,2038,,2040,6939565321418.686,X
4011,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.282,2040,,2041,6939565321419.018,X
4012,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.235,2040,,2042,6939565321419.879,X
4013,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.189,2040,,2043,6939565321420.482,X
4014,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.211,2038,,2044,6939565321425.126,X
4015,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.278,2038,,2045,6939565321426.316,X
4016,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.221,2038,,2046,6939565321427.794,X
4017,122891,python_function,<built-in function isinstance>,122891,0.549,2038,,2047,6939565321429.318,X
4018,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,382.742,2038,,2048,6939565321433.699,X
4019,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.994,2048,,2049,6939565321434.453,X
4020,122891,python_function,<built-in function linear>,122891,164.97,2048,,2050,6939565321437.083,X
4021,122891,python_function,torch/_tensor.py(1356): unflatten,122891,28.427,2048,,2051,6939565321603.824,X
4022,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.445,2051,,2052,6939565321604.557,X
4023,122891,python_function,<built-in function isinstance>,122891,0.423,2051,,2053,6939565321605.928,X
4024,122891,python_function,<built-in function isinstance>,122891,0.091,2051,,2054,6939565321607.183,X
4025,122891,python_function,<built-in function isinstance>,122891,0.231,2051,,2055,6939565321607.946,X
4026,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,21.572,2051,,2056,6939565321610.333,X
4027,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,15.235,2048,,2057,6939565321632.955,X
4028,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.377,2048,,2058,6939565321650.448,X
4029,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,15.83,2048,,2059,6939565321666.111,X
4030,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,77.97,2048,,2060,6939565321683.099,X
4031,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.337,2038,,2061,6939565321819.632,X
4032,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.258,2038,,2062,6939565321831.563,X
4033,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.652,2038,,2063,6939565321846.637,X
4034,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.061,2038,,2064,6939565321855.756,X
4035,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.391,2038,,2065,6939565321869.192,X
4036,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.353,2038,,2066,6939565321876.991,X
4037,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.158,2038,,2067,6939565321889.023,X
4038,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.475,2038,,2068,6939565321891.028,X
4039,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.646,2038,,2069,6939565321902.699,X
4040,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.568,2038,,2070,6939565321911.442,X
4041,122891,python_function,<built-in function scaled_dot_product_attention>,122891,163.678,2038,,2071,6939565321920.304,X
4042,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,16.437,2038,,2072,6939565322085.045,X
4043,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.581,2038,,2073,6939565322101.99,X
4044,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.787,2038,,2074,6939565322103.069,X
4045,122891,python_function,<built-in function linear>,122891,102.435,2038,,2075,6939565322113.648,X
4046,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.311,2038,,2076,6939565322217.196,X
4047,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.411,2038,,2077,6939565322218.786,X
4048,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.482,2016,,2078,6939565322244.334,X
4049,122891,python_function,nn.Module: Dropout_34,122891,117.85,2016,34,2079,6939565322254.478,X
4050,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,115.281,2079,,2080,6939565322256.447,X
4051,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.114,2080,,2081,6939565322257.916,X
4052,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,106.721,2080,,2082,6939565322264.627,X
4053,122891,python_function,torch/nn/functional.py(1401): dropout,122891,104.468,2082,,2083,6939565322266.602,X
4054,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.34,2083,,2084,6939565322267.275,X
4055,122891,python_function,torch/_VF.py(27): __getattr__,122891,2.131,2083,,2085,6939565322274.461,X
4056,122891,python_function,<built-in function getattr>,122891,0.712,2085,,2086,6939565322275.577,X
4057,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,93.87,2083,,2087,6939565322277.003,X
4058,122891,python_function,nn.Module: LayerNorm_25,122891,116.663,2014,25,2088,6939565322434.158,X
4059,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,114.521,2088,,2089,6939565322435.731,X
4060,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.646,2089,,2090,6939565322436.356,X
4061,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,109.374,2089,,2091,6939565322440.568,X
4062,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.075,2091,,2092,6939565322443.548,X
4063,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.528,2091,,2093,6939565322445.799,X
4064,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,102.478,2091,,2094,6939565322447.233,X
4065,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.434,2094,,2095,6939565322447.739,X
4066,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.512,2094,,2096,6939565322451.365,X
4067,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.798,2096,,2097,6939565322451.998,X
4068,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,96.116,2094,,2098,6939565322453.4,X
4069,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.775,2014,,2099,6939565322554.149,X
4070,122891,python_function,torch/nn/modules/transformer.py(1119): _mha_block,122891,1128.453,2014,,2100,6939565322557.067,X
4071,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.254,2100,,2101,6939565322558.594,X
4072,122891,python_function,nn.Module: MultiheadAttention_15,122891,974.147,2100,15,2102,6939565322573.921,X
4073,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,970.613,2102,,2103,6939565322576.284,X
4074,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.081,2103,,2104,6939565322576.796,X
4075,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,964.483,2103,,2105,6939565322581.245,X
4076,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.487,2105,,2106,6939565322582.199,X
4077,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.547,2105,,2107,6939565322584.306,X
4078,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.405,2105,,2108,6939565322586.667,X
4079,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.415,2105,,2109,6939565322588.217,X
4080,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,7.662,2105,,2110,6939565322589.945,X
4081,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.276,2110,,2111,6939565322597.013,X
4082,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.664,2105,,2112,6939565322602.724,X
4083,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.592,2105,,2113,6939565322605.266,X
4084,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.93,2105,,2114,6939565322608.186,X
4085,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.93,2105,,2115,6939565322612.779,X
4086,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.173,2105,,2116,6939565322615.445,X
4087,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.612,2105,,2117,6939565322618.341,X
4088,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,922.075,2105,,2118,6939565322620.751,X
4089,122891,python_function,<built-in function _has_torch_function>,122891,0.791,2118,,2119,6939565322621.52,X
4090,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.474,2118,,2120,6939565322623.431,X
4091,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.292,2120,,2121,6939565322623.825,X
4092,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.282,2120,,2122,6939565322624.673,X
4093,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.174,2120,,2123,6939565322625.335,X
4094,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.225,2118,,2124,6939565322629.668,X
4095,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.248,2118,,2125,6939565322630.874,X
4096,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.259,2118,,2126,6939565322632.29,X
4097,122891,python_function,<built-in function isinstance>,122891,0.559,2118,,2127,6939565322633.873,X
4098,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,494.648,2118,,2128,6939565322637.795,X
4099,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.355,2128,,2129,6939565322638.268,X
4100,122891,python_function,torch/_tensor.py(968): split,122891,39.324,2128,,2130,6939565322641.253,X
4101,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.254,2130,,2131,6939565322641.866,X
4102,122891,python_function,<built-in function isinstance>,122891,0.331,2130,,2132,6939565322642.502,X
4103,122891,python_function,<built-in function isinstance>,122891,0.168,2130,,2133,6939565322644.073,X
4104,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.392,2130,,2134,6939565322648.315,X
4105,122891,python_function,<built-in function getattr>,122891,0.362,2134,,2135,6939565322649.238,X
4106,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,30.367,2130,,2136,6939565322650.066,X
4107,122891,python_function,torch/_tensor.py(968): split,122891,24.653,2128,,2137,6939565322682.065,X
4108,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.297,2137,,2138,6939565322682.359,X
4109,122891,python_function,<built-in function isinstance>,122891,0.472,2137,,2139,6939565322682.996,X
4110,122891,python_function,<built-in function isinstance>,122891,0.144,2137,,2140,6939565322683.919,X
4111,122891,python_function,torch/_VF.py(27): __getattr__,122891,0.823,2137,,2141,6939565322687.119,X
4112,122891,python_function,<built-in function getattr>,122891,0.18,2141,,2142,6939565322687.694,X
4113,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,18.331,2137,,2143,6939565322688.226,X
4114,122891,python_function,<built-in function linear>,122891,126.779,2128,,2144,6939565322707.495,X
4115,122891,python_function,<built-in function linear>,122891,101.614,2128,,2145,6939565322834.918,X
4116,122891,python_function,torch/_tensor.py(1356): unflatten,122891,23.612,2128,,2146,6939565322938.174,X
4117,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.447,2146,,2147,6939565322938.775,X
4118,122891,python_function,<built-in function isinstance>,122891,0.352,2146,,2148,6939565322940.131,X
4119,122891,python_function,<built-in function isinstance>,122891,0.077,2146,,2149,6939565322941.072,X
4120,122891,python_function,<built-in function isinstance>,122891,0.16,2146,,2150,6939565322941.723,X
4121,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,17.741,2146,,2151,6939565322943.818,X
4122,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,16.475,2128,,2152,6939565322962.547,X
4123,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.049,2128,,2153,6939565322981.219,X
4124,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,15.286,2128,,2154,6939565322996.896,X
4125,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,78.153,2128,,2155,6939565323013.515,X
4126,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.072,2118,,2156,6939565323137.474,X
4127,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.069,2118,,2157,6939565323149.048,X
4128,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.615,2118,,2158,6939565323163.819,X
4129,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.829,2118,,2159,6939565323171.892,X
4130,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.5,2118,,2160,6939565323185.078,X
4131,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.166,2118,,2161,6939565323195.066,X
4132,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.883,2118,,2162,6939565323206.955,X
4133,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.688,2118,,2163,6939565323208.715,X
4134,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.099,2118,,2164,6939565323217.521,X
4135,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.966,2118,,2165,6939565323226.656,X
4136,122891,python_function,<built-in function scaled_dot_product_attention>,122891,165.248,2118,,2166,6939565323236.874,X
4137,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,16.723,2118,,2167,6939565323403.037,X
4138,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.612,2118,,2168,6939565323420.313,X
4139,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.643,2118,,2169,6939565323421.376,X
4140,122891,python_function,<built-in function linear>,122891,97.811,2118,,2170,6939565323431.791,X
4141,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.306,2118,,2171,6939565323530.766,X
4142,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.635,2118,,2172,6939565323532.354,X
4143,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.031,2100,,2173,6939565323555.495,X
4144,122891,python_function,nn.Module: Dropout_35,122891,119.484,2100,35,2174,6939565323565.448,X
4145,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,116.929,2174,,2175,6939565323567.458,X
4146,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.004,2175,,2176,6939565323568.534,X
4147,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,109.017,2175,,2177,6939565323574.969,X
4148,122891,python_function,torch/nn/functional.py(1401): dropout,122891,106.858,2177,,2178,6939565323576.818,X
4149,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.288,2178,,2179,6939565323577.417,X
4150,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.717,2178,,2180,6939565323583.11,X
4151,122891,python_function,<built-in function getattr>,122891,0.61,2180,,2181,6939565323584.026,X
4152,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,98.25,2178,,2182,6939565323585.233,X
4153,122891,python_function,nn.Module: LayerNorm_26,122891,116.669,2014,26,2183,6939565323747.35,X
4154,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,114.709,2183,,2184,6939565323748.795,X
4155,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.645,2184,,2185,6939565323749.42,X
4156,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,109.229,2184,,2186,6939565323753.917,X
4157,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.811,2186,,2187,6939565323756.983,X
4158,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.545,2186,,2188,6939565323758.979,X
4159,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,102.424,2186,,2189,6939565323760.468,X
4160,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.443,2189,,2190,6939565323760.938,X
4161,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.787,2189,,2191,6939565323764.566,X
4162,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.741,2191,,2192,6939565323765.518,X
4163,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,95.736,2189,,2193,6939565323766.967,X
4164,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.003,2014,,2194,6939565323867.58,X
4165,122891,python_function,torch/nn/modules/transformer.py(1139): _ff_block,122891,540.82,2014,,2195,6939565323870.519,X
4166,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.514,2195,,2196,6939565323872.101,X
4167,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.194,2195,,2197,6939565323874.882,X
4168,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.161,2195,,2198,6939565323877.513,X
4169,122891,python_function,nn.Module: Linear_20,122891,118.284,2195,20,2199,6939565323886.502,X
4170,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,116.448,2199,,2200,6939565323887.742,X
4171,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.002,2200,,2201,6939565323888.243,X
4172,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,111.403,2200,,2202,6939565323892.157,X
4173,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.628,2202,,2203,6939565323894.551,X
4174,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.71,2202,,2204,6939565323896.392,X
4175,122891,python_function,<built-in function linear>,122891,106.041,2202,,2205,6939565323897.308,X
4176,122891,python_function,torch/nn/functional.py(1693): relu,122891,56.415,2195,,2206,6939565324006.075,X
4177,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.379,2206,,2207,6939565324006.562,X
4178,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,54.224,2206,,2208,6939565324007.96,X
4179,122891,python_function,nn.Module: Dropout_36,122891,103.365,2195,36,2209,6939565324069.703,X
4180,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,101.434,2209,,2210,6939565324071.03,X
4181,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.319,2210,,2211,6939565324071.485,X
4182,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,96.702,2210,,2212,6939565324075.454,X
4183,122891,python_function,torch/nn/functional.py(1401): dropout,122891,95.256,2212,,2213,6939565324076.647,X
4184,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.229,2213,,2214,6939565324076.858,X
4185,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.481,2213,,2215,6939565324082.964,X
4186,122891,python_function,<built-in function getattr>,122891,0.545,2215,,2216,6939565324083.798,X
4187,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,86.86,2213,,2217,6939565324084.851,X
4188,122891,python_function,nn.Module: Linear_21,122891,126.491,2195,21,2218,6939565324183.914,X
4189,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,124.685,2218,,2219,6939565324185.15,X
4190,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.081,2219,,2220,6939565324185.53,X
4191,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,120.212,2219,,2221,6939565324189.283,X
4192,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.112,2221,,2222,6939565324191.249,X
4193,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.551,2221,,2223,6939565324193.568,X
4194,122891,python_function,<built-in function linear>,122891,114.935,2221,,2224,6939565324194.375,X
4195,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.025,2195,,2225,6939565324313.597,X
4196,122891,python_function,nn.Module: Dropout_37,122891,91.727,2195,37,2226,6939565324319.143,X
4197,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,89.99,2226,,2227,6939565324320.236,X
4198,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.203,2227,,2228,6939565324320.714,X
4199,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,85.927,2227,,2229,6939565324324.005,X
4200,122891,python_function,torch/nn/functional.py(1401): dropout,122891,84.768,2229,,2230,6939565324324.908,X
4201,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.321,2230,,2231,6939565324325.141,X
4202,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.075,2230,,2232,6939565324329.4,X
4203,122891,python_function,<built-in function getattr>,122891,0.339,2232,,2233,6939565324330.075,X
4204,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,78.645,2230,,2234,6939565324330.808,X
4205,122891,python_function,nn.Module: LayerNorm_27,122891,108.909,2014,27,2235,6939565324465.367,X
4206,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,107.047,2235,,2236,6939565324466.731,X
4207,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.149,2236,,2237,6939565324467.425,X
4208,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,102.384,2236,,2238,6939565324471.088,X
4209,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.878,2238,,2239,6939565324473.645,X
4210,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.534,2238,,2240,6939565324475.704,X
4211,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,96.201,2238,,2241,6939565324476.983,X
4212,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.389,2241,,2242,6939565324477.433,X
4213,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.095,2241,,2243,6939565324480.463,X
4214,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.544,2243,,2244,6939565324480.918,X
4215,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,90.896,2241,,2245,6939565324482.106,X
4216,122891,python_function,nn.Module: TransformerDecoderLayer_5,122891,3282.42,1050,5,2246,6939565324587.466,X
4217,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,3278.475,2246,,2247,6939565324589.977,X
4218,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.02,2247,,2248,6939565324590.453,X
4219,122891,python_function,torch/nn/modules/transformer.py(1031): forward,122891,3273.149,2247,,2249,6939565324594.25,X
4220,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.179,2249,,2250,6939565324596.522,X
4221,122891,python_function,torch/nn/modules/transformer.py(1100): _sa_block,122891,1037.129,2249,,2251,6939565324599.701,X
4222,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.394,2251,,2252,6939565324601.123,X
4223,122891,python_function,nn.Module: MultiheadAttention_16,122891,866.786,2251,16,2253,6939565324612.598,X
4224,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,863.174,2253,,2254,6939565324614.669,X
4225,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.665,2254,,2255,6939565324615.061,X
4226,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,858.002,2254,,2256,6939565324618.521,X
4227,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.725,2256,,2257,6939565324619.739,X
4228,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.544,2256,,2258,6939565324622.563,X
4229,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.434,2256,,2259,6939565324624.95,X
4230,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.427,2256,,2260,6939565324626.594,X
4231,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.204,2256,,2261,6939565324628.53,X
4232,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.284,2261,,2262,6939565324630.126,X
4233,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.733,2256,,2263,6939565324632.875,X
4234,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.576,2256,,2264,6939565324635.298,X
4235,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.635,2256,,2265,6939565324637.915,X
4236,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.65,2256,,2266,6939565324640.136,X
4237,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.596,2256,,2267,6939565324645.52,X
4238,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.529,2256,,2268,6939565324647.959,X
4239,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.9,2256,,2269,6939565324650.865,X
4240,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.132,2256,,2270,6939565324655.221,X
4241,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.187,2256,,2271,6939565324658.078,X
4242,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.959,2256,,2272,6939565324661.059,X
4243,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,808.583,2256,,2273,6939565324663.878,X
4244,122891,python_function,<built-in function _has_torch_function>,122891,0.733,2273,,2274,6939565324664.864,X
4245,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.62,2273,,2275,6939565324666.693,X
4246,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.297,2275,,2276,6939565324667.201,X
4247,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.283,2275,,2277,6939565324668.065,X
4248,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.186,2275,,2278,6939565324668.672,X
4249,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.242,2273,,2279,6939565324673.185,X
4250,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.283,2273,,2280,6939565324674.332,X
4251,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.218,2273,,2281,6939565324676.045,X
4252,122891,python_function,<built-in function isinstance>,122891,0.644,2273,,2282,6939565324677.689,X
4253,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,349.361,2273,,2283,6939565324682.162,X
4254,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.862,2283,,2284,6939565324682.692,X
4255,122891,python_function,<built-in function linear>,122891,121.862,2283,,2285,6939565324685.068,X
4256,122891,python_function,torch/_tensor.py(1356): unflatten,122891,37.355,2283,,2286,6939565324808.133,X
4257,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.404,2286,,2287,6939565324808.812,X
4258,122891,python_function,<built-in function isinstance>,122891,0.367,2286,,2288,6939565324810.08,X
4259,122891,python_function,<built-in function isinstance>,122891,0.068,2286,,2289,6939565324819.357,X
4260,122891,python_function,<built-in function isinstance>,122891,0.244,2286,,2290,6939565324820.199,X
4261,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,22.613,2286,,2291,6939565324822.561,X
4262,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,15.393,2283,,2292,6939565324846.236,X
4263,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,12.421,2283,,2293,6939565324863.929,X
4264,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,22.574,2283,,2294,6939565324877.624,X
4265,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,75.862,2283,,2295,6939565324901.402,X
4266,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.47,2273,,2296,6939565325034.594,X
4267,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.123,2273,,2297,6939565325046.586,X
4268,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.914,2273,,2298,6939565325061.365,X
4269,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.125,2273,,2299,6939565325070.702,X
4270,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.492,2273,,2300,6939565325083.165,X
4271,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,10.083,2273,,2301,6939565325092.055,X
4272,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.036,2273,,2302,6939565325103.835,X
4273,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.454,2273,,2303,6939565325105.729,X
4274,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.836,2273,,2304,6939565325118.36,X
4275,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.477,2273,,2305,6939565325128.262,X
4276,122891,python_function,<built-in function scaled_dot_product_attention>,122891,183.738,2273,,2306,6939565325136.996,X
4277,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,17.087,2273,,2307,6939565325321.672,X
4278,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.636,2273,,2308,6939565325339.261,X
4279,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.731,2273,,2309,6939565325340.495,X
4280,122891,python_function,<built-in function linear>,122891,103.945,2273,,2310,6939565325353.074,X
4281,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.309,2273,,2311,6939565325458.292,X
4282,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,11.657,2273,,2312,6939565325459.89,X
4283,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.207,2251,,2313,6939565325486.934,X
4284,122891,python_function,nn.Module: Dropout_38,122891,139.308,2251,38,2314,6939565325496.827,X
4285,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,136.599,2314,,2315,6939565325498.824,X
4286,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.044,2315,,2316,6939565325500.108,X
4287,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,128.354,2315,,2317,6939565325506.631,X
4288,122891,python_function,torch/nn/functional.py(1401): dropout,122891,126.344,2317,,2318,6939565325508.352,X
4289,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.463,2318,,2319,6939565325508.909,X
4290,122891,python_function,torch/_VF.py(27): __getattr__,122891,2.084,2318,,2320,6939565325516.06,X
4291,122891,python_function,<built-in function getattr>,122891,0.783,2320,,2321,6939565325517.171,X
4292,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,115.958,2318,,2322,6939565325518.517,X
4293,122891,python_function,nn.Module: LayerNorm_28,122891,124.405,2249,28,2323,6939565325705.575,X
4294,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,122.147,2323,,2324,6939565325707.309,X
4295,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,2.231,2324,,2325,6939565325707.99,X
4296,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,115.924,2324,,2326,6939565325713.208,X
4297,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.259,2326,,2327,6939565325716.555,X
4298,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.468,2326,,2328,6939565325719.067,X
4299,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,108.06,2326,,2329,6939565325720.798,X
4300,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.468,2329,,2330,6939565325721.332,X
4301,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.547,2329,,2331,6939565325725.266,X
4302,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.675,2331,,2332,6939565325726.038,X
4303,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,101.305,2329,,2333,6939565325727.401,X
4304,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.046,2249,,2334,6939565325833.364,X
4305,122891,python_function,torch/nn/modules/transformer.py(1119): _mha_block,122891,1129.377,2249,,2335,6939565325836.541,X
4306,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.2,2335,,2336,6939565325838.117,X
4307,122891,python_function,nn.Module: MultiheadAttention_17,122891,980.998,2335,17,2337,6939565325849.923,X
4308,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,976.906,2337,,2338,6939565325852.751,X
4309,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.168,2338,,2339,6939565325853.2,X
4310,122891,python_function,torch/nn/modules/activation.py(1134): forward,122891,970.926,2338,,2340,6939565325857.54,X
4311,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.536,2340,,2341,6939565325858.649,X
4312,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.526,2340,,2342,6939565325860.969,X
4313,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.408,2340,,2343,6939565325863.553,X
4314,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.265,2340,,2344,6939565325865.102,X
4315,122891,python_function,torch/backends/mha/__init__.py(9): get_fastpath_enabled,122891,2.176,2340,,2345,6939565325866.662,X
4316,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.3,2345,,2346,6939565325868.235,X
4317,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.674,2340,,2347,6939565325873.853,X
4318,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.586,2340,,2348,6939565325876.418,X
4319,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.922,2340,,2349,6939565325879.437,X
4320,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.099,2340,,2350,6939565325884.084,X
4321,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.155,2340,,2351,6939565325886.968,X
4322,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.743,2340,,2352,6939565325889.855,X
4323,122891,python_function,torch/nn/functional.py(5868): multi_head_attention_forward,122891,932.905,2340,,2353,6939565325892.481,X
4324,122891,python_function,<built-in function _has_torch_function>,122891,0.938,2353,,2354,6939565325893.406,X
4325,122891,python_function,torch/nn/functional.py(5770): _mha_shape_check,122891,2.593,2353,,2355,6939565325895.364,X
4326,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.416,2355,,2356,6939565325895.801,X
4327,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.294,2355,,2357,6939565325896.687,X
4328,122891,python_function,<built-in method dim of Tensor object at 0x7fb80d4c1800>,122891,0.186,2355,,2358,6939565325897.334,X
4329,122891,python_function,torch/nn/functional.py(5860): _none_or_dtype,122891,0.238,2353,,2359,6939565325901.864,X
4330,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.443,2353,,2360,6939565325903.117,X
4331,122891,python_function,torch/nn/functional.py(5832): _canonical_mask,122891,0.207,2353,,2361,6939565325904.786,X
4332,122891,python_function,<built-in function isinstance>,122891,0.614,2353,,2362,6939565325906.492,X
4333,122891,python_function,torch/nn/functional.py(5463): _in_projection_packed,122891,514.444,2353,,2363,6939565325910.904,X
4334,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.44,2363,,2364,6939565325911.44,X
4335,122891,python_function,torch/_tensor.py(968): split,122891,41.405,2363,,2365,6939565325914.581,X
4336,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.272,2365,,2366,6939565325915.234,X
4337,122891,python_function,<built-in function isinstance>,122891,0.352,2365,,2367,6939565325916.091,X
4338,122891,python_function,<built-in function isinstance>,122891,0.237,2365,,2368,6939565325917.458,X
4339,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.791,2365,,2369,6939565325922.82,X
4340,122891,python_function,<built-in function getattr>,122891,0.531,2369,,2370,6939565325923.963,X
4341,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,30.742,2365,,2371,6939565325925.045,X
4342,122891,python_function,torch/_tensor.py(968): split,122891,30.412,2363,,2372,6939565325957.285,X
4343,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.326,2372,,2373,6939565325957.561,X
4344,122891,python_function,<built-in function isinstance>,122891,0.443,2372,,2374,6939565325958.282,X
4345,122891,python_function,<built-in function isinstance>,122891,0.146,2372,,2375,6939565325959.292,X
4346,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.02,2372,,2376,6939565325962.569,X
4347,122891,python_function,<built-in function getattr>,122891,0.109,2376,,2377,6939565325963.184,X
4348,122891,python_function,<built-in method split_with_sizes of type object at 0x7fb97ee5f1c0>,122891,23.681,2372,,2378,6939565325963.853,X
4349,122891,python_function,<built-in function linear>,122891,137.441,2363,,2379,6939565325988.542,X
4350,122891,python_function,<built-in function linear>,122891,100.676,2363,,2380,6939565326126.521,X
4351,122891,python_function,torch/_tensor.py(1356): unflatten,122891,24.494,2363,,2381,6939565326228.579,X
4352,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.364,2381,,2382,6939565326229.189,X
4353,122891,python_function,<built-in function isinstance>,122891,0.369,2381,,2383,6939565326230.418,X
4354,122891,python_function,<built-in function isinstance>,122891,0.311,2381,,2384,6939565326231.358,X
4355,122891,python_function,<built-in function isinstance>,122891,0.177,2381,,2385,6939565326232.407,X
4356,122891,python_function,<built-in method unflatten of Tensor object at 0x7fb80d4fdfd0>,122891,17.889,2381,,2386,6939565326234.884,X
4357,122891,python_function,<built-in method unsqueeze of Tensor object at 0x7fb80d4fe7a0>,122891,16.489,2363,,2387,6939565326253.879,X
4358,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,13.774,2363,,2388,6939565326272.568,X
4359,122891,python_function,<built-in method squeeze of Tensor object at 0x7fb80d4fd850>,122891,15.334,2363,,2389,6939565326287.643,X
4360,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,78.479,2363,,2390,6939565326304.192,X
4361,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.526,2353,,2391,6939565326430.597,X
4362,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.681,2353,,2392,6939565326440.624,X
4363,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.41,2353,,2393,6939565326455.971,X
4364,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.387,2353,,2394,6939565326463.821,X
4365,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.665,2353,,2395,6939565326477.446,X
4366,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,11.264,2353,,2396,6939565326485.483,X
4367,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,0.924,2353,,2397,6939565326498.458,X
4368,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,8.084,2353,,2398,6939565326500.258,X
4369,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,10.012,2353,,2399,6939565326509.522,X
4370,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,7.787,2353,,2400,6939565326520.608,X
4371,122891,python_function,<built-in function scaled_dot_product_attention>,122891,154.594,2353,,2401,6939565326529.726,X
4372,122891,python_function,<built-in method permute of Tensor object at 0x7fb80d4fdfd0>,122891,17.314,2353,,2402,6939565326685.301,X
4373,122891,python_function,<built-in method contiguous of Tensor object at 0x7fb80d4fe7a0>,122891,0.526,2353,,2403,6939565326703.191,X
4374,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.394,2353,,2404,6939565326704.23,X
4375,122891,python_function,<built-in function linear>,122891,96.037,2353,,2405,6939565326715.683,X
4376,122891,python_function,<built-in method size of Tensor object at 0x7fb9945619e0>,122891,1.347,2353,,2406,6939565326812.904,X
4377,122891,python_function,<built-in method view of Tensor object at 0x7fb80d4fdfd0>,122891,9.697,2353,,2407,6939565326814.663,X
4378,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,3.26,2335,,2408,6939565326838.368,X
4379,122891,python_function,nn.Module: Dropout_39,122891,117.075,2335,39,2409,6939565326848.29,X
4380,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,114.34,2409,,2410,6939565326850.367,X
4381,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,3.237,2410,,2411,6939565326851.322,X
4382,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,105.912,2410,,2412,6939565326858.356,X
4383,122891,python_function,torch/nn/functional.py(1401): dropout,122891,103.915,2412,,2413,6939565326860.063,X
4384,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.356,2413,,2414,6939565326860.583,X
4385,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.966,2413,,2415,6939565326866.395,X
4386,122891,python_function,<built-in function getattr>,122891,0.777,2415,,2416,6939565326867.49,X
4387,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,95.018,2413,,2417,6939565326868.753,X
4388,122891,python_function,nn.Module: LayerNorm_29,122891,118.243,2249,29,2418,6939565327026.783,X
4389,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,116.117,2418,,2419,6939565327028.389,X
4390,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.684,2419,,2420,6939565327029.005,X
4391,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,110.877,2419,,2421,6939565327033.276,X
4392,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.077,2421,,2422,6939565327036.01,X
4393,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.546,2421,,2423,6939565327038.42,X
4394,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,104.061,2421,,2424,6939565327039.821,X
4395,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.503,2424,,2425,6939565327040.399,X
4396,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.462,2424,,2426,6939565327044.24,X
4397,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.73,2426,,2427,6939565327044.878,X
4398,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,97.453,2424,,2428,6939565327046.241,X
4399,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.051,2249,,2429,6939565327148.187,X
4400,122891,python_function,torch/nn/modules/transformer.py(1139): _ff_block,122891,554.876,2249,,2430,6939565327151.335,X
4401,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.283,2430,,2431,6939565327152.734,X
4402,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.324,2430,,2432,6939565327155.49,X
4403,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.122,2430,,2433,6939565327158.266,X
4404,122891,python_function,nn.Module: Linear_22,122891,130.026,2430,22,2434,6939565327207.038,X
4405,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,128.08,2434,,2435,6939565327208.432,X
4406,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.046,2435,,2436,6939565327209.024,X
4407,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,123.7,2435,,2437,6939565327212.377,X
4408,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.631,2437,,2438,6939565327214.609,X
4409,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.504,2437,,2439,6939565327216.482,X
4410,122891,python_function,<built-in function linear>,122891,118.679,2437,,2440,6939565327217.203,X
4411,122891,python_function,torch/nn/functional.py(1693): relu,122891,57.828,2430,,2441,6939565327338.487,X
4412,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.479,2441,,2442,6939565327339.119,X
4413,122891,python_function,<built-in method relu of type object at 0x7fb97ee5f1c0>,122891,55.285,2441,,2443,6939565327340.761,X
4414,122891,python_function,nn.Module: Dropout_40,122891,89.907,2430,40,2444,6939565327404.048,X
4415,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,88.146,2444,,2445,6939565327405.318,X
4416,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.395,2445,,2446,6939565327405.795,X
4417,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,83.874,2445,,2447,6939565327409.302,X
4418,122891,python_function,torch/nn/functional.py(1401): dropout,122891,82.33,2447,,2448,6939565327410.571,X
4419,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.184,2448,,2449,6939565327410.766,X
4420,122891,python_function,torch/_VF.py(27): __getattr__,122891,1.333,2448,,2450,6939565327416.415,X
4421,122891,python_function,<built-in function getattr>,122891,0.423,2450,,2451,6939565327417.275,X
4422,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,74.561,2448,,2452,6939565327418.067,X
4423,122891,python_function,nn.Module: Linear_23,122891,112.353,2430,23,2453,6939565327502.638,X
4424,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,110.812,2453,,2454,6939565327503.676,X
4425,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,0.972,2454,,2455,6939565327504.107,X
4426,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,107.222,2454,,2456,6939565327506.88,X
4427,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.056,2456,,2457,6939565327508.529,X
4428,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.323,2456,,2458,6939565327510.418,X
4429,122891,python_function,<built-in function linear>,122891,102.987,2456,,2459,6939565327510.903,X
4430,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,1.909,2430,,2460,6939565327618.678,X
4431,122891,python_function,nn.Module: Dropout_41,122891,81.847,2430,41,2461,6939565327623.924,X
4432,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,80.45,2461,,2462,6939565327624.848,X
4433,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.178,2462,,2463,6939565327625.232,X
4434,122891,python_function,torch/nn/modules/dropout.py(69): forward,122891,76.33,2462,,2464,6939565327628.702,X
4435,122891,python_function,torch/nn/functional.py(1401): dropout,122891,75.08,2464,,2465,6939565327629.687,X
4436,122891,python_function,<built-in function _has_torch_function_unary>,122891,0.282,2465,,2466,6939565327629.995,X
4437,122891,python_function,torch/_VF.py(27): __getattr__,122891,0.918,2465,,2467,6939565327633.672,X
4438,122891,python_function,<built-in function getattr>,122891,0.314,2467,,2468,6939565327634.219,X
4439,122891,python_function,<built-in method dropout of type object at 0x7fb97ee5f1c0>,122891,69.765,2465,,2469,6939565327634.815,X
4440,122891,python_function,nn.Module: LayerNorm_30,122891,110.058,2249,30,2470,6939565327755.798,X
4441,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,108.143,2470,,2471,6939565327756.888,X
4442,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.068,2471,,2472,6939565327757.284,X
4443,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,104.239,2471,,2473,6939565327760.284,X
4444,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.93,2473,,2474,6939565327762.502,X
4445,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.35,2473,,2475,6939565327764.304,X
4446,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,98.896,2473,,2476,6939565327765.299,X
4447,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.3,2476,,2477,6939565327765.638,X
4448,122891,python_function,torch/backends/__init__.py(38): __get__,122891,1.237,2476,,2478,6939565327768.932,X
4449,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.541,2478,,2479,6939565327769.546,X
4450,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,93.337,2476,,2480,6939565327770.628,X
4451,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.598,1050,,2481,6939565327876.408,X
4452,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.994,1050,,2482,6939565327880.506,X
4453,122891,python_function,nn.Module: LayerNorm_31,122891,102.68,1050,31,2483,6939565327888.857,X
4454,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,100.894,2483,,2484,6939565327890.171,X
4455,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.123,2484,,2485,6939565327890.675,X
4456,122891,python_function,torch/nn/modules/normalization.py(216): forward,122891,96.768,2484,,2486,6939565327893.941,X
4457,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.525,2486,,2487,6939565327895.475,X
4458,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.404,2486,,2488,6939565327896.826,X
4459,122891,python_function,torch/nn/functional.py(2879): layer_norm,122891,92.648,2486,,2489,6939565327897.828,X
4460,122891,python_function,<built-in function _has_torch_function_variadic>,122891,0.365,2489,,2490,6939565327898.063,X
4461,122891,python_function,torch/backends/__init__.py(38): __get__,122891,0.569,2489,,2491,6939565327899.688,X
4462,122891,python_function,<built-in function _get_cudnn_enabled>,122891,0.177,2491,,2492,6939565327900.018,X
4463,122891,python_function,<built-in method layer_norm of type object at 0x7fb97ee5f1c0>,122891,89.7,2489,,2493,6939565327900.601,X
4464,122891,python_function,<built-in method transpose of Tensor object at 0x7fb80d4c1990>,122891,14.704,91,,2494,6939565328000.558,X
4465,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,2.014,91,,2495,6939565328019.115,X
4466,122891,python_function,nn.Module: Linear_24,122891,168.939,91,24,2496,6939565328030.55,X
4467,122891,python_function,torch/nn/modules/module.py(1740): _call_impl,122891,166.346,2496,,2497,6939565328032.237,X
4468,122891,python_function,<built-in method _get_tracing_state of PyCapsule object at 0x7fb97f3fbb70>,122891,1.499,2497,,2498,6939565328032.836,X
4469,122891,python_function,torch/nn/modules/linear.py(124): forward,122891,160.153,2497,,2499,6939565328037.747,X
4470,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.564,2499,,2500,6939565328041.187,X
4471,122891,python_function,torch/nn/modules/module.py(1918): __getattr__,122891,0.351,2499,,2501,6939565328042.817,X
4472,122891,python_function,<built-in function linear>,122891,154.256,2499,,2502,6939565328043.38,X
4473,122891,python_function,torch/profiler/profiler.py(766): step,122891,540.662,57,,2503,6939565329813.834,X
4474,122891,python_function,torch/autograd/profiler.py(738): __exit__,122891,178.139,2503,,2504,6939565329819.451,X
4475,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.4,2504,,2505,6939565329828.128,X
4476,122891,python_function,torch/_ops.py(939): __call__,122891,148.326,2504,,2506,6939565329844.214,X
4477,122891,python_function,torch/_ops.py(993): _must_dispatch_in_python,122891,95.554,2506,,2507,6939565329846.73,X
4478,122891,python_function,torch/utils/_pytree.py(1197): tree_any,122891,87.786,2507,,2508,6939565329853.654,X
4479,122891,python_function,<built-in function any>,122891,74.807,2508,,2509,6939565329866.122,X
4480,122891,python_function,torch/utils/_pytree.py(890): tree_iter,122891,40.888,2509,,2510,6939565329868.159,X
4481,122891,python_function,torch/utils/_pytree.py(672): _is_leaf,122891,13.479,2510,,2511,6939565329871.506,X
4482,122891,python_function,torch/utils/_pytree.py(665): _get_node_type,122891,8.823,2511,,2512,6939565329874.719,X
4483,122891,python_function,torch/utils/_pytree.py(654): _is_namedtuple_instance,122891,5.66,2512,,2513,6939565329877.335,X
4484,122891,python_function,<built-in function len>,122891,0.584,2513,,2514,6939565329880.944,X
4485,122891,python_function,torch/utils/_pytree.py(665): _get_node_type,122891,2.16,2510,,2515,6939565329886.005,X
4486,122891,python_function,torch/utils/_pytree.py(654): _is_namedtuple_instance,122891,1.389,2515,,2516,6939565329886.353,X
4487,122891,python_function,<built-in function len>,122891,0.09,2516,,2517,6939565329887.186,X
4488,122891,python_function,torch/utils/_pytree.py(411): _tuple_flatten,122891,1.83,2510,,2518,6939565329892.376,X
4489,122891,python_function,torch/utils/_pytree.py(890): tree_iter,122891,10.94,2510,,2519,6939565329897.935,X
4490,122891,python_function,torch/utils/_pytree.py(672): _is_leaf,122891,2.112,2519,,2520,6939565329898.509,X
4491,122891,python_function,torch/utils/_pytree.py(665): _get_node_type,122891,1.332,2520,,2521,6939565329898.973,X
4492,122891,python_function,torch/utils/_pytree.py(654): _is_namedtuple_instance,122891,0.756,2521,,2522,6939565329899.355,X
4493,122891,python_function,<built-in function len>,122891,0.058,2522,,2523,6939565329899.798,X
4494,122891,python_function,torch/utils/_pytree.py(665): _get_node_type,122891,1.134,2519,,2524,6939565329901.1,X
4495,122891,python_function,torch/utils/_pytree.py(654): _is_namedtuple_instance,122891,0.653,2524,,2525,6939565329901.415,X
4496,122891,python_function,<built-in function len>,122891,0.049,2525,,2526,6939565329901.806,X
4497,122891,python_function,torch/utils/_pytree.py(411): _tuple_flatten,122891,0.59,2519,,2527,6939565329902.891,X
4498,122891,python_function,torch/utils/_pytree.py(890): tree_iter,122891,4.139,2519,,2528,6939565329904.46,X
4499,122891,python_function,torch/utils/_pytree.py(672): _is_leaf,122891,3.496,2528,,2529,6939565329904.82,X
4500,122891,python_function,torch/utils/_pytree.py(665): _get_node_type,122891,2.868,2529,,2530,6939565329905.16,X
4501,122891,python_function,torch/utils/_pytree.py(654): _is_namedtuple_instance,122891,2.387,2530,,2531,6939565329905.447,X
4502,122891,python_function,<built-in function len>,122891,0.115,2531,,2532,6939565329907.277,X
4503,122891,python_function,torch/_ops.py(995): <lambda>,122891,5.016,2509,,2533,6939565329910.125,X
4504,122891,python_function,<built-in function isinstance>,122891,1.188,2533,,2534,6939565329913.825,X
4505,122891,python_function,torch/utils/_pytree.py(904): tree_iter,122891,24.091,2509,,2535,6939565329916.1,X
4506,122891,python_function,torch/utils/_pytree.py(904): tree_iter,122891,1.884,2535,,2536,6939565329916.588,X
4507,122891,python_function,torch/utils/_pytree.py(896): tree_iter,122891,0.193,2536,,2537,6939565329917.251,X
4508,122891,python_function,torch/utils/_pytree.py(890): tree_iter,122891,19.923,2535,,2538,6939565329919.806,X
4509,122891,python_function,torch/utils/_pytree.py(672): _is_leaf,122891,2.21,2538,,2539,6939565329920.242,X
4510,122891,python_function,torch/utils/_pytree.py(665): _get_node_type,122891,1.356,2539,,2540,6939565329920.594,X
4511,122891,python_function,torch/utils/_pytree.py(654): _is_namedtuple_instance,122891,0.886,2540,,2541,6939565329920.871,X
4512,122891,python_function,<built-in function len>,122891,0.054,2541,,2542,6939565329921.394,X
4513,122891,python_function,torch/utils/_pytree.py(665): _get_node_type,122891,1.062,2538,,2543,6939565329922.814,X
4514,122891,python_function,torch/utils/_pytree.py(654): _is_namedtuple_instance,122891,0.591,2543,,2544,6939565329923.097,X
4515,122891,python_function,<built-in function len>,122891,0.054,2544,,2545,6939565329923.428,X
4516,122891,python_function,torch/utils/_pytree.py(439): _dict_flatten,122891,13.747,2538,,2546,6939565329925.471,X
4517,122891,python_function,<built-in method values of collections.defaultdict object at 0x7fb81c7b0ea0>,122891,0.485,2546,,2547,6939565329928.088,X
4518,122891,python_function,<built-in method keys of dict object at 0x7fb800eacc00>,122891,0.588,2546,,2548,6939565329937.681,X
4519,122891,python_function,<built-in method  of PyCapsule object at 0x7fb80d70fbd0>,122891,45.111,2506,,2549,6939565329947.063,X
4520,122891,python_function,<built-in method __exit__ of torch._C.DisableTorchFunctionSubclass object at 0x7fb80412de90>,122891,0.402,2503,,2550,6939565329996.841,X
4521,122891,python_function,torch/profiler/profiler.py(420): schedule_fn,122891,8.13,2503,,2551,6939565330003.567,X
4522,122891,python_function,torch/profiler/profiler.py(789): _transit_action,122891,340.171,2503,,2552,6939565330014.325,X
4523,122891,python_function,<built-in method get of dict object at 0x7fb9945a62c0>,122891,13.029,2552,,2553,6939565330018.927,X
4524,122891,python_function,enum.py(783): __hash__,122891,4.995,2553,,2554,6939565330024.388,X
4525,122891,python_function,<built-in function hash>,122891,0.951,2554,,2555,6939565330028.294,X
4526,122891,python_function,enum.py(783): __hash__,122891,0.844,2553,,2556,6939565330030.133,X
4527,122891,python_function,<built-in function hash>,122891,0.147,2556,,2557,6939565330030.751,X
4528,122891,python_function,torch/profiler/profiler.py(208): stop_trace,122891,319.16,2552,,2558,6939565330035.336,X
4529,122891,python_function,torch/autograd/profiler.py(344): __exit__,122891,313.502,2558,,2559,6939565330040.994,X
4530,122891,python_function,<built-in function hasattr>,122891,0.871,2559,,2560,6939565330043.15,X
4531,122891,python_function,<built-in function getattr>,122891,0.225,2559,,2561,6939565330045.147,X
4532,122891,python_function,<built-in function hasattr>,122891,1.381,2559,,2562,6939565330046.401,X
4533,122891,python_function,torch/cuda/__init__.py(944): synchronize,122891,303.762,2559,,2563,6939565330050.734,X
4534,122891,python_function,torch/cuda/__init__.py(289): _lazy_init,122891,8.994,2563,,2564,6939565330053.863,X
4535,122891,python_function,torch/cuda/__init__.py(242): is_initialized,122891,5.288,2564,,2565,6939565330057.116,X
4536,122891,python_function,<built-in function _cuda_isInBadFork>,122891,1.491,2565,,2566,6939565330060.458,X
4537,122891,python_function,torch/cuda/__init__.py(439): __init__,122891,110.687,2563,,2567,6939565330067.517,X
4538,122891,python_function,torch/cuda/_utils.py(9): _get_device_index,122891,105.504,2567,,2568,6939565330070.952,X
4539,122891,python_function,<built-in function isinstance>,122891,0.508,2568,,2569,6939565330072.201,X
4540,122891,python_function,<built-in function isinstance>,122891,0.084,2568,,2570,6939565330073.763,X
4541,122891,python_function,<built-in function isinstance>,122891,0.085,2568,,2571,6939565330075.88,X
4542,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.154,2568,,2572,6939565330076.889,X
4543,122891,python_function,<built-in function isinstance>,122891,0.114,2568,,2573,6939565330077.897,X
4544,122891,python_function,torch/_utils.py(773): _get_device_index,122891,87.357,2568,,2574,6939565330088.852,X
4545,122891,python_function,<built-in function isinstance>,122891,0.109,2574,,2575,6939565330090.414,X
4546,122891,python_function,<built-in function isinstance>,122891,0.137,2574,,2576,6939565330091.676,X
4547,122891,python_function,<built-in function isinstance>,122891,0.095,2574,,2577,6939565330092.634,X
4548,122891,python_function,torch/_jit_internal.py(103): is_scripting,122891,0.083,2574,,2578,6939565330093.862,X
4549,122891,python_function,torch/_utils.py(747): _get_current_device_index,122891,80.046,2574,,2579,6939565330095.851,X
4550,122891,python_function,torch/_utils.py(733): _get_device_attr,122891,76.103,2579,,2580,6939565330099.324,X
4551,122891,python_function,torch/_utils.py(718): _get_available_device_type,122891,47.097,2580,,2581,6939565330101.56,X
4552,122891,python_function,torch/cuda/__init__.py(116): is_available,122891,44.302,2581,,2582,6939565330104.089,X
4553,122891,python_function,torch/cuda/__init__.py(107): _is_compiled,122891,2.468,2582,,2583,6939565330106.604,X
4554,122891,python_function,<built-in function hasattr>,122891,0.775,2583,,2584,6939565330108.208,X
4555,122891,python_function,torch/cuda/__init__.py(112): _nvml_based_avail,122891,32.63,2582,,2585,6939565330111.363,X
4556,122891,python_function,os.py(772): getenv,122891,28.023,2585,,2586,6939565330115.324,X
4557,122891,python_function,_collections_abc.py(821): get,122891,23.296,2586,,2587,6939565330119.758,X
4558,122891,python_function,os.py(675): __getitem__,122891,17.636,2587,,2588,6939565330122.713,X
4559,122891,python_function,os.py(755): encode,122891,6.827,2588,,2589,6939565330126.398,X
4560,122891,python_function,<built-in function isinstance>,122891,0.112,2589,,2590,6939565330127.386,X
4561,122891,python_function,<built-in method encode of str object at 0x7fb97db4cee0>,122891,1.529,2589,,2591,6939565330131.485,X
4562,122891,python_function,<built-in function _cuda_getDeviceCount>,122891,1.402,2582,,2592,6939565330146.593,X
4563,122891,python_function,<built-in method lower of str object at 0x7fb99e2355b0>,122891,0.899,2580,,2593,6939565330152.121,X
4564,122891,python_function,torch/_utils.py(749): <lambda>,122891,20.364,2580,,2594,6939565330154.819,X
4565,122891,python_function,torch/cuda/__init__.py(938): current_device,122891,17.586,2594,,2595,6939565330157.424,X
4566,122891,python_function,torch/cuda/__init__.py(289): _lazy_init,122891,1.316,2595,,2596,6939565330158.8,X
4567,122891,python_function,torch/cuda/__init__.py(242): is_initialized,122891,0.757,2596,,2597,6939565330159.15,X
4568,122891,python_function,<built-in function _cuda_isInBadFork>,122891,0.285,2597,,2598,6939565330159.502,X
4569,122891,python_function,<built-in function _cuda_getDevice>,122891,11.727,2595,,2599,6939565330163.144,X
4570,122891,python_function,torch/cuda/__init__.py(443): __enter__,122891,5.982,2563,,2600,6939565330180.079,X
4571,122891,python_function,<built-in function _cuda_exchangeDevice>,122891,3.29,2600,,2601,6939565330182.468,X
4572,122891,python_function,<built-in function _cuda_synchronize>,122891,39.514,2563,,2602,6939565330189.675,X
4573,122891,python_function,torch/cuda/__init__.py(446): __exit__,122891,123.311,2563,,2603,6939565330231.185,X
4574,122891,python_function,<built-in function _cuda_maybeExchangeDevice>,122891,2.399,2603,,2604,6939565330326.287,X
4575,122891,python_function,<built-in function perf_counter_ns>,122891,2.052,2603,,2605,6939565330333.809,X
4576,122891,python_function,<built-in method _disable_profiler of PyCapsule object at 0x7fb81c785050>,122891,15.54,2603,,2606,6939565330338.956,X
