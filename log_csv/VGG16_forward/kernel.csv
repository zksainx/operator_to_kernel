shared memory,pid,dur,stream,ts,est. achieved occupancy %,tid,cat,warps per SM,grid,registers per thread,correlation,ph,name,device,External id,queued,blocks per SM,context,block
0,0,2.112,7,6940680648793.148,39,7,kernel,18.761906,"[197, 1, 1]",16,15,X,"void cask__5x_cudnn::computeOffsetsKernel<false, false>(cask__5x_cudnn::ComputeOffsetsParams)",0,5,0,2.345238,1,"[256, 1, 1]"
16384,0,37.889,7,6940680648795.996,33,7,kernel,18.666666,"[392, 1, 1]",128,18,X,_5x_cudnn_ampere_scudnn_128x64_relu_medium_nn_v1,0,5,0,4.666667,1,"[128, 1, 1]"
0,0,47.041,7,6940680651182.941,100,7,kernel,597.333313,"[12544, 1, 1]",16,26,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,8,0,149.333328,1,"[128, 1, 1]"
0,0,1.952,7,6940680651230.718,0,7,kernel,0.047619,"[1, 1, 1]",24,32,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,9,0,0.011905,1,"[128, 1, 1]"
144,0,63.937,7,6940680651233.406,25,7,kernel,12.190476,"[64, 1, 1]",40,88,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,12,0,0.761905,1,"[512, 1, 1]"
0,0,46.656,7,6940680651345.696,100,7,kernel,298.666656,"[6272, 1, 1]",18,112,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,25,0,74.666664,1,"[128, 1, 1]"
8704,0,2.912,7,6940680651393.088,3,7,kernel,1.52381,"[2, 16, 1]",40,139,X,"void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)",0,30,0,0.380952,1,"[32, 4, 1]"
49152,0,142.019,7,6940680651396.864,33,7,kernel,74.666664,"[2, 28, 14]",126,141,X,_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1,0,30,0,9.333333,1,"[256, 1, 1]"
0,0,45.729,7,6940680651539.65,100,7,kernel,597.333313,"[12544, 1, 1]",16,149,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,33,0,149.333328,1,"[128, 1, 1]"
0,0,2.016,7,6940680651586.083,0,7,kernel,0.047619,"[1, 1, 1]",24,155,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,34,0,0.011905,1,"[128, 1, 1]"
144,0,63.393,7,6940680651588.803,25,7,kernel,12.190476,"[64, 1, 1]",40,211,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,37,0,0.761905,1,"[512, 1, 1]"
0,0,46.272,7,6940680651699.781,100,7,kernel,298.666656,"[6272, 1, 1]",18,235,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,50,0,74.666664,1,"[128, 1, 1]"
0,0,43.169,7,6940680651746.885,100,7,kernel,298.666656,"[3136, 1, 1]",26,258,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,53,0,37.333332,1,"[256, 1, 1]"
4224,0,9.792,7,6940680651790.886,100,7,kernel,74.666664,"[392, 2, 1]",38,276,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,57,0,9.333333,1,"[256, 1, 1]"
4224,0,4.032,7,6940680651801.414,51,7,kernel,24.380953,"[1, 2, 128]",38,278,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,57,0,3.047619,1,"[256, 1, 1]"
98304,0,60.609,7,6940680651806.182,0,7,kernel,4.666667,"[1, 49, 1]",254,283,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize256x128x32_stage2_warpsize4x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,57,0,0.583333,1,"[256, 1, 1]"
0,0,16.8,7,6940680651867.847,100,7,kernel,298.666656,"[6272, 1, 1]",16,292,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,60,0,74.666664,1,"[128, 1, 1]"
0,0,1.983,7,6940680651885.384,0,7,kernel,0.047619,"[1, 1, 1]",24,298,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,61,0,0.011905,1,"[128, 1, 1]"
144,0,22.049,7,6940680651888.135,51,7,kernel,24.380953,"[128, 1, 1]",40,354,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,64,0,1.52381,1,"[512, 1, 1]"
0,0,23.616,7,6940680651934.632,100,7,kernel,149.333328,"[3136, 1, 1]",18,374,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,77,0,37.333332,1,"[128, 1, 1]"
4224,0,24.481,7,6940680654125.926,100,7,kernel,149.333328,"[392, 4, 1]",38,400,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,82,0,18.666666,1,"[256, 1, 1]"
4224,0,5.088,7,6940680654151.175,100,7,kernel,48.761906,"[1, 4, 128]",38,402,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,82,0,6.095238,1,"[256, 1, 1]"
49152,0,96.354,7,6940680654159.111,17,7,kernel,9.333333,"[1, 98, 2]",230,406,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,82,0,2.333333,1,"[128, 1, 1]"
0,0,19.872,7,6940680654256.297,100,7,kernel,298.666656,"[6272, 1, 1]",16,415,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,85,0,74.666664,1,"[128, 1, 1]"
0,0,1.824,7,6940680654276.873,0,7,kernel,0.047619,"[1, 1, 1]",24,421,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,86,0,0.011905,1,"[128, 1, 1]"
144,0,21.6,7,6940680654279.401,51,7,kernel,24.380953,"[128, 1, 1]",40,473,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,89,0,1.52381,1,"[512, 1, 1]"
0,0,23.041,7,6940680654325.257,100,7,kernel,149.333328,"[3136, 1, 1]",18,493,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,102,0,37.333332,1,"[128, 1, 1]"
0,0,19.392,7,6940680654349.066,100,7,kernel,149.333328,"[1568, 1, 1]",26,512,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,105,0,18.666666,1,"[256, 1, 1]"
4224,0,4.0,7,6940680654369.29,78,7,kernel,37.333332,"[98, 4, 1]",38,534,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,109,0,4.666667,1,"[256, 1, 1]"
4224,0,6.464,7,6940680654374.282,100,7,kernel,97.523811,"[1, 4, 256]",38,536,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,109,0,12.190476,1,"[256, 1, 1]"
49152,0,51.265,7,6940680654381.578,5,7,kernel,2.380952,"[2, 25, 1]",230,539,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,109,0,0.595238,1,"[128, 1, 1]"
0,0,6.784,7,6940680654433.675,100,7,kernel,149.333328,"[3136, 1, 1]",16,548,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,112,0,37.333332,1,"[128, 1, 1]"
0,0,1.504,7,6940680654441.195,0,7,kernel,0.047619,"[1, 1, 1]",24,554,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,113,0,0.011905,1,"[128, 1, 1]"
14480,0,9.824,7,6940680654443.435,100,7,kernel,48.761906,"[256, 1, 1]",40,607,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,116,0,3.047619,1,"[512, 1, 1]"
0,0,6.688,7,6940680654463.435,100,7,kernel,74.666664,"[1568, 1, 1]",18,628,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,129,0,18.666666,1,"[128, 1, 1]"
4224,0,6.208,7,6940680654470.955,100,7,kernel,74.666664,"[98, 8, 1]",38,654,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,134,0,9.333333,1,"[256, 1, 1]"
4224,0,10.88,7,6940680654477.868,100,7,kernel,195.047623,"[1, 8, 256]",38,656,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,134,0,24.380953,1,"[256, 1, 1]"
49152,0,79.745,7,6940680654491.564,17,7,kernel,19.047619,"[2, 25, 8]",230,660,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,134,0,4.761905,1,"[128, 1, 1]"
0,0,6.848,7,6940680654572.173,100,7,kernel,149.333328,"[3136, 1, 1]",16,669,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,137,0,37.333332,1,"[128, 1, 1]"
0,0,1.824,7,6940680654579.789,0,7,kernel,0.047619,"[1, 1, 1]",24,675,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,138,0,0.011905,1,"[128, 1, 1]"
14480,0,9.344,7,6940680654582.349,100,7,kernel,48.761906,"[256, 1, 1]",40,728,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,141,0,3.047619,1,"[512, 1, 1]"
0,0,6.305,7,6940680654601.613,100,7,kernel,74.666664,"[1568, 1, 1]",18,749,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,154,0,18.666666,1,"[128, 1, 1]"
4224,0,6.976,7,6940680654608.717,100,7,kernel,74.666664,"[98, 8, 1]",38,771,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,159,0,9.333333,1,"[256, 1, 1]"
4224,0,10.656,7,6940680654616.494,100,7,kernel,195.047623,"[1, 8, 256]",38,773,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,159,0,24.380953,1,"[256, 1, 1]"
49152,0,80.77,7,6940680654629.613,17,7,kernel,19.047619,"[2, 25, 8]",230,777,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,159,0,4.761905,1,"[128, 1, 1]"
0,0,7.104,7,6940680654711.247,100,7,kernel,149.333328,"[3136, 1, 1]",16,786,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,162,0,37.333332,1,"[128, 1, 1]"
0,0,1.888,7,6940680654719.055,0,7,kernel,0.047619,"[1, 1, 1]",24,792,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,163,0,0.011905,1,"[128, 1, 1]"
14480,0,10.496,7,6940680654721.775,100,7,kernel,48.761906,"[256, 1, 1]",40,845,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,166,0,3.047619,1,"[512, 1, 1]"
0,0,5.952,7,6940680654742.575,100,7,kernel,74.666664,"[1568, 1, 1]",18,866,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,179,0,18.666666,1,"[128, 1, 1]"
0,0,6.432,7,6940680654749.296,100,7,kernel,74.666664,"[784, 1, 1]",26,885,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,182,0,9.333333,1,"[256, 1, 1]"
4224,0,4.576,7,6940680656226.212,40,7,kernel,19.047619,"[25, 8, 1]",38,903,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,186,0,2.380952,1,"[256, 1, 1]"
4224,0,17.536,7,6940680656231.492,100,7,kernel,390.095245,"[1, 8, 512]",38,905,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,186,0,48.761906,1,"[256, 1, 1]"
73728,0,49.185,7,6940680656249.828,0,7,kernel,2.666667,"[28, 2, 1]",162,909,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_16x6_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_16x6_nhwc_align4::Params),0,186,0,0.666667,1,"[128, 1, 1]"
4224,0,3.392,7,6940680656299.813,79,7,kernel,38.095238,"[25, 16, 1]",40,913,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,186,0,4.761905,1,"[256, 1, 1]"
0,0,4.096,7,6940680656304.037,100,7,kernel,74.666664,"[1568, 1, 1]",16,921,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,189,0,18.666666,1,"[128, 1, 1]"
0,0,1.504,7,6940680656308.869,0,7,kernel,0.047619,"[1, 1, 1]",24,927,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,190,0,0.011905,1,"[128, 1, 1]"
4240,0,9.824,7,6940680656311.141,100,7,kernel,97.523811,"[512, 1, 1]",40,980,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,193,0,6.095238,1,"[512, 1, 1]"
0,0,2.368,7,6940680656324.933,78,7,kernel,37.333332,"[784, 1, 1]",18,1001,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,206,0,9.333333,1,"[128, 1, 1]"
4224,0,3.136,7,6940680656328.069,79,7,kernel,38.095238,"[25, 16, 1]",38,1027,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,211,0,4.761905,1,"[256, 1, 1]"
4224,0,33.824,7,6940680656331.941,100,7,kernel,780.190491,"[1, 16, 512]",38,1029,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,211,0,97.523811,1,"[256, 1, 1]"
73728,0,92.482,7,6940680656366.597,0,7,kernel,2.666667,"[28, 2, 1]",168,1033,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4::Params),0,211,0,0.666667,1,"[128, 1, 1]"
4224,0,3.456,7,6940680656459.847,79,7,kernel,38.095238,"[25, 16, 1]",40,1037,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,211,0,4.761905,1,"[256, 1, 1]"
0,0,4.224,7,6940680656464.007,100,7,kernel,74.666664,"[1568, 1, 1]",16,1045,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,214,0,18.666666,1,"[128, 1, 1]"
0,0,1.504,7,6940680656469.031,0,7,kernel,0.047619,"[1, 1, 1]",24,1051,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,215,0,0.011905,1,"[128, 1, 1]"
4240,0,9.536,7,6940680656471.303,100,7,kernel,97.523811,"[512, 1, 1]",40,1104,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,218,0,6.095238,1,"[512, 1, 1]"
0,0,2.336,7,6940680656484.743,78,7,kernel,37.333332,"[784, 1, 1]",18,1125,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,231,0,9.333333,1,"[128, 1, 1]"
4224,0,2.848,7,6940680656487.815,79,7,kernel,38.095238,"[25, 16, 1]",38,1147,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,236,0,4.761905,1,"[256, 1, 1]"
4224,0,33.792,7,6940680656491.432,100,7,kernel,780.190491,"[1, 16, 512]",38,1149,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,236,0,97.523811,1,"[256, 1, 1]"
73728,0,91.841,7,6940680656525.928,0,7,kernel,2.666667,"[28, 2, 1]",168,1153,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4::Params),0,236,0,0.666667,1,"[128, 1, 1]"
4224,0,3.424,7,6940680656618.505,79,7,kernel,38.095238,"[25, 16, 1]",40,1157,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,236,0,4.761905,1,"[256, 1, 1]"
0,0,4.031,7,6940680656622.73,100,7,kernel,74.666664,"[1568, 1, 1]",16,1165,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,239,0,18.666666,1,"[128, 1, 1]"
0,0,1.504,7,6940680656627.529,0,7,kernel,0.047619,"[1, 1, 1]",24,1171,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,240,0,0.011905,1,"[128, 1, 1]"
4240,0,9.567,7,6940680656629.77,100,7,kernel,97.523811,"[512, 1, 1]",40,1224,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,243,0,6.095238,1,"[512, 1, 1]"
0,0,2.336,7,6940680656643.274,78,7,kernel,37.333332,"[784, 1, 1]",18,1245,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,256,0,9.333333,1,"[128, 1, 1]"
0,0,3.936,7,6940680656646.314,78,7,kernel,37.333332,"[392, 1, 1]",26,1264,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,259,0,4.666667,1,"[256, 1, 1]"
4224,0,2.016,7,6940680656650.986,22,7,kernel,10.666667,"[7, 16, 1]",38,1282,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,263,0,1.333333,1,"[256, 1, 1]"
4224,0,33.536,7,6940680656654.186,100,7,kernel,780.190491,"[1, 16, 512]",38,1284,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,263,0,97.523811,1,"[256, 1, 1]"
100352,0,37.729,7,6940680656690.122,0,7,kernel,3.047619,"[8, 2, 4]",198,1290,X,sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,263,0,0.761905,1,"[128, 1, 1]"
4224,0,2.624,7,6940680656728.683,22,7,kernel,10.666667,"[7, 16, 1]",40,1293,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,263,0,1.333333,1,"[256, 1, 1]"
0,0,2.688,7,6940680656732.043,39,7,kernel,18.666666,"[392, 1, 1]",16,1301,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,266,0,4.666667,1,"[128, 1, 1]"
0,0,1.888,7,6940680659079.212,0,7,kernel,0.047619,"[1, 1, 1]",24,1307,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,267,0,0.011905,1,"[128, 1, 1]"
2192,0,9.408,7,6940680659081.804,100,7,kernel,97.523811,"[512, 1, 1]",40,1364,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,270,0,6.095238,1,"[512, 1, 1]"
0,0,1.665,7,6940680659094.475,19,7,kernel,9.333333,"[196, 1, 1]",18,1385,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,283,0,2.333333,1,"[128, 1, 1]"
4224,0,2.048,7,6940680659096.876,22,7,kernel,10.666667,"[7, 16, 1]",38,1407,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,288,0,1.333333,1,"[256, 1, 1]"
4224,0,34.144,7,6940680659099.724,100,7,kernel,780.190491,"[1, 16, 512]",38,1409,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,288,0,97.523811,1,"[256, 1, 1]"
100352,0,37.697,7,6940680659136.652,0,7,kernel,3.047619,"[8, 2, 4]",198,1415,X,sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,288,0,0.761905,1,"[128, 1, 1]"
4224,0,2.592,7,6940680659175.053,22,7,kernel,10.666667,"[7, 16, 1]",40,1418,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,288,0,1.333333,1,"[256, 1, 1]"
0,0,2.816,7,6940680659178.381,39,7,kernel,18.666666,"[392, 1, 1]",16,1426,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,291,0,4.666667,1,"[128, 1, 1]"
0,0,1.536,7,6940680659181.997,0,7,kernel,0.047619,"[1, 1, 1]",24,1432,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,292,0,0.011905,1,"[128, 1, 1]"
2192,0,8.512,7,6940680659184.237,100,7,kernel,97.523811,"[512, 1, 1]",40,1485,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,295,0,6.095238,1,"[512, 1, 1]"
0,0,1.632,7,6940680659195.949,19,7,kernel,9.333333,"[196, 1, 1]",18,1506,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,308,0,2.333333,1,"[128, 1, 1]"
4224,0,2.016,7,6940680659198.381,22,7,kernel,10.666667,"[7, 16, 1]",38,1532,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,313,0,1.333333,1,"[256, 1, 1]"
4224,0,33.089,7,6940680659201.197,100,7,kernel,780.190491,"[1, 16, 512]",38,1534,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,313,0,97.523811,1,"[256, 1, 1]"
100352,0,37.184,7,6940680659237.038,0,7,kernel,3.047619,"[8, 2, 4]",198,1540,X,sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,313,0,0.761905,1,"[128, 1, 1]"
4224,0,2.592,7,6940680659274.99,22,7,kernel,10.666667,"[7, 16, 1]",40,1543,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,313,0,1.333333,1,"[256, 1, 1]"
0,0,2.72,7,6940680659278.446,39,7,kernel,18.666666,"[392, 1, 1]",16,1551,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,316,0,4.666667,1,"[128, 1, 1]"
0,0,1.536,7,6940680659281.87,0,7,kernel,0.047619,"[1, 1, 1]",24,1557,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,317,0,0.011905,1,"[128, 1, 1]"
2192,0,8.384,7,6940680659284.11,100,7,kernel,97.523811,"[512, 1, 1]",40,1610,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,320,0,6.095238,1,"[512, 1, 1]"
0,0,1.601,7,6940680659295.694,19,7,kernel,9.333333,"[196, 1, 1]",18,1631,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,333,0,2.333333,1,"[128, 1, 1]"
0,0,2.625,7,6940680659298.094,19,7,kernel,9.333333,"[98, 1, 1]",26,1650,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,336,0,1.166667,1,"[256, 1, 1]"
528,0,691.531,7,6940680659301.518,25,7,kernel,48.761906,"[1024, 1, 1]",159,1673,X,"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, true, false, 7, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",0,342,0,12.190476,1,"[32, 4, 1]"
0,0,2.209,7,6940680659993.848,3,7,kernel,1.52381,"[16, 1, 1]",47,1704,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,344,0,0.190476,1,"[256, 1, 1]"
2560,0,115.33,7,6940680659996.792,51,7,kernel,24.380953,"[512, 1, 1]",58,1723,X,"void gemv2T_kernel_val<int, int, float, float, float, float, 128, 16, 4, 4, false, true, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>, float, float)",0,353,0,6.095238,1,"[128, 1, 1]"
0,0,3.008,7,6940680660112.858,3,7,kernel,1.52381,"[16, 1, 1]",47,1754,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,355,0,0.190476,1,"[256, 1, 1]"
528,0,29.985,7,6940680660116.666,25,7,kernel,11.904762,"[250, 1, 1]",159,1773,X,"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, true, false, 7, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",0,364,0,2.976191,1,"[32, 4, 1]"
