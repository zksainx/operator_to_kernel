shared memory,pid,dur,stream,ts,est. achieved occupancy %,tid,cat,warps per SM,grid,registers per thread,correlation,ph,name,device,External id,queued,blocks per SM,context,block
0,0,1.472,7,6939565712147.097,0,7,kernel,0.047619,"[1, 1, 1]",16,78910,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22534,0,0.011905,1,"[128, 1, 1]"
0,0,3.775,7,6939565713382.551,100,7,kernel,93.047623,"[1954, 1, 1]",16,78940,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23058,0,23.261906,1,"[128, 1, 1]"
30720,0,50.688,7,6939565713571.862,14,7,kernel,6.857143,"[4, 1, 36]",126,78958,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x64_8x5_nn_align1>(cutlass_80_simt_sgemm_128x64_8x5_nn_align1::Params),0,23054,0,1.714286,1,"[128, 1, 1]"
0,0,4.16,7,6939565713623.414,25,7,kernel,12.190476,"[16, 4, 1]",44,78959,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23054,0,0.761905,1,"[32, 16, 1]"
0,0,3.712,7,6939565713823.83,100,7,kernel,93.047623,"[1954, 1, 1]",16,78977,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23066,0,23.261906,1,"[128, 1, 1]"
16384,0,46.656,7,6939565713828.278,67,7,kernel,119.238098,"[4, 313, 1]",57,78991,X,ampere_sgemm_128x32_nn,0,23062,0,14.904762,1,"[256, 1, 1]"
16,0,4.256,7,6939565713875.766,8,7,kernel,3.809524,"[20, 1, 1]",32,79004,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23070,0,0.238095,1,"[32, 16, 1]"
16,0,3.073,7,6939565714439.38,5,7,kernel,2.380952,"[50, 1, 1]",40,79057,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,23095,0,0.595238,1,"[128, 1, 1]"
0,0,4.736,7,6939565714443.285,0,7,kernel,0.190476,"[2, 1, 1]",38,79059,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,23095,0,0.02381,1,"[256, 1, 1]"
16,0,3.072,7,6939565715107.988,5,7,kernel,2.380952,"[50, 1, 1]",40,79100,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,23109,0,0.595238,1,"[128, 1, 1]"
0,0,4.736,7,6939565715111.892,0,7,kernel,0.190476,"[2, 1, 1]",38,79102,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,23109,0,0.02381,1,"[256, 1, 1]"
0,0,1.791,7,6939565715117.428,5,7,kernel,2.380952,"[50, 1, 1]",22,79131,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,23125,0,0.595238,1,"[128, 1, 1]"
16384,0,16.704,7,6939565715361.843,25,7,kernel,12.190476,"[16, 2, 4]",57,79157,X,ampere_sgemm_128x32_nn,0,23137,0,1.52381,1,"[256, 1, 1]"
0,0,3.392,7,6939565715379.315,67,7,kernel,48.761906,"[64, 4, 1]",44,79159,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23137,0,3.047619,1,"[32, 16, 1]"
16384,0,14.208,7,6939565715383.571,51,7,kernel,24.380953,"[16, 16, 1]",57,79176,X,ampere_sgemm_128x32_nt,0,23141,0,3.047619,1,"[256, 1, 1]"
16,0,10.08,7,6939565715665.715,0,7,kernel,0.047619,"[1, 1, 1]",48,79189,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23145,0,0.011905,1,"[32, 4, 1]"
0,0,3.296,7,6939565716897.649,20,7,kernel,9.523809,"[200, 1, 1]",22,79222,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,23166,0,2.380952,1,"[128, 1, 1]"
0,0,3.135,7,6939565716901.681,20,7,kernel,9.523809,"[200, 1, 1]",22,79236,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,23171,0,2.380952,1,"[128, 1, 1]"
16384,0,16.352,7,6939565716905.649,44,7,kernel,21.333334,"[4, 2, 28]",57,79261,X,ampere_sgemm_128x32_nn,0,23181,0,2.666667,1,"[256, 1, 1]"
0,0,3.648,7,6939565716922.737,25,7,kernel,12.190476,"[16, 4, 1]",44,79263,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23181,0,0.761905,1,"[32, 16, 1]"
16384,0,14.431,7,6939565716927.185,51,7,kernel,24.380953,"[16, 16, 1]",57,79280,X,ampere_sgemm_32x128_nt,0,23185,0,3.047619,1,"[256, 1, 1]"
16,0,7.584,7,6939565716942.417,0,7,kernel,0.190476,"[4, 1, 1]",48,79293,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23189,0,0.047619,1,"[32, 4, 1]"
0,0,1.92,7,6939565716950.768,5,7,kernel,2.380952,"[50, 1, 1]",22,79316,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23208,0,0.595238,1,"[128, 1, 1]"
16,0,2.975,7,6939565716953.425,5,7,kernel,2.380952,"[50, 1, 1]",40,79351,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,23211,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565716957.169,0,7,kernel,0.190476,"[2, 1, 1]",38,79353,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,23211,0,0.02381,1,"[256, 1, 1]"
0,0,2.079,7,6939565718178.863,5,7,kernel,2.380952,"[50, 1, 1]",22,79382,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,23227,0,0.595238,1,"[128, 1, 1]"
25600,0,7.328,7,6939565718181.679,6,7,kernel,3.047619,"[8, 1, 8]",80,79412,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,23239,0,0.761905,1,"[128, 1, 1]"
0,0,2.049,7,6939565718189.71,25,7,kernel,12.190476,"[16, 4, 1]",44,79413,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23239,0,0.761905,1,"[32, 16, 1]"
25600,0,6.113,7,6939565718192.526,6,7,kernel,3.047619,"[64, 1, 1]",80,79435,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,23243,0,0.761905,1,"[128, 1, 1]"
16,0,7.999,7,6939565718199.471,0,7,kernel,0.047619,"[1, 1, 1]",48,79447,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23247,0,0.011905,1,"[32, 4, 1]"
0,0,2.209,7,6939565718834.797,5,7,kernel,2.380952,"[50, 1, 1]",22,79510,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,23289,0,0.595238,1,"[128, 1, 1]"
16,0,3.519,7,6939565718837.838,10,7,kernel,4.761905,"[25, 1, 1]",32,79524,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23290,0,0.297619,1,"[32, 16, 1]"
0,0,2.015,7,6939565718842.094,0,7,kernel,0.095238,"[2, 1, 1]",16,79537,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23298,0,0.02381,1,"[128, 1, 1]"
53504,0,36.607,7,6939565718844.91,0,7,kernel,0.380952,"[1, 8, 1]",236,79551,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,23283,0,0.095238,1,"[128, 1, 1]"
0,0,1.568,7,6939565720225.739,10,7,kernel,4.761905,"[100, 1, 1]",16,79609,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,23348,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565720231.371,10,7,kernel,4.761905,"[100, 1, 1]",16,79634,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,23358,0,1.190476,1,"[128, 1, 1]"
0,0,1.632,7,6939565720236.171,10,7,kernel,4.761905,"[100, 1, 1]",22,79648,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23362,0,1.190476,1,"[128, 1, 1]"
0,0,2.304,7,6939565720238.507,20,7,kernel,9.523809,"[200, 1, 1]",16,79683,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23383,0,2.380952,1,"[128, 1, 1]"
16384,0,9.888,7,6939565721205.738,25,7,kernel,12.190476,"[4, 2, 16]",57,79709,X,ampere_sgemm_128x32_nn,0,23394,0,1.52381,1,"[256, 1, 1]"
0,0,2.689,7,6939565721216.361,25,7,kernel,12.190476,"[16, 4, 1]",44,79711,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23394,0,0.761905,1,"[32, 16, 1]"
16384,0,8.192,7,6939565721220.074,25,7,kernel,12.190476,"[16, 8, 1]",57,79728,X,ampere_sgemm_32x128_nt,0,23398,0,1.52381,1,"[256, 1, 1]"
16,0,8.033,7,6939565721229.001,0,7,kernel,0.095238,"[2, 1, 1]",48,79741,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23402,0,0.02381,1,"[32, 4, 1]"
25600,0,7.2,7,6939565721237.769,6,7,kernel,3.047619,"[8, 1, 8]",80,79779,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,23422,0,0.761905,1,"[128, 1, 1]"
0,0,1.983,7,6939565721245.77,25,7,kernel,12.190476,"[16, 4, 1]",44,79780,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23422,0,0.761905,1,"[32, 16, 1]"
25600,0,5.952,7,6939565721248.522,6,7,kernel,3.047619,"[64, 1, 1]",80,79802,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,23426,0,0.761905,1,"[128, 1, 1]"
16,0,5.984,7,6939565721255.305,0,7,kernel,0.047619,"[1, 1, 1]",48,79814,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23430,0,0.011905,1,"[32, 4, 1]"
0,0,1.857,7,6939565721262.121,5,7,kernel,2.380952,"[50, 1, 1]",22,79829,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23441,0,0.595238,1,"[128, 1, 1]"
0,0,2.4,7,6939565721572.265,0,7,kernel,0.190476,"[2, 2, 1]",30,79846,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,23444,0,0.047619,1,"[128, 1, 1]"
0,0,10.688,7,6939565721575.433,100,7,kernel,97.523811,"[1024, 2, 1]",30,79865,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,23451,0,24.380953,1,"[128, 1, 1]"
16,0,3.105,7,6939565721869.704,5,7,kernel,2.380952,"[50, 1, 1]",40,79902,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,23458,0,0.595238,1,"[128, 1, 1]"
0,0,4.704,7,6939565721873.512,0,7,kernel,0.190476,"[2, 1, 1]",38,79904,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,23458,0,0.02381,1,"[256, 1, 1]"
0,0,2.08,7,6939565722325.064,5,7,kernel,2.380952,"[50, 1, 1]",22,79933,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,23474,0,0.595238,1,"[128, 1, 1]"
25600,0,7.552,7,6939565722327.944,6,7,kernel,3.047619,"[8, 1, 8]",80,79963,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,23486,0,0.761905,1,"[128, 1, 1]"
0,0,2.016,7,6939565722336.328,25,7,kernel,12.190476,"[16, 4, 1]",44,79964,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23486,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565722339.144,6,7,kernel,3.047619,"[64, 1, 1]",80,79986,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,23490,0,0.761905,1,"[128, 1, 1]"
16,0,10.08,7,6939565722548.903,0,7,kernel,0.047619,"[1, 1, 1]",48,79998,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23494,0,0.011905,1,"[32, 4, 1]"
0,0,2.24,7,6939565722848.231,5,7,kernel,2.380952,"[50, 1, 1]",22,80061,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,23536,0,0.595238,1,"[128, 1, 1]"
16,0,3.552,7,6939565722851.207,10,7,kernel,4.761905,"[25, 1, 1]",32,80075,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23537,0,0.297619,1,"[32, 16, 1]"
0,0,2.304,7,6939565723068.327,0,7,kernel,0.095238,"[2, 1, 1]",16,80088,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23545,0,0.02381,1,"[128, 1, 1]"
53504,0,37.472,7,6939565723071.463,0,7,kernel,0.380952,"[1, 8, 1]",236,80102,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,23530,0,0.095238,1,"[128, 1, 1]"
0,0,1.6,7,6939565723461.35,15,7,kernel,7.142857,"[150, 1, 1]",16,80160,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,23595,0,1.785714,1,"[128, 1, 1]"
0,0,1.601,7,6939565723702.917,15,7,kernel,7.142857,"[150, 1, 1]",16,80185,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,23605,0,1.785714,1,"[128, 1, 1]"
0,0,2.463,7,6939565723708.422,15,7,kernel,7.142857,"[150, 1, 1]",22,80199,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23609,0,1.785714,1,"[128, 1, 1]"
0,0,1.217,7,6939565723711.717,15,7,kernel,7.142857,"[150, 1, 1]",16,80219,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,23616,0,1.785714,1,"[128, 1, 1]"
0,0,2.976,7,6939565723916.709,15,7,kernel,7.142857,"[150, 1, 1]",22,80233,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23620,0,1.785714,1,"[128, 1, 1]"
0,0,3.04,7,6939565724132.421,30,7,kernel,14.285714,"[300, 1, 1]",16,80268,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23641,0,3.571429,1,"[128, 1, 1]"
16384,0,13.6,7,6939565724136.165,38,7,kernel,18.285715,"[4, 2, 24]",57,80294,X,ampere_sgemm_128x32_nn,0,23652,0,2.285714,1,"[256, 1, 1]"
0,0,2.752,7,6939565724150.501,25,7,kernel,12.190476,"[16, 4, 1]",44,80296,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23652,0,0.761905,1,"[32, 16, 1]"
16384,0,12.288,7,6939565724154.053,38,7,kernel,18.285715,"[16, 12, 1]",57,80313,X,ampere_sgemm_32x128_nt,0,23656,0,2.285714,1,"[256, 1, 1]"
16,0,9.888,7,6939565724776.164,0,7,kernel,0.142857,"[3, 1, 1]",48,80326,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23660,0,0.035714,1,"[32, 4, 1]"
0,0,2.112,7,6939565724786.852,5,7,kernel,2.380952,"[50, 1, 1]",22,80349,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23679,0,0.595238,1,"[128, 1, 1]"
16,0,2.816,7,6939565724789.764,5,7,kernel,2.380952,"[50, 1, 1]",40,80384,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,23682,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565724793.38,0,7,kernel,0.190476,"[2, 1, 1]",38,80386,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,23682,0,0.02381,1,"[256, 1, 1]"
0,0,1.664,7,6939565724798.532,5,7,kernel,2.380952,"[50, 1, 1]",22,80415,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,23698,0,0.595238,1,"[128, 1, 1]"
16384,0,17.376,7,6939565725266.595,25,7,kernel,12.190476,"[16, 2, 4]",57,80441,X,ampere_sgemm_128x32_nn,0,23710,0,1.52381,1,"[256, 1, 1]"
0,0,3.136,7,6939565725284.803,67,7,kernel,48.761906,"[64, 4, 1]",44,80443,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23710,0,3.047619,1,"[32, 16, 1]"
16384,0,13.888,7,6939565725288.707,51,7,kernel,24.380953,"[16, 16, 1]",57,80460,X,ampere_sgemm_128x32_nt,0,23714,0,3.047619,1,"[256, 1, 1]"
16,0,7.936,7,6939565725303.395,0,7,kernel,0.047619,"[1, 1, 1]",48,80473,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23718,0,0.011905,1,"[32, 4, 1]"
0,0,2.496,7,6939565725312.163,20,7,kernel,9.523809,"[200, 1, 1]",22,80506,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,23739,0,2.380952,1,"[128, 1, 1]"
0,0,3.072,7,6939565725315.427,20,7,kernel,9.523809,"[200, 1, 1]",22,80520,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,23744,0,2.380952,1,"[128, 1, 1]"
16384,0,17.44,7,6939565725768.13,44,7,kernel,21.333334,"[4, 2, 28]",57,80545,X,ampere_sgemm_128x32_nn,0,23754,0,2.666667,1,"[256, 1, 1]"
0,0,3.424,7,6939565725786.274,25,7,kernel,12.190476,"[16, 4, 1]",44,80547,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23754,0,0.761905,1,"[32, 16, 1]"
16384,0,14.784,7,6939565725790.402,51,7,kernel,24.380953,"[16, 16, 1]",57,80564,X,ampere_sgemm_32x128_nt,0,23758,0,3.047619,1,"[256, 1, 1]"
16,0,7.68,7,6939565725805.986,0,7,kernel,0.190476,"[4, 1, 1]",48,80577,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23762,0,0.047619,1,"[32, 4, 1]"
0,0,1.728,7,6939565725814.402,5,7,kernel,2.380952,"[50, 1, 1]",22,80600,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23781,0,0.595238,1,"[128, 1, 1]"
16,0,3.008,7,6939565725816.866,5,7,kernel,2.380952,"[50, 1, 1]",40,80635,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,23784,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565725820.642,0,7,kernel,0.190476,"[2, 1, 1]",38,80637,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,23784,0,0.02381,1,"[256, 1, 1]"
0,0,2.08,7,6939565726043.874,5,7,kernel,2.380952,"[50, 1, 1]",22,80666,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,23800,0,0.595238,1,"[128, 1, 1]"
25600,0,7.201,7,6939565726258.625,6,7,kernel,3.047619,"[8, 1, 8]",80,80696,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,23812,0,0.761905,1,"[128, 1, 1]"
0,0,2.304,7,6939565726266.561,25,7,kernel,12.190476,"[16, 4, 1]",44,80697,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23812,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565726269.697,6,7,kernel,3.047619,"[64, 1, 1]",80,80719,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,23816,0,0.761905,1,"[128, 1, 1]"
16,0,7.84,7,6939565726276.897,0,7,kernel,0.047619,"[1, 1, 1]",48,80731,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23820,0,0.011905,1,"[32, 4, 1]"
0,0,2.368,7,6939565726614.209,5,7,kernel,2.380952,"[50, 1, 1]",22,80794,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,23862,0,0.595238,1,"[128, 1, 1]"
16,0,3.552,7,6939565726617.345,10,7,kernel,4.761905,"[25, 1, 1]",32,80808,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23863,0,0.297619,1,"[32, 16, 1]"
0,0,2.048,7,6939565726621.633,0,7,kernel,0.095238,"[2, 1, 1]",16,80821,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23871,0,0.02381,1,"[128, 1, 1]"
53504,0,36.576,7,6939565726804.993,0,7,kernel,0.380952,"[1, 8, 1]",236,80835,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,23856,0,0.095238,1,"[128, 1, 1]"
0,0,1.6,7,6939565727095.552,10,7,kernel,4.761905,"[100, 1, 1]",16,80893,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,23921,0,1.190476,1,"[128, 1, 1]"
0,0,1.568,7,6939565727320.128,10,7,kernel,4.761905,"[100, 1, 1]",16,80918,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,23931,0,1.190476,1,"[128, 1, 1]"
0,0,1.855,7,6939565727325.728,10,7,kernel,4.761905,"[100, 1, 1]",22,80932,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23935,0,1.190476,1,"[128, 1, 1]"
0,0,2.912,7,6939565727919.583,20,7,kernel,9.523809,"[200, 1, 1]",16,80967,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,23956,0,2.380952,1,"[128, 1, 1]"
16384,0,10.272,7,6939565727923.231,25,7,kernel,12.190476,"[4, 2, 16]",57,80993,X,ampere_sgemm_128x32_nn,0,23967,0,1.52381,1,"[256, 1, 1]"
0,0,2.368,7,6939565727934.302,25,7,kernel,12.190476,"[16, 4, 1]",44,80995,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23967,0,0.761905,1,"[32, 16, 1]"
16384,0,8.159,7,6939565727937.439,25,7,kernel,12.190476,"[16, 8, 1]",57,81012,X,ampere_sgemm_32x128_nt,0,23971,0,1.52381,1,"[256, 1, 1]"
16,0,7.775,7,6939565727946.431,0,7,kernel,0.095238,"[2, 1, 1]",48,81025,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,23975,0,0.02381,1,"[32, 4, 1]"
0,0,1.857,7,6939565727955.038,5,7,kernel,2.380952,"[50, 1, 1]",22,81044,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,23986,0,0.595238,1,"[128, 1, 1]"
25600,0,6.943,7,6939565727957.695,6,7,kernel,3.047619,"[8, 1, 8]",80,81076,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,23996,0,0.761905,1,"[128, 1, 1]"
0,0,1.985,7,6939565727965.342,25,7,kernel,12.190476,"[16, 4, 1]",44,81077,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,23996,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565727968.159,6,7,kernel,3.047619,"[64, 1, 1]",80,81099,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,24000,0,0.761905,1,"[128, 1, 1]"
16,0,6.208,7,6939565727974.942,0,7,kernel,0.047619,"[1, 1, 1]",48,81111,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24004,0,0.011905,1,"[32, 4, 1]"
0,0,2.176,7,6939565728348.766,5,7,kernel,2.380952,"[50, 1, 1]",22,81126,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24015,0,0.595238,1,"[128, 1, 1]"
0,0,2.432,7,6939565728351.742,0,7,kernel,0.190476,"[2, 2, 1]",30,81143,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,24018,0,0.047619,1,"[128, 1, 1]"
0,0,10.912,7,6939565728355.006,100,7,kernel,97.523811,"[1024, 2, 1]",30,81162,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,24025,0,24.380953,1,"[128, 1, 1]"
16,0,2.912,7,6939565728366.686,5,7,kernel,2.380952,"[50, 1, 1]",40,81199,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,24032,0,0.595238,1,"[128, 1, 1]"
0,0,4.48,7,6939565728370.334,0,7,kernel,0.190476,"[2, 1, 1]",38,81201,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,24032,0,0.02381,1,"[256, 1, 1]"
0,0,2.112,7,6939565728693.693,5,7,kernel,2.380952,"[50, 1, 1]",22,81230,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24048,0,0.595238,1,"[128, 1, 1]"
25600,0,7.169,7,6939565728696.925,6,7,kernel,3.047619,"[8, 1, 8]",80,81260,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,24060,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565728704.797,25,7,kernel,12.190476,"[16, 4, 1]",44,81261,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24060,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565728707.613,6,7,kernel,3.047619,"[64, 1, 1]",80,81283,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,24064,0,0.761905,1,"[128, 1, 1]"
16,0,8.159,7,6939565728714.366,0,7,kernel,0.047619,"[1, 1, 1]",48,81295,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24068,0,0.011905,1,"[32, 4, 1]"
0,0,2.24,7,6939565729306.428,5,7,kernel,2.380952,"[50, 1, 1]",22,81358,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,24110,0,0.595238,1,"[128, 1, 1]"
16,0,3.521,7,6939565729309.468,10,7,kernel,4.761905,"[25, 1, 1]",32,81372,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24111,0,0.297619,1,"[32, 16, 1]"
0,0,1.985,7,6939565729313.756,0,7,kernel,0.095238,"[2, 1, 1]",16,81385,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,24119,0,0.02381,1,"[128, 1, 1]"
53504,0,36.864,7,6939565729316.54,0,7,kernel,0.380952,"[1, 8, 1]",236,81399,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,24104,0,0.095238,1,"[128, 1, 1]"
0,0,1.568,7,6939565729567.324,15,7,kernel,7.142857,"[150, 1, 1]",16,81457,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24169,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565729572.796,15,7,kernel,7.142857,"[150, 1, 1]",16,81482,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24179,0,1.785714,1,"[128, 1, 1]"
0,0,3.008,7,6939565729817.979,15,7,kernel,7.142857,"[150, 1, 1]",22,81496,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24183,0,1.785714,1,"[128, 1, 1]"
0,0,1.599,7,6939565729821.724,15,7,kernel,7.142857,"[150, 1, 1]",16,81516,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24190,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565729827.1,15,7,kernel,7.142857,"[150, 1, 1]",22,81530,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24194,0,1.785714,1,"[128, 1, 1]"
0,0,3.04,7,6939565730043.323,30,7,kernel,14.285714,"[300, 1, 1]",16,81565,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,24215,0,3.571429,1,"[128, 1, 1]"
16384,0,14.08,7,6939565730047.067,38,7,kernel,18.285715,"[4, 2, 24]",57,81591,X,ampere_sgemm_128x32_nn,0,24226,0,2.285714,1,"[256, 1, 1]"
0,0,2.784,7,6939565730061.915,25,7,kernel,12.190476,"[16, 4, 1]",44,81593,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24226,0,0.761905,1,"[32, 16, 1]"
16384,0,12.576,7,6939565730065.371,38,7,kernel,18.285715,"[16, 12, 1]",57,81610,X,ampere_sgemm_32x128_nt,0,24230,0,2.285714,1,"[256, 1, 1]"
16,0,10.016,7,6939565730280.699,0,7,kernel,0.142857,"[3, 1, 1]",48,81623,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24234,0,0.035714,1,"[32, 4, 1]"
0,0,2.176,7,6939565730291.547,5,7,kernel,2.380952,"[50, 1, 1]",22,81646,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24253,0,0.595238,1,"[128, 1, 1]"
16,0,3.168,7,6939565730500.954,5,7,kernel,2.380952,"[50, 1, 1]",40,81681,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,24256,0,0.595238,1,"[128, 1, 1]"
0,0,4.767,7,6939565730504.859,0,7,kernel,0.190476,"[2, 1, 1]",38,81683,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,24256,0,0.02381,1,"[256, 1, 1]"
0,0,1.696,7,6939565730510.395,5,7,kernel,2.380952,"[50, 1, 1]",22,81712,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24272,0,0.595238,1,"[128, 1, 1]"
16384,0,17.12,7,6939565730718.202,25,7,kernel,12.190476,"[16, 2, 4]",57,81738,X,ampere_sgemm_128x32_nn,0,24284,0,1.52381,1,"[256, 1, 1]"
0,0,3.2,7,6939565730736.026,67,7,kernel,48.761906,"[64, 4, 1]",44,81740,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24284,0,3.047619,1,"[32, 16, 1]"
16384,0,13.792,7,6939565730739.962,51,7,kernel,24.380953,"[16, 16, 1]",57,81757,X,ampere_sgemm_128x32_nt,0,24288,0,3.047619,1,"[256, 1, 1]"
16,0,7.968,7,6939565730754.49,0,7,kernel,0.047619,"[1, 1, 1]",48,81770,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24292,0,0.011905,1,"[32, 4, 1]"
0,0,2.848,7,6939565731220.857,20,7,kernel,9.523809,"[200, 1, 1]",22,81803,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24313,0,2.380952,1,"[128, 1, 1]"
0,0,3.584,7,6939565731224.473,20,7,kernel,9.523809,"[200, 1, 1]",22,81817,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,24318,0,2.380952,1,"[128, 1, 1]"
16384,0,16.608,7,6939565731228.921,44,7,kernel,21.333334,"[4, 2, 28]",57,81842,X,ampere_sgemm_128x32_nn,0,24328,0,2.666667,1,"[256, 1, 1]"
0,0,3.232,7,6939565731246.297,25,7,kernel,12.190476,"[16, 4, 1]",44,81844,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24328,0,0.761905,1,"[32, 16, 1]"
16384,0,14.272,7,6939565731250.233,51,7,kernel,24.380953,"[16, 16, 1]",57,81861,X,ampere_sgemm_32x128_nt,0,24332,0,3.047619,1,"[256, 1, 1]"
16,0,7.552,7,6939565731265.337,0,7,kernel,0.190476,"[4, 1, 1]",48,81874,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24336,0,0.047619,1,"[32, 4, 1]"
0,0,1.984,7,6939565731273.625,5,7,kernel,2.380952,"[50, 1, 1]",22,81897,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24355,0,0.595238,1,"[128, 1, 1]"
16,0,3.04,7,6939565731653.592,5,7,kernel,2.380952,"[50, 1, 1]",40,81932,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,24358,0,0.595238,1,"[128, 1, 1]"
0,0,4.896,7,6939565731657.72,0,7,kernel,0.190476,"[2, 1, 1]",38,81934,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,24358,0,0.02381,1,"[256, 1, 1]"
0,0,1.665,7,6939565731663.448,5,7,kernel,2.380952,"[50, 1, 1]",22,81963,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24374,0,0.595238,1,"[128, 1, 1]"
25600,0,6.943,7,6939565731665.913,6,7,kernel,3.047619,"[8, 1, 8]",80,81993,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,24386,0,0.761905,1,"[128, 1, 1]"
0,0,2.049,7,6939565731673.688,25,7,kernel,12.190476,"[16, 4, 1]",44,81994,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24386,0,0.761905,1,"[32, 16, 1]"
25600,0,6.176,7,6939565731676.536,6,7,kernel,3.047619,"[64, 1, 1]",80,82016,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,24390,0,0.761905,1,"[128, 1, 1]"
16,0,9.92,7,6939565732469.687,0,7,kernel,0.047619,"[1, 1, 1]",48,82028,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24394,0,0.011905,1,"[32, 4, 1]"
0,0,2.208,7,6939565732480.311,5,7,kernel,2.380952,"[50, 1, 1]",22,82091,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,24436,0,0.595238,1,"[128, 1, 1]"
16,0,3.296,7,6939565732483.575,10,7,kernel,4.761905,"[25, 1, 1]",32,82105,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24437,0,0.297619,1,"[32, 16, 1]"
0,0,2.048,7,6939565732487.671,0,7,kernel,0.095238,"[2, 1, 1]",16,82118,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,24445,0,0.02381,1,"[128, 1, 1]"
53504,0,36.64,7,6939565732490.487,0,7,kernel,0.380952,"[1, 8, 1]",236,82132,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,24430,0,0.095238,1,"[128, 1, 1]"
0,0,1.088,7,6939565732528.407,10,7,kernel,4.761905,"[100, 1, 1]",16,82190,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24495,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565732532.407,10,7,kernel,4.761905,"[100, 1, 1]",16,82215,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24505,0,1.190476,1,"[128, 1, 1]"
0,0,2.464,7,6939565732738.615,10,7,kernel,4.761905,"[100, 1, 1]",22,82229,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24509,0,1.190476,1,"[128, 1, 1]"
0,0,3.04,7,6939565732980.886,20,7,kernel,9.523809,"[200, 1, 1]",16,82264,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,24530,0,2.380952,1,"[128, 1, 1]"
16384,0,10.464,7,6939565732984.79,25,7,kernel,12.190476,"[4, 2, 16]",57,82290,X,ampere_sgemm_128x32_nn,0,24541,0,1.52381,1,"[256, 1, 1]"
0,0,2.496,7,6939565732995.99,25,7,kernel,12.190476,"[16, 4, 1]",44,82292,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24541,0,0.761905,1,"[32, 16, 1]"
16384,0,8.16,7,6939565732999.286,25,7,kernel,12.190476,"[16, 8, 1]",57,82309,X,ampere_sgemm_32x128_nt,0,24545,0,1.52381,1,"[256, 1, 1]"
16,0,8.064,7,6939565733008.31,0,7,kernel,0.095238,"[2, 1, 1]",48,82322,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24549,0,0.02381,1,"[32, 4, 1]"
0,0,2.176,7,6939565733238.71,5,7,kernel,2.380952,"[50, 1, 1]",22,82337,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24560,0,0.595238,1,"[128, 1, 1]"
25600,0,7.136,7,6939565733241.59,6,7,kernel,3.047619,"[8, 1, 8]",80,82369,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,24570,0,0.761905,1,"[128, 1, 1]"
0,0,2.08,7,6939565733249.558,25,7,kernel,12.190476,"[16, 4, 1]",44,82370,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24570,0,0.761905,1,"[32, 16, 1]"
25600,0,5.952,7,6939565733252.342,6,7,kernel,3.047619,"[64, 1, 1]",80,82392,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,24574,0,0.761905,1,"[128, 1, 1]"
16,0,10.016,7,6939565733477.814,0,7,kernel,0.047619,"[1, 1, 1]",48,82404,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24578,0,0.011905,1,"[32, 4, 1]"
0,0,2.176,7,6939565733488.565,5,7,kernel,2.380952,"[50, 1, 1]",22,82419,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24589,0,0.595238,1,"[128, 1, 1]"
0,0,2.239,7,6939565733491.51,0,7,kernel,0.190476,"[2, 2, 1]",30,82436,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,24592,0,0.047619,1,"[128, 1, 1]"
0,0,11.232,7,6939565733689.877,100,7,kernel,97.523811,"[1024, 2, 1]",30,82455,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,24599,0,24.380953,1,"[128, 1, 1]"
16,0,3.456,7,6939565733701.909,5,7,kernel,2.380952,"[50, 1, 1]",40,82492,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,24606,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565733706.133,0,7,kernel,0.190476,"[2, 1, 1]",38,82494,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,24606,0,0.02381,1,"[256, 1, 1]"
0,0,2.112,7,6939565733924.981,5,7,kernel,2.380952,"[50, 1, 1]",22,82523,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24622,0,0.595238,1,"[128, 1, 1]"
25600,0,6.88,7,6939565734381.844,6,7,kernel,3.047619,"[8, 1, 8]",80,82553,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,24634,0,0.761905,1,"[128, 1, 1]"
0,0,2.336,7,6939565734389.524,25,7,kernel,12.190476,"[16, 4, 1]",44,82554,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24634,0,0.761905,1,"[32, 16, 1]"
25600,0,5.952,7,6939565734392.628,6,7,kernel,3.047619,"[64, 1, 1]",80,82576,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,24638,0,0.761905,1,"[128, 1, 1]"
16,0,8.0,7,6939565734399.38,0,7,kernel,0.047619,"[1, 1, 1]",48,82588,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24642,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565734408.116,5,7,kernel,2.380952,"[50, 1, 1]",22,82651,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,24684,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565734410.324,10,7,kernel,4.761905,"[25, 1, 1]",32,82665,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24685,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565734414.452,0,7,kernel,0.095238,"[2, 1, 1]",16,82678,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,24693,0,0.02381,1,"[128, 1, 1]"
53504,0,36.704,7,6939565735121.427,0,7,kernel,0.380952,"[1, 8, 1]",236,82692,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,24678,0,0.095238,1,"[128, 1, 1]"
0,0,1.568,7,6939565735158.995,15,7,kernel,7.142857,"[150, 1, 1]",16,82750,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24743,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565735163.507,15,7,kernel,7.142857,"[150, 1, 1]",16,82775,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24753,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565735167.603,15,7,kernel,7.142857,"[150, 1, 1]",22,82789,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24757,0,1.785714,1,"[128, 1, 1]"
0,0,1.248,7,6939565735170.067,15,7,kernel,7.142857,"[150, 1, 1]",16,82809,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,24764,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565735174.195,15,7,kernel,7.142857,"[150, 1, 1]",22,82823,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24768,0,1.785714,1,"[128, 1, 1]"
0,0,2.4,7,6939565735176.691,30,7,kernel,14.285714,"[300, 1, 1]",16,82858,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,24789,0,3.571429,1,"[128, 1, 1]"
16384,0,13.728,7,6939565735600.786,38,7,kernel,18.285715,"[4, 2, 24]",57,82884,X,ampere_sgemm_128x32_nn,0,24800,0,2.285714,1,"[256, 1, 1]"
0,0,3.072,7,6939565735615.25,25,7,kernel,12.190476,"[16, 4, 1]",44,82886,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24800,0,0.761905,1,"[32, 16, 1]"
16384,0,12.8,7,6939565735619.186,38,7,kernel,18.285715,"[16, 12, 1]",57,82903,X,ampere_sgemm_32x128_nt,0,24804,0,2.285714,1,"[256, 1, 1]"
16,0,7.776,7,6939565735632.722,0,7,kernel,0.142857,"[3, 1, 1]",48,82916,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24808,0,0.035714,1,"[32, 4, 1]"
0,0,1.952,7,6939565735641.266,5,7,kernel,2.380952,"[50, 1, 1]",22,82939,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24827,0,0.595238,1,"[128, 1, 1]"
16,0,2.848,7,6939565735643.89,5,7,kernel,2.380952,"[50, 1, 1]",40,82974,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,24830,0,0.595238,1,"[128, 1, 1]"
0,0,4.512,7,6939565735647.506,0,7,kernel,0.190476,"[2, 1, 1]",38,82976,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,24830,0,0.02381,1,"[256, 1, 1]"
0,0,1.792,7,6939565735652.85,5,7,kernel,2.380952,"[50, 1, 1]",22,83005,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24846,0,0.595238,1,"[128, 1, 1]"
16384,0,17.088,7,6939565736310.609,25,7,kernel,12.190476,"[16, 2, 4]",57,83031,X,ampere_sgemm_128x32_nn,0,24858,0,1.52381,1,"[256, 1, 1]"
0,0,3.136,7,6939565736328.433,67,7,kernel,48.761906,"[64, 4, 1]",44,83033,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24858,0,3.047619,1,"[32, 16, 1]"
16384,0,13.984,7,6939565736332.401,51,7,kernel,24.380953,"[16, 16, 1]",57,83050,X,ampere_sgemm_128x32_nt,0,24862,0,3.047619,1,"[256, 1, 1]"
16,0,8.064,7,6939565736347.249,0,7,kernel,0.047619,"[1, 1, 1]",48,83063,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24866,0,0.011905,1,"[32, 4, 1]"
0,0,2.56,7,6939565736356.017,20,7,kernel,9.523809,"[200, 1, 1]",22,83096,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24887,0,2.380952,1,"[128, 1, 1]"
0,0,3.04,7,6939565736359.313,20,7,kernel,9.523809,"[200, 1, 1]",22,83110,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,24892,0,2.380952,1,"[128, 1, 1]"
16384,0,15.168,7,6939565736363.089,44,7,kernel,21.333334,"[4, 2, 28]",57,83135,X,ampere_sgemm_128x32_nn,0,24902,0,2.666667,1,"[256, 1, 1]"
0,0,3.2,7,6939565736379.057,25,7,kernel,12.190476,"[16, 4, 1]",44,83137,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24902,0,0.761905,1,"[32, 16, 1]"
16384,0,14.976,7,6939565736383.025,51,7,kernel,24.380953,"[16, 16, 1]",57,83154,X,ampere_sgemm_32x128_nt,0,24906,0,3.047619,1,"[256, 1, 1]"
16,0,7.072,7,6939565736398.737,0,7,kernel,0.190476,"[4, 1, 1]",48,83167,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24910,0,0.047619,1,"[32, 4, 1]"
0,0,1.952,7,6939565736406.513,5,7,kernel,2.380952,"[50, 1, 1]",22,83190,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,24929,0,0.595238,1,"[128, 1, 1]"
16,0,2.88,7,6939565736409.169,5,7,kernel,2.380952,"[50, 1, 1]",40,83225,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,24932,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565736412.785,0,7,kernel,0.190476,"[2, 1, 1]",38,83227,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,24932,0,0.02381,1,"[256, 1, 1]"
0,0,2.24,7,6939565736977.616,5,7,kernel,2.380952,"[50, 1, 1]",22,83256,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,24948,0,0.595238,1,"[128, 1, 1]"
25600,0,7.264,7,6939565736980.592,6,7,kernel,3.047619,"[8, 1, 8]",80,83286,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,24960,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565736988.624,25,7,kernel,12.190476,"[16, 4, 1]",44,83287,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,24960,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565736991.568,6,7,kernel,3.047619,"[64, 1, 1]",80,83309,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,24964,0,0.761905,1,"[128, 1, 1]"
16,0,8.032,7,6939565736998.512,0,7,kernel,0.047619,"[1, 1, 1]",48,83321,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,24968,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565737007.28,5,7,kernel,2.380952,"[50, 1, 1]",22,83384,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,25010,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565737009.424,10,7,kernel,4.761905,"[25, 1, 1]",32,83398,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25011,0,0.297619,1,"[32, 16, 1]"
0,0,2.08,7,6939565737013.552,0,7,kernel,0.095238,"[2, 1, 1]",16,83411,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25019,0,0.02381,1,"[128, 1, 1]"
53504,0,36.8,7,6939565737302.799,0,7,kernel,0.380952,"[1, 8, 1]",236,83425,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,25004,0,0.095238,1,"[128, 1, 1]"
0,0,1.568,7,6939565737522.159,10,7,kernel,4.761905,"[100, 1, 1]",16,83483,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25069,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565737526.671,10,7,kernel,4.761905,"[100, 1, 1]",16,83508,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25079,0,1.190476,1,"[128, 1, 1]"
0,0,1.6,7,6939565737530.767,10,7,kernel,4.761905,"[100, 1, 1]",22,83522,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25083,0,1.190476,1,"[128, 1, 1]"
0,0,2.879,7,6939565737740.975,20,7,kernel,9.523809,"[200, 1, 1]",16,83557,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25104,0,2.380952,1,"[128, 1, 1]"
16384,0,9.888,7,6939565737938.99,25,7,kernel,12.190476,"[4, 2, 16]",57,83583,X,ampere_sgemm_128x32_nn,0,25115,0,1.52381,1,"[256, 1, 1]"
0,0,2.624,7,6939565737949.614,25,7,kernel,12.190476,"[16, 4, 1]",44,83585,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25115,0,0.761905,1,"[32, 16, 1]"
16384,0,8.16,7,6939565737953.07,25,7,kernel,12.190476,"[16, 8, 1]",57,83602,X,ampere_sgemm_32x128_nt,0,25119,0,1.52381,1,"[256, 1, 1]"
16,0,7.776,7,6939565737962.03,0,7,kernel,0.095238,"[2, 1, 1]",48,83615,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25123,0,0.02381,1,"[32, 4, 1]"
0,0,1.92,7,6939565737970.638,5,7,kernel,2.380952,"[50, 1, 1]",22,83630,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25134,0,0.595238,1,"[128, 1, 1]"
25600,0,6.848,7,6939565738173.326,6,7,kernel,3.047619,"[8, 1, 8]",80,83662,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,25144,0,0.761905,1,"[128, 1, 1]"
0,0,2.336,7,6939565738181.006,25,7,kernel,12.190476,"[16, 4, 1]",44,83663,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25144,0,0.761905,1,"[32, 16, 1]"
25600,0,5.984,7,6939565738184.142,6,7,kernel,3.047619,"[64, 1, 1]",80,83685,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,25148,0,0.761905,1,"[128, 1, 1]"
16,0,7.872,7,6939565738190.958,0,7,kernel,0.047619,"[1, 1, 1]",48,83697,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25152,0,0.011905,1,"[32, 4, 1]"
0,0,1.952,7,6939565738199.534,5,7,kernel,2.380952,"[50, 1, 1]",22,83712,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25163,0,0.595238,1,"[128, 1, 1]"
0,0,2.208,7,6939565738239.662,0,7,kernel,0.190476,"[2, 2, 1]",30,83729,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,25166,0,0.047619,1,"[128, 1, 1]"
0,0,10.784,7,6939565738445.837,100,7,kernel,97.523811,"[1024, 2, 1]",30,83748,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,25173,0,24.380953,1,"[128, 1, 1]"
16,0,3.232,7,6939565738457.422,5,7,kernel,2.380952,"[50, 1, 1]",40,83785,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,25180,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565738461.357,0,7,kernel,0.190476,"[2, 1, 1]",38,83787,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,25180,0,0.02381,1,"[256, 1, 1]"
0,0,2.144,7,6939565738675.309,5,7,kernel,2.380952,"[50, 1, 1]",22,83816,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,25196,0,0.595238,1,"[128, 1, 1]"
25600,0,7.168,7,6939565738678.285,6,7,kernel,3.047619,"[8, 1, 8]",80,83846,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,25208,0,0.761905,1,"[128, 1, 1]"
0,0,2.016,7,6939565738686.221,25,7,kernel,12.190476,"[16, 4, 1]",44,83847,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25208,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565738689.037,6,7,kernel,3.047619,"[64, 1, 1]",80,83869,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,25212,0,0.761905,1,"[128, 1, 1]"
16,0,7.712,7,6939565738879.149,0,7,kernel,0.047619,"[1, 1, 1]",48,83881,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25216,0,0.011905,1,"[32, 4, 1]"
0,0,2.176,7,6939565739176.556,5,7,kernel,2.380952,"[50, 1, 1]",22,83944,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,25258,0,0.595238,1,"[128, 1, 1]"
16,0,3.52,7,6939565739179.596,10,7,kernel,4.761905,"[25, 1, 1]",32,83958,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25259,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565739183.884,0,7,kernel,0.095238,"[2, 1, 1]",16,83971,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25267,0,0.02381,1,"[128, 1, 1]"
53504,0,36.832,7,6939565739186.668,0,7,kernel,0.380952,"[1, 8, 1]",236,83985,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,25252,0,0.095238,1,"[128, 1, 1]"
0,0,1.568,7,6939565739464.908,15,7,kernel,7.142857,"[150, 1, 1]",16,84043,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25317,0,1.785714,1,"[128, 1, 1]"
0,0,1.536,7,6939565739888.139,15,7,kernel,7.142857,"[150, 1, 1]",16,84068,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25327,0,1.785714,1,"[128, 1, 1]"
0,0,2.08,7,6939565739893.707,15,7,kernel,7.142857,"[150, 1, 1]",22,84082,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25331,0,1.785714,1,"[128, 1, 1]"
0,0,1.248,7,6939565739896.491,15,7,kernel,7.142857,"[150, 1, 1]",16,84102,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25338,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565739901.227,15,7,kernel,7.142857,"[150, 1, 1]",22,84116,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25342,0,1.785714,1,"[128, 1, 1]"
0,0,2.368,7,6939565739903.659,30,7,kernel,14.285714,"[300, 1, 1]",16,84151,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25363,0,3.571429,1,"[128, 1, 1]"
16384,0,13.184,7,6939565739906.795,38,7,kernel,18.285715,"[4, 2, 24]",57,84177,X,ampere_sgemm_128x32_nn,0,25374,0,2.285714,1,"[256, 1, 1]"
0,0,2.848,7,6939565739920.811,25,7,kernel,12.190476,"[16, 4, 1]",44,84179,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25374,0,0.761905,1,"[32, 16, 1]"
16384,0,13.696,7,6939565740341.034,38,7,kernel,18.285715,"[16, 12, 1]",57,84196,X,ampere_sgemm_32x128_nt,0,25378,0,2.285714,1,"[256, 1, 1]"
16,0,7.712,7,6939565740355.498,0,7,kernel,0.142857,"[3, 1, 1]",48,84209,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25382,0,0.035714,1,"[32, 4, 1]"
0,0,2.176,7,6939565740363.946,5,7,kernel,2.380952,"[50, 1, 1]",22,84232,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25401,0,0.595238,1,"[128, 1, 1]"
16,0,2.816,7,6939565740366.89,5,7,kernel,2.380952,"[50, 1, 1]",40,84267,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,25404,0,0.595238,1,"[128, 1, 1]"
0,0,4.544,7,6939565740370.506,0,7,kernel,0.190476,"[2, 1, 1]",38,84269,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,25404,0,0.02381,1,"[256, 1, 1]"
0,0,1.856,7,6939565740375.818,5,7,kernel,2.380952,"[50, 1, 1]",22,84298,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,25420,0,0.595238,1,"[128, 1, 1]"
16384,0,17.472,7,6939565741256.617,25,7,kernel,12.190476,"[16, 2, 4]",57,84324,X,ampere_sgemm_128x32_nn,0,25432,0,1.52381,1,"[256, 1, 1]"
0,0,3.136,7,6939565741274.825,67,7,kernel,48.761906,"[64, 4, 1]",44,84326,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25432,0,3.047619,1,"[32, 16, 1]"
16384,0,13.984,7,6939565741278.729,51,7,kernel,24.380953,"[16, 16, 1]",57,84343,X,ampere_sgemm_128x32_nt,0,25436,0,3.047619,1,"[256, 1, 1]"
16,0,7.904,7,6939565741293.513,0,7,kernel,0.047619,"[1, 1, 1]",48,84356,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25440,0,0.011905,1,"[32, 4, 1]"
0,0,2.4,7,6939565741302.185,20,7,kernel,9.523809,"[200, 1, 1]",22,84389,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,25461,0,2.380952,1,"[128, 1, 1]"
0,0,3.008,7,6939565741305.449,20,7,kernel,9.523809,"[200, 1, 1]",22,84403,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,25466,0,2.380952,1,"[128, 1, 1]"
16384,0,15.137,7,6939565741309.224,44,7,kernel,21.333334,"[4, 2, 28]",57,84428,X,ampere_sgemm_128x32_nn,0,25476,0,2.666667,1,"[256, 1, 1]"
0,0,3.743,7,6939565741325.161,25,7,kernel,12.190476,"[16, 4, 1]",44,84430,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25476,0,0.761905,1,"[32, 16, 1]"
16384,0,13.856,7,6939565741329.609,51,7,kernel,24.380953,"[16, 16, 1]",57,84447,X,ampere_sgemm_32x128_nt,0,25480,0,3.047619,1,"[256, 1, 1]"
16,0,7.04,7,6939565741344.169,0,7,kernel,0.190476,"[4, 1, 1]",48,84460,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25484,0,0.047619,1,"[32, 4, 1]"
0,0,1.856,7,6939565741351.945,5,7,kernel,2.380952,"[50, 1, 1]",22,84483,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25503,0,0.595238,1,"[128, 1, 1]"
16,0,2.944,7,6939565741354.537,5,7,kernel,2.380952,"[50, 1, 1]",40,84518,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,25506,0,0.595238,1,"[128, 1, 1]"
0,0,4.384,7,6939565741358.345,0,7,kernel,0.190476,"[2, 1, 1]",38,84520,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,25506,0,0.02381,1,"[256, 1, 1]"
0,0,1.665,7,6939565741363.528,5,7,kernel,2.380952,"[50, 1, 1]",22,84549,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,25522,0,0.595238,1,"[128, 1, 1]"
25600,0,7.008,7,6939565741366.025,6,7,kernel,3.047619,"[8, 1, 8]",80,84579,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,25534,0,0.761905,1,"[128, 1, 1]"
0,0,1.953,7,6939565741373.832,25,7,kernel,12.190476,"[16, 4, 1]",44,84580,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25534,0,0.761905,1,"[32, 16, 1]"
25600,0,6.144,7,6939565741376.489,6,7,kernel,3.047619,"[64, 1, 1]",80,84602,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,25538,0,0.761905,1,"[128, 1, 1]"
16,0,6.848,7,6939565741383.432,0,7,kernel,0.047619,"[1, 1, 1]",48,84614,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25542,0,0.011905,1,"[32, 4, 1]"
0,0,2.239,7,6939565742708.903,5,7,kernel,2.380952,"[50, 1, 1]",22,84677,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,25584,0,0.595238,1,"[128, 1, 1]"
16,0,3.553,7,6939565742711.942,10,7,kernel,4.761905,"[25, 1, 1]",32,84691,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25585,0,0.297619,1,"[32, 16, 1]"
0,0,2.017,7,6939565742716.23,0,7,kernel,0.095238,"[2, 1, 1]",16,84704,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25593,0,0.02381,1,"[128, 1, 1]"
53504,0,36.768,7,6939565742719.014,0,7,kernel,0.380952,"[1, 8, 1]",236,84718,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,25578,0,0.095238,1,"[128, 1, 1]"
0,0,1.121,7,6939565742756.71,10,7,kernel,4.761905,"[100, 1, 1]",16,84776,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25643,0,1.190476,1,"[128, 1, 1]"
0,0,1.185,7,6939565742760.71,10,7,kernel,4.761905,"[100, 1, 1]",16,84801,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25653,0,1.190476,1,"[128, 1, 1]"
0,0,1.6,7,6939565742765.062,10,7,kernel,4.761905,"[100, 1, 1]",22,84815,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25657,0,1.190476,1,"[128, 1, 1]"
0,0,2.144,7,6939565742767.463,20,7,kernel,9.523809,"[200, 1, 1]",16,84850,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25678,0,2.380952,1,"[128, 1, 1]"
16384,0,9.824,7,6939565742770.438,25,7,kernel,12.190476,"[4, 2, 16]",57,84876,X,ampere_sgemm_128x32_nn,0,25689,0,1.52381,1,"[256, 1, 1]"
0,0,2.527,7,6939565742780.935,25,7,kernel,12.190476,"[16, 4, 1]",44,84878,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25689,0,0.761905,1,"[32, 16, 1]"
16384,0,8.192,7,6939565742784.23,25,7,kernel,12.190476,"[16, 8, 1]",57,84895,X,ampere_sgemm_32x128_nt,0,25693,0,1.52381,1,"[256, 1, 1]"
16,0,7.744,7,6939565742793.542,0,7,kernel,0.095238,"[2, 1, 1]",48,84908,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25697,0,0.02381,1,"[32, 4, 1]"
0,0,1.856,7,6939565742802.022,5,7,kernel,2.380952,"[50, 1, 1]",22,84923,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25708,0,0.595238,1,"[128, 1, 1]"
25600,0,6.944,7,6939565742804.646,6,7,kernel,3.047619,"[8, 1, 8]",80,84955,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,25718,0,0.761905,1,"[128, 1, 1]"
0,0,2.016,7,6939565742812.326,25,7,kernel,12.190476,"[16, 4, 1]",44,84956,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25718,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565742815.11,6,7,kernel,3.047619,"[64, 1, 1]",80,84978,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,25722,0,0.761905,1,"[128, 1, 1]"
16,0,10.112,7,6939565744792.035,0,7,kernel,0.047619,"[1, 1, 1]",48,84990,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25726,0,0.011905,1,"[32, 4, 1]"
0,0,2.304,7,6939565744802.915,5,7,kernel,2.380952,"[50, 1, 1]",22,85005,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25737,0,0.595238,1,"[128, 1, 1]"
0,0,2.24,7,6939565744805.987,0,7,kernel,0.190476,"[2, 2, 1]",30,85022,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,25740,0,0.047619,1,"[128, 1, 1]"
0,0,11.36,7,6939565744808.931,100,7,kernel,97.523811,"[1024, 2, 1]",30,85041,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,25747,0,24.380953,1,"[128, 1, 1]"
16,0,2.784,7,6939565744821.123,5,7,kernel,2.380952,"[50, 1, 1]",40,85078,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,25754,0,0.595238,1,"[128, 1, 1]"
0,0,4.512,7,6939565744824.707,0,7,kernel,0.190476,"[2, 1, 1]",38,85080,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,25754,0,0.02381,1,"[256, 1, 1]"
0,0,1.792,7,6939565744830.083,5,7,kernel,2.380952,"[50, 1, 1]",22,85109,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,25770,0,0.595238,1,"[128, 1, 1]"
25600,0,6.976,7,6939565744832.707,6,7,kernel,3.047619,"[8, 1, 8]",80,85139,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,25782,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565744840.419,25,7,kernel,12.190476,"[16, 4, 1]",44,85140,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25782,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565744843.235,6,7,kernel,3.047619,"[64, 1, 1]",80,85162,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,25786,0,0.761905,1,"[128, 1, 1]"
16,0,7.296,7,6939565744850.179,0,7,kernel,0.047619,"[1, 1, 1]",48,85174,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25790,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565744858.243,5,7,kernel,2.380952,"[50, 1, 1]",22,85237,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,25832,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565744860.707,10,7,kernel,4.761905,"[25, 1, 1]",32,85251,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25833,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565744864.835,0,7,kernel,0.095238,"[2, 1, 1]",16,85264,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25841,0,0.02381,1,"[128, 1, 1]"
53504,0,36.896,7,6939565744867.651,0,7,kernel,0.380952,"[1, 8, 1]",236,85278,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,25826,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565744905.667,15,7,kernel,7.142857,"[150, 1, 1]",16,85336,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25891,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565744909.475,15,7,kernel,7.142857,"[150, 1, 1]",16,85361,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25901,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565744913.795,15,7,kernel,7.142857,"[150, 1, 1]",22,85375,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25905,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565744916.259,15,7,kernel,7.142857,"[150, 1, 1]",16,85395,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,25912,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565744920.387,15,7,kernel,7.142857,"[150, 1, 1]",22,85409,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25916,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565744922.883,30,7,kernel,14.285714,"[300, 1, 1]",16,85444,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,25937,0,3.571429,1,"[128, 1, 1]"
16384,0,13.28,7,6939565745141.442,38,7,kernel,18.285715,"[4, 2, 24]",57,85470,X,ampere_sgemm_128x32_nn,0,25948,0,2.285714,1,"[256, 1, 1]"
0,0,3.008,7,6939565745155.554,25,7,kernel,12.190476,"[16, 4, 1]",44,85472,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,25948,0,0.761905,1,"[32, 16, 1]"
16384,0,12.735,7,6939565745159.331,38,7,kernel,18.285715,"[16, 12, 1]",57,85489,X,ampere_sgemm_32x128_nt,0,25952,0,2.285714,1,"[256, 1, 1]"
16,0,7.743,7,6939565745172.835,0,7,kernel,0.142857,"[3, 1, 1]",48,85502,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,25956,0,0.035714,1,"[32, 4, 1]"
0,0,2.144,7,6939565745424.93,5,7,kernel,2.380952,"[50, 1, 1]",22,85525,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,25975,0,0.595238,1,"[128, 1, 1]"
16,0,3.04,7,6939565745427.81,5,7,kernel,2.380952,"[50, 1, 1]",40,85560,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,25978,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565745431.586,0,7,kernel,0.190476,"[2, 1, 1]",38,85562,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,25978,0,0.02381,1,"[256, 1, 1]"
0,0,2.048,7,6939565745631.266,5,7,kernel,2.380952,"[50, 1, 1]",22,85591,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,25994,0,0.595238,1,"[128, 1, 1]"
16384,0,17.312,7,6939565745685.698,25,7,kernel,12.190476,"[16, 2, 4]",57,85617,X,ampere_sgemm_128x32_nn,0,26006,0,1.52381,1,"[256, 1, 1]"
0,0,2.88,7,6939565745703.778,67,7,kernel,48.761906,"[64, 4, 1]",44,85619,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26006,0,3.047619,1,"[32, 16, 1]"
16384,0,14.912,7,6939565745945.793,51,7,kernel,24.380953,"[16, 16, 1]",57,85636,X,ampere_sgemm_128x32_nt,0,26010,0,3.047619,1,"[256, 1, 1]"
16,0,8.096,7,6939565745961.537,0,7,kernel,0.047619,"[1, 1, 1]",48,85649,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26014,0,0.011905,1,"[32, 4, 1]"
0,0,2.848,7,6939565746167.713,20,7,kernel,9.523809,"[200, 1, 1]",22,85682,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26035,0,2.380952,1,"[128, 1, 1]"
0,0,2.976,7,6939565746171.329,20,7,kernel,9.523809,"[200, 1, 1]",22,85696,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,26040,0,2.380952,1,"[128, 1, 1]"
16384,0,16.64,7,6939565746175.105,44,7,kernel,21.333334,"[4, 2, 28]",57,85721,X,ampere_sgemm_128x32_nn,0,26050,0,2.666667,1,"[256, 1, 1]"
0,0,3.68,7,6939565746192.481,25,7,kernel,12.190476,"[16, 4, 1]",44,85723,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26050,0,0.761905,1,"[32, 16, 1]"
16384,0,14.56,7,6939565746196.897,51,7,kernel,24.380953,"[16, 16, 1]",57,85740,X,ampere_sgemm_32x128_nt,0,26054,0,3.047619,1,"[256, 1, 1]"
16,0,9.472,7,6939565746426.112,0,7,kernel,0.190476,"[4, 1, 1]",48,85753,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26058,0,0.047619,1,"[32, 4, 1]"
0,0,2.272,7,6939565746436.288,5,7,kernel,2.380952,"[50, 1, 1]",22,85776,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26077,0,0.595238,1,"[128, 1, 1]"
16,0,3.105,7,6939565746883.647,5,7,kernel,2.380952,"[50, 1, 1]",40,85811,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,26080,0,0.595238,1,"[128, 1, 1]"
0,0,4.768,7,6939565746887.552,0,7,kernel,0.190476,"[2, 1, 1]",38,85813,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,26080,0,0.02381,1,"[256, 1, 1]"
0,0,1.792,7,6939565746893.088,5,7,kernel,2.380952,"[50, 1, 1]",22,85842,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26096,0,0.595238,1,"[128, 1, 1]"
25600,0,7.135,7,6939565746895.712,6,7,kernel,3.047619,"[8, 1, 8]",80,85872,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,26108,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565746903.68,25,7,kernel,12.190476,"[16, 4, 1]",44,85873,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26108,0,0.761905,1,"[32, 16, 1]"
25600,0,6.081,7,6939565746906.527,6,7,kernel,3.047619,"[64, 1, 1]",80,85895,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,26112,0,0.761905,1,"[128, 1, 1]"
16,0,7.84,7,6939565746913.696,0,7,kernel,0.047619,"[1, 1, 1]",48,85907,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26116,0,0.011905,1,"[32, 4, 1]"
0,0,2.24,7,6939565747275.455,5,7,kernel,2.380952,"[50, 1, 1]",22,85970,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,26158,0,0.595238,1,"[128, 1, 1]"
16,0,3.52,7,6939565747278.431,10,7,kernel,4.761905,"[25, 1, 1]",32,85984,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26159,0,0.297619,1,"[32, 16, 1]"
0,0,2.368,7,6939565748631.741,0,7,kernel,0.095238,"[2, 1, 1]",16,85997,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,26167,0,0.02381,1,"[128, 1, 1]"
53504,0,37.152,7,6939565748634.877,0,7,kernel,0.380952,"[1, 8, 1]",236,86011,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,26152,0,0.095238,1,"[128, 1, 1]"
0,0,1.312,7,6939565748672.925,10,7,kernel,4.761905,"[100, 1, 1]",16,86069,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26217,0,1.190476,1,"[128, 1, 1]"
0,0,1.217,7,6939565748677.116,10,7,kernel,4.761905,"[100, 1, 1]",16,86094,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26227,0,1.190476,1,"[128, 1, 1]"
0,0,1.6,7,6939565748681.245,10,7,kernel,4.761905,"[100, 1, 1]",22,86108,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26231,0,1.190476,1,"[128, 1, 1]"
0,0,2.176,7,6939565748683.581,20,7,kernel,9.523809,"[200, 1, 1]",16,86143,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,26252,0,2.380952,1,"[128, 1, 1]"
16384,0,9.632,7,6939565748686.524,25,7,kernel,12.190476,"[4, 2, 16]",57,86169,X,ampere_sgemm_128x32_nn,0,26263,0,1.52381,1,"[256, 1, 1]"
0,0,2.944,7,6939565748696.893,25,7,kernel,12.190476,"[16, 4, 1]",44,86171,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26263,0,0.761905,1,"[32, 16, 1]"
16384,0,8.192,7,6939565748700.669,25,7,kernel,12.190476,"[16, 8, 1]",57,86188,X,ampere_sgemm_32x128_nt,0,26267,0,1.52381,1,"[256, 1, 1]"
16,0,7.904,7,6939565748709.661,0,7,kernel,0.095238,"[2, 1, 1]",48,86201,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26271,0,0.02381,1,"[32, 4, 1]"
0,0,1.791,7,6939565748718.269,5,7,kernel,2.380952,"[50, 1, 1]",22,86216,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26282,0,0.595238,1,"[128, 1, 1]"
25600,0,6.977,7,6939565748720.924,6,7,kernel,3.047619,"[8, 1, 8]",80,86248,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,26292,0,0.761905,1,"[128, 1, 1]"
0,0,2.015,7,6939565748728.765,25,7,kernel,12.190476,"[16, 4, 1]",44,86249,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26292,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565748731.517,6,7,kernel,3.047619,"[64, 1, 1]",80,86271,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,26296,0,0.761905,1,"[128, 1, 1]"
16,0,5.855,7,6939565748738.397,0,7,kernel,0.047619,"[1, 1, 1]",48,86283,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26300,0,0.011905,1,"[32, 4, 1]"
0,0,1.856,7,6939565748745.053,5,7,kernel,2.380952,"[50, 1, 1]",22,86298,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26311,0,0.595238,1,"[128, 1, 1]"
0,0,2.081,7,6939565748747.644,0,7,kernel,0.190476,"[2, 2, 1]",30,86315,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,26314,0,0.047619,1,"[128, 1, 1]"
0,0,11.072,7,6939565749010.396,100,7,kernel,97.523811,"[1024, 2, 1]",30,86334,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,26321,0,24.380953,1,"[128, 1, 1]"
16,0,3.456,7,6939565749022.172,5,7,kernel,2.380952,"[50, 1, 1]",40,86371,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,26328,0,0.595238,1,"[128, 1, 1]"
0,0,4.672,7,6939565749026.396,0,7,kernel,0.190476,"[2, 1, 1]",38,86373,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,26328,0,0.02381,1,"[256, 1, 1]"
0,0,2.112,7,6939565749252.508,5,7,kernel,2.380952,"[50, 1, 1]",22,86402,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26344,0,0.595238,1,"[128, 1, 1]"
25600,0,7.2,7,6939565749255.324,6,7,kernel,3.047619,"[8, 1, 8]",80,86432,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,26356,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565749263.356,25,7,kernel,12.190476,"[16, 4, 1]",44,86433,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26356,0,0.761905,1,"[32, 16, 1]"
25600,0,6.016,7,6939565749280.348,6,7,kernel,3.047619,"[64, 1, 1]",80,86455,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,26360,0,0.761905,1,"[128, 1, 1]"
16,0,8.064,7,6939565749328.636,0,7,kernel,0.047619,"[1, 1, 1]",48,86467,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26364,0,0.011905,1,"[32, 4, 1]"
0,0,2.08,7,6939565749747.387,5,7,kernel,2.380952,"[50, 1, 1]",22,86530,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,26406,0,0.595238,1,"[128, 1, 1]"
16,0,3.52,7,6939565749750.235,10,7,kernel,4.761905,"[25, 1, 1]",32,86544,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26407,0,0.297619,1,"[32, 16, 1]"
0,0,2.336,7,6939565749962.811,0,7,kernel,0.095238,"[2, 1, 1]",16,86557,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,26415,0,0.02381,1,"[128, 1, 1]"
53504,0,37.248,7,6939565749965.947,0,7,kernel,0.380952,"[1, 8, 1]",236,86571,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,26400,0,0.095238,1,"[128, 1, 1]"
0,0,1.6,7,6939565750459.13,15,7,kernel,7.142857,"[150, 1, 1]",16,86629,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26465,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565750464.602,15,7,kernel,7.142857,"[150, 1, 1]",16,86654,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26475,0,1.785714,1,"[128, 1, 1]"
0,0,1.729,7,6939565750469.081,15,7,kernel,7.142857,"[150, 1, 1]",22,86668,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26479,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565750471.546,15,7,kernel,7.142857,"[150, 1, 1]",16,86688,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26486,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565750476.122,15,7,kernel,7.142857,"[150, 1, 1]",22,86702,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26490,0,1.785714,1,"[128, 1, 1]"
0,0,3.136,7,6939565750729.945,30,7,kernel,14.285714,"[300, 1, 1]",16,86737,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,26511,0,3.571429,1,"[128, 1, 1]"
16384,0,13.727,7,6939565751811.224,38,7,kernel,18.285715,"[4, 2, 24]",57,86763,X,ampere_sgemm_128x32_nn,0,26522,0,2.285714,1,"[256, 1, 1]"
0,0,3.039,7,6939565751825.752,25,7,kernel,12.190476,"[16, 4, 1]",44,86765,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26522,0,0.761905,1,"[32, 16, 1]"
16384,0,11.872,7,6939565751829.527,38,7,kernel,18.285715,"[16, 12, 1]",57,86782,X,ampere_sgemm_32x128_nt,0,26526,0,2.285714,1,"[256, 1, 1]"
16,0,7.873,7,6939565751842.263,0,7,kernel,0.142857,"[3, 1, 1]",48,86795,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26530,0,0.035714,1,"[32, 4, 1]"
0,0,1.887,7,6939565751850.936,5,7,kernel,2.380952,"[50, 1, 1]",22,86818,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26549,0,0.595238,1,"[128, 1, 1]"
16,0,2.847,7,6939565751853.56,5,7,kernel,2.380952,"[50, 1, 1]",40,86853,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,26552,0,0.595238,1,"[128, 1, 1]"
0,0,4.447,7,6939565751857.144,0,7,kernel,0.190476,"[2, 1, 1]",38,86855,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,26552,0,0.02381,1,"[256, 1, 1]"
16,0,2.753,7,6939565751862.359,5,7,kernel,2.380952,"[50, 1, 1]",40,86896,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,26566,0,0.595238,1,"[128, 1, 1]"
0,0,4.415,7,6939565751865.944,0,7,kernel,0.190476,"[2, 1, 1]",38,86898,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,26566,0,0.02381,1,"[256, 1, 1]"
0,0,1.728,7,6939565751871.159,5,7,kernel,2.380952,"[50, 1, 1]",22,86927,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26582,0,0.595238,1,"[128, 1, 1]"
16384,0,16.223,7,6939565751873.624,25,7,kernel,12.190476,"[16, 2, 4]",57,86953,X,ampere_sgemm_128x32_nn,0,26594,0,1.52381,1,"[256, 1, 1]"
0,0,2.913,7,6939565751890.647,67,7,kernel,48.761906,"[64, 4, 1]",44,86955,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26594,0,3.047619,1,"[32, 16, 1]"
16384,0,14.336,7,6939565751894.263,51,7,kernel,24.380953,"[16, 16, 1]",57,86972,X,ampere_sgemm_128x32_nt,0,26598,0,3.047619,1,"[256, 1, 1]"
16,0,5.984,7,6939565751909.399,0,7,kernel,0.047619,"[1, 1, 1]",48,86985,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26602,0,0.011905,1,"[32, 4, 1]"
0,0,2.271,7,6939565751916.184,20,7,kernel,9.523809,"[200, 1, 1]",22,87018,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26623,0,2.380952,1,"[128, 1, 1]"
0,0,2.976,7,6939565751919.255,20,7,kernel,9.523809,"[200, 1, 1]",22,87032,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,26628,0,2.380952,1,"[128, 1, 1]"
16384,0,16.736,7,6939565752156.727,44,7,kernel,21.333334,"[4, 2, 28]",57,87057,X,ampere_sgemm_128x32_nn,0,26638,0,2.666667,1,"[256, 1, 1]"
0,0,3.648,7,6939565752174.167,25,7,kernel,12.190476,"[16, 4, 1]",44,87059,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26638,0,0.761905,1,"[32, 16, 1]"
16384,0,15.072,7,6939565752178.615,51,7,kernel,24.380953,"[16, 16, 1]",57,87076,X,ampere_sgemm_32x128_nt,0,26642,0,3.047619,1,"[256, 1, 1]"
16,0,7.648,7,6939565752194.519,0,7,kernel,0.190476,"[4, 1, 1]",48,87089,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26646,0,0.047619,1,"[32, 4, 1]"
0,0,2.273,7,6939565752431.926,5,7,kernel,2.380952,"[50, 1, 1]",22,87112,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26665,0,0.595238,1,"[128, 1, 1]"
16,0,3.04,7,6939565752434.999,5,7,kernel,2.380952,"[50, 1, 1]",40,87147,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,26668,0,0.595238,1,"[128, 1, 1]"
0,0,4.48,7,6939565752438.774,0,7,kernel,0.190476,"[2, 1, 1]",38,87149,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,26668,0,0.02381,1,"[256, 1, 1]"
0,0,2.208,7,6939565752660.15,5,7,kernel,2.380952,"[50, 1, 1]",22,87178,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26684,0,0.595238,1,"[128, 1, 1]"
25600,0,7.104,7,6939565752663.158,6,7,kernel,3.047619,"[8, 1, 8]",80,87208,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,26696,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565752670.998,25,7,kernel,12.190476,"[16, 4, 1]",44,87209,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26696,0,0.761905,1,"[32, 16, 1]"
25600,0,6.144,7,6939565752673.814,6,7,kernel,3.047619,"[64, 1, 1]",80,87231,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,26700,0,0.761905,1,"[128, 1, 1]"
16,0,9.984,7,6939565752901.974,0,7,kernel,0.047619,"[1, 1, 1]",48,87243,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26704,0,0.011905,1,"[32, 4, 1]"
0,0,2.144,7,6939565753126.262,5,7,kernel,2.380952,"[50, 1, 1]",22,87306,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,26746,0,0.595238,1,"[128, 1, 1]"
16,0,3.551,7,6939565753129.238,10,7,kernel,4.761905,"[25, 1, 1]",32,87320,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26747,0,0.297619,1,"[32, 16, 1]"
0,0,2.015,7,6939565753133.526,0,7,kernel,0.095238,"[2, 1, 1]",16,87333,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,26755,0,0.02381,1,"[128, 1, 1]"
53504,0,36.735,7,6939565753136.31,0,7,kernel,0.380952,"[1, 8, 1]",236,87347,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,26740,0,0.095238,1,"[128, 1, 1]"
0,0,1.6,7,6939565753447.957,15,7,kernel,7.142857,"[150, 1, 1]",16,87405,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26805,0,1.785714,1,"[128, 1, 1]"
0,0,1.6,7,6939565753806.292,15,7,kernel,7.142857,"[150, 1, 1]",16,87430,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26815,0,1.785714,1,"[128, 1, 1]"
0,0,2.208,7,6939565753811.412,15,7,kernel,7.142857,"[150, 1, 1]",22,87444,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26819,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565753814.356,15,7,kernel,7.142857,"[150, 1, 1]",16,87464,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,26826,0,1.785714,1,"[128, 1, 1]"
0,0,1.632,7,6939565753818.676,15,7,kernel,7.142857,"[150, 1, 1]",22,87478,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26830,0,1.785714,1,"[128, 1, 1]"
0,0,2.4,7,6939565753821.172,30,7,kernel,14.285714,"[300, 1, 1]",16,87513,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,26851,0,3.571429,1,"[128, 1, 1]"
16384,0,13.632,7,6939565754159.86,38,7,kernel,18.285715,"[4, 2, 24]",57,87539,X,ampere_sgemm_128x32_nn,0,26862,0,2.285714,1,"[256, 1, 1]"
0,0,3.104,7,6939565754174.356,25,7,kernel,12.190476,"[16, 4, 1]",44,87541,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26862,0,0.761905,1,"[32, 16, 1]"
16384,0,12.095,7,6939565754178.228,38,7,kernel,18.285715,"[16, 12, 1]",57,87558,X,ampere_sgemm_32x128_nt,0,26866,0,2.285714,1,"[256, 1, 1]"
16,0,7.776,7,6939565754191.156,0,7,kernel,0.142857,"[3, 1, 1]",48,87571,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26870,0,0.035714,1,"[32, 4, 1]"
0,0,1.888,7,6939565754199.7,5,7,kernel,2.380952,"[50, 1, 1]",22,87594,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26889,0,0.595238,1,"[128, 1, 1]"
16,0,2.849,7,6939565754202.323,5,7,kernel,2.380952,"[50, 1, 1]",40,87629,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,26892,0,0.595238,1,"[128, 1, 1]"
0,0,4.511,7,6939565754205.908,0,7,kernel,0.190476,"[2, 1, 1]",38,87631,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,26892,0,0.02381,1,"[256, 1, 1]"
0,0,2.272,7,6939565756470.928,5,7,kernel,2.380952,"[50, 1, 1]",22,87660,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26908,0,0.595238,1,"[128, 1, 1]"
16384,0,17.248,7,6939565756473.936,25,7,kernel,12.190476,"[16, 2, 4]",57,87686,X,ampere_sgemm_128x32_nn,0,26920,0,1.52381,1,"[256, 1, 1]"
0,0,2.88,7,6939565756491.984,67,7,kernel,48.761906,"[64, 4, 1]",44,87688,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26920,0,3.047619,1,"[32, 16, 1]"
16384,0,14.016,7,6939565756495.664,51,7,kernel,24.380953,"[16, 16, 1]",57,87705,X,ampere_sgemm_128x32_nt,0,26924,0,3.047619,1,"[256, 1, 1]"
16,0,7.904,7,6939565756510.448,0,7,kernel,0.047619,"[1, 1, 1]",48,87718,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26928,0,0.011905,1,"[32, 4, 1]"
0,0,2.496,7,6939565756519.152,20,7,kernel,9.523809,"[200, 1, 1]",22,87751,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,26949,0,2.380952,1,"[128, 1, 1]"
0,0,3.072,7,6939565756522.448,20,7,kernel,9.523809,"[200, 1, 1]",22,87765,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,26954,0,2.380952,1,"[128, 1, 1]"
16384,0,15.136,7,6939565756526.224,44,7,kernel,21.333334,"[4, 2, 28]",57,87790,X,ampere_sgemm_128x32_nn,0,26964,0,2.666667,1,"[256, 1, 1]"
0,0,3.488,7,6939565756542.16,25,7,kernel,12.190476,"[16, 4, 1]",44,87792,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,26964,0,0.761905,1,"[32, 16, 1]"
16384,0,13.984,7,6939565756546.416,51,7,kernel,24.380953,"[16, 16, 1]",57,87809,X,ampere_sgemm_32x128_nt,0,26968,0,3.047619,1,"[256, 1, 1]"
16,0,6.944,7,6939565756561.136,0,7,kernel,0.190476,"[4, 1, 1]",48,87822,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,26972,0,0.047619,1,"[32, 4, 1]"
0,0,1.824,7,6939565756568.752,5,7,kernel,2.380952,"[50, 1, 1]",22,87845,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,26991,0,0.595238,1,"[128, 1, 1]"
16,0,2.88,7,6939565756571.408,5,7,kernel,2.380952,"[50, 1, 1]",40,87880,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,26994,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565756575.152,0,7,kernel,0.190476,"[2, 1, 1]",38,87882,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,26994,0,0.02381,1,"[256, 1, 1]"
0,0,1.632,7,6939565756580.464,5,7,kernel,2.380952,"[50, 1, 1]",22,87911,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27010,0,0.595238,1,"[128, 1, 1]"
25600,0,6.784,7,6939565756582.8,6,7,kernel,3.047619,"[8, 1, 8]",80,87941,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,27022,0,0.761905,1,"[128, 1, 1]"
0,0,1.984,7,6939565756590.384,25,7,kernel,12.190476,"[16, 4, 1]",44,87942,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27022,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565756593.2,6,7,kernel,3.047619,"[64, 1, 1]",80,87964,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,27026,0,0.761905,1,"[128, 1, 1]"
16,0,6.912,7,6939565756599.984,0,7,kernel,0.047619,"[1, 1, 1]",48,87976,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27030,0,0.011905,1,"[32, 4, 1]"
0,0,1.407,7,6939565756607.76,5,7,kernel,2.380952,"[50, 1, 1]",22,88039,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,27072,0,0.595238,1,"[128, 1, 1]"
16,0,3.424,7,6939565756609.968,10,7,kernel,4.761905,"[25, 1, 1]",32,88053,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27073,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565756614.224,0,7,kernel,0.095238,"[2, 1, 1]",16,88066,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,27081,0,0.02381,1,"[128, 1, 1]"
53504,0,36.704,7,6939565756617.04,0,7,kernel,0.380952,"[1, 8, 1]",236,88080,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,27066,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565756654.896,15,7,kernel,7.142857,"[150, 1, 1]",16,88138,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27131,0,1.785714,1,"[128, 1, 1]"
0,0,1.215,7,6939565756658.864,15,7,kernel,7.142857,"[150, 1, 1]",16,88163,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27141,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565756662.96,15,7,kernel,7.142857,"[150, 1, 1]",22,88177,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27145,0,1.785714,1,"[128, 1, 1]"
0,0,1.217,7,6939565756665.423,15,7,kernel,7.142857,"[150, 1, 1]",16,88197,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27152,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565756669.552,15,7,kernel,7.142857,"[150, 1, 1]",22,88211,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27156,0,1.785714,1,"[128, 1, 1]"
0,0,2.241,7,6939565756672.047,30,7,kernel,14.285714,"[300, 1, 1]",16,88246,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,27177,0,3.571429,1,"[128, 1, 1]"
16384,0,12.959,7,6939565756675.152,38,7,kernel,18.285715,"[4, 2, 24]",57,88272,X,ampere_sgemm_128x32_nn,0,27188,0,2.285714,1,"[256, 1, 1]"
0,0,4.193,7,6939565756688.911,25,7,kernel,12.190476,"[16, 4, 1]",44,88274,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27188,0,0.761905,1,"[32, 16, 1]"
16384,0,11.649,7,6939565756693.839,38,7,kernel,18.285715,"[16, 12, 1]",57,88291,X,ampere_sgemm_32x128_nt,0,27192,0,2.285714,1,"[256, 1, 1]"
16,0,7.84,7,6939565756706.287,0,7,kernel,0.142857,"[3, 1, 1]",48,88304,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27196,0,0.035714,1,"[32, 4, 1]"
0,0,2.304,7,6939565756980.751,5,7,kernel,2.380952,"[50, 1, 1]",22,88327,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27215,0,0.595238,1,"[128, 1, 1]"
16,0,3.072,7,6939565756983.791,5,7,kernel,2.380952,"[50, 1, 1]",40,88362,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,27218,0,0.595238,1,"[128, 1, 1]"
0,0,4.512,7,6939565756987.727,0,7,kernel,0.190476,"[2, 1, 1]",38,88364,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,27218,0,0.02381,1,"[256, 1, 1]"
0,0,2.015,7,6939565757318.287,5,7,kernel,2.380952,"[50, 1, 1]",22,88393,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27234,0,0.595238,1,"[128, 1, 1]"
16384,0,17.472,7,6939565757321.039,25,7,kernel,12.190476,"[16, 2, 4]",57,88419,X,ampere_sgemm_128x32_nn,0,27246,0,1.52381,1,"[256, 1, 1]"
0,0,2.816,7,6939565757339.278,67,7,kernel,48.761906,"[64, 4, 1]",44,88421,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27246,0,3.047619,1,"[32, 16, 1]"
16384,0,14.496,7,6939565757342.863,51,7,kernel,24.380953,"[16, 16, 1]",57,88438,X,ampere_sgemm_128x32_nt,0,27250,0,3.047619,1,"[256, 1, 1]"
16,0,7.936,7,6939565757358.126,0,7,kernel,0.047619,"[1, 1, 1]",48,88451,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27254,0,0.011905,1,"[32, 4, 1]"
0,0,2.209,7,6939565757366.894,20,7,kernel,9.523809,"[200, 1, 1]",22,88484,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27275,0,2.380952,1,"[128, 1, 1]"
0,0,3.936,7,6939565758376.909,20,7,kernel,9.523809,"[200, 1, 1]",22,88498,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,27280,0,2.380952,1,"[128, 1, 1]"
16384,0,16.736,7,6939565758381.645,44,7,kernel,21.333334,"[4, 2, 28]",57,88523,X,ampere_sgemm_128x32_nn,0,27290,0,2.666667,1,"[256, 1, 1]"
0,0,3.456,7,6939565758399.213,25,7,kernel,12.190476,"[16, 4, 1]",44,88525,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27290,0,0.761905,1,"[32, 16, 1]"
16384,0,14.368,7,6939565758403.437,51,7,kernel,24.380953,"[16, 16, 1]",57,88542,X,ampere_sgemm_32x128_nt,0,27294,0,3.047619,1,"[256, 1, 1]"
16,0,7.616,7,6939565758418.637,0,7,kernel,0.190476,"[4, 1, 1]",48,88555,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27298,0,0.047619,1,"[32, 4, 1]"
0,0,1.76,7,6939565758426.957,5,7,kernel,2.380952,"[50, 1, 1]",22,88578,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27317,0,0.595238,1,"[128, 1, 1]"
16,0,3.232,7,6939565758429.453,5,7,kernel,2.380952,"[50, 1, 1]",40,88613,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,27320,0,0.595238,1,"[128, 1, 1]"
0,0,4.48,7,6939565758433.517,0,7,kernel,0.190476,"[2, 1, 1]",38,88615,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,27320,0,0.02381,1,"[256, 1, 1]"
0,0,1.632,7,6939565758438.893,5,7,kernel,2.380952,"[50, 1, 1]",22,88644,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27336,0,0.595238,1,"[128, 1, 1]"
25600,0,6.976,7,6939565758441.357,6,7,kernel,3.047619,"[8, 1, 8]",80,88674,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,27348,0,0.761905,1,"[128, 1, 1]"
0,0,1.984,7,6939565758449.133,25,7,kernel,12.190476,"[16, 4, 1]",44,88675,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27348,0,0.761905,1,"[32, 16, 1]"
25600,0,6.145,7,6939565758451.82,6,7,kernel,3.047619,"[64, 1, 1]",80,88697,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,27352,0,0.761905,1,"[128, 1, 1]"
16,0,6.624,7,6939565758458.733,0,7,kernel,0.047619,"[1, 1, 1]",48,88709,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27356,0,0.011905,1,"[32, 4, 1]"
0,0,1.472,7,6939565758466.156,5,7,kernel,2.380952,"[50, 1, 1]",22,88772,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,27398,0,0.595238,1,"[128, 1, 1]"
16,0,3.424,7,6939565758468.365,10,7,kernel,4.761905,"[25, 1, 1]",32,88786,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27399,0,0.297619,1,"[32, 16, 1]"
0,0,2.432,7,6939565758962.156,0,7,kernel,0.095238,"[2, 1, 1]",16,88799,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,27407,0,0.02381,1,"[128, 1, 1]"
53504,0,36.928,7,6939565758965.292,0,7,kernel,0.380952,"[1, 8, 1]",236,88813,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,27392,0,0.095238,1,"[128, 1, 1]"
0,0,1.344,7,6939565759003.084,15,7,kernel,7.142857,"[150, 1, 1]",16,88871,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27457,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565759007.34,15,7,kernel,7.142857,"[150, 1, 1]",16,88896,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27467,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565759011.436,15,7,kernel,7.142857,"[150, 1, 1]",22,88910,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27471,0,1.785714,1,"[128, 1, 1]"
0,0,1.248,7,6939565759013.9,15,7,kernel,7.142857,"[150, 1, 1]",16,88930,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27478,0,1.785714,1,"[128, 1, 1]"
0,0,3.04,7,6939565759258.475,15,7,kernel,7.142857,"[150, 1, 1]",22,88944,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27482,0,1.785714,1,"[128, 1, 1]"
0,0,2.56,7,6939565759262.252,30,7,kernel,14.285714,"[300, 1, 1]",16,88979,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,27503,0,3.571429,1,"[128, 1, 1]"
16384,0,13.664,7,6939565759596.331,38,7,kernel,18.285715,"[4, 2, 24]",57,89005,X,ampere_sgemm_128x32_nn,0,27514,0,2.285714,1,"[256, 1, 1]"
0,0,3.2,7,6939565759610.763,25,7,kernel,12.190476,"[16, 4, 1]",44,89007,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27514,0,0.761905,1,"[32, 16, 1]"
16384,0,12.128,7,6939565759614.731,38,7,kernel,18.285715,"[16, 12, 1]",57,89024,X,ampere_sgemm_32x128_nt,0,27518,0,2.285714,1,"[256, 1, 1]"
16,0,7.488,7,6939565759627.627,0,7,kernel,0.142857,"[3, 1, 1]",48,89037,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27522,0,0.035714,1,"[32, 4, 1]"
0,0,1.823,7,6939565759635.883,5,7,kernel,2.380952,"[50, 1, 1]",22,89060,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27541,0,0.595238,1,"[128, 1, 1]"
16,0,2.912,7,6939565759638.507,5,7,kernel,2.380952,"[50, 1, 1]",40,89095,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,27544,0,0.595238,1,"[128, 1, 1]"
0,0,8.193,7,6939565759848.81,0,7,kernel,0.190476,"[2, 1, 1]",38,89097,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,27544,0,0.02381,1,"[256, 1, 1]"
0,0,2.049,7,6939565759857.77,5,7,kernel,2.380952,"[50, 1, 1]",22,89126,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27560,0,0.595238,1,"[128, 1, 1]"
16384,0,17.152,7,6939565759860.586,25,7,kernel,12.190476,"[16, 2, 4]",57,89152,X,ampere_sgemm_128x32_nn,0,27572,0,1.52381,1,"[256, 1, 1]"
0,0,2.848,7,6939565759878.442,67,7,kernel,48.761906,"[64, 4, 1]",44,89154,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27572,0,3.047619,1,"[32, 16, 1]"
16384,0,15.936,7,6939565760092.554,51,7,kernel,24.380953,"[16, 16, 1]",57,89171,X,ampere_sgemm_128x32_nt,0,27576,0,3.047619,1,"[256, 1, 1]"
16,0,8.16,7,6939565760109.258,0,7,kernel,0.047619,"[1, 1, 1]",48,89184,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27580,0,0.011905,1,"[32, 4, 1]"
0,0,3.424,7,6939565760118.122,20,7,kernel,9.523809,"[200, 1, 1]",22,89217,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27601,0,2.380952,1,"[128, 1, 1]"
0,0,3.551,7,6939565760345.482,20,7,kernel,9.523809,"[200, 1, 1]",22,89231,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,27606,0,2.380952,1,"[128, 1, 1]"
16384,0,16.992,7,6939565760349.738,44,7,kernel,21.333334,"[4, 2, 28]",57,89256,X,ampere_sgemm_128x32_nn,0,27616,0,2.666667,1,"[256, 1, 1]"
0,0,3.648,7,6939565760367.466,25,7,kernel,12.190476,"[16, 4, 1]",44,89258,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27616,0,0.761905,1,"[32, 16, 1]"
16384,0,14.144,7,6939565760371.913,51,7,kernel,24.380953,"[16, 16, 1]",57,89275,X,ampere_sgemm_32x128_nt,0,27620,0,3.047619,1,"[256, 1, 1]"
16,0,7.648,7,6939565760386.826,0,7,kernel,0.190476,"[4, 1, 1]",48,89288,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27624,0,0.047619,1,"[32, 4, 1]"
0,0,2.207,7,6939565761054.857,5,7,kernel,2.380952,"[50, 1, 1]",22,89311,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27643,0,0.595238,1,"[128, 1, 1]"
16,0,3.136,7,6939565761057.8,5,7,kernel,2.380952,"[50, 1, 1]",40,89346,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,27646,0,0.595238,1,"[128, 1, 1]"
0,0,4.447,7,6939565761061.737,0,7,kernel,0.190476,"[2, 1, 1]",38,89348,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,27646,0,0.02381,1,"[256, 1, 1]"
0,0,1.633,7,6939565761066.92,5,7,kernel,2.380952,"[50, 1, 1]",22,89377,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27662,0,0.595238,1,"[128, 1, 1]"
25600,0,7.36,7,6939565761069.256,6,7,kernel,3.047619,"[8, 1, 8]",80,89407,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,27674,0,0.761905,1,"[128, 1, 1]"
0,0,2.017,7,6939565761077.448,25,7,kernel,12.190476,"[16, 4, 1]",44,89408,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27674,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565761080.2,6,7,kernel,3.047619,"[64, 1, 1]",80,89430,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,27678,0,0.761905,1,"[128, 1, 1]"
16,0,7.904,7,6939565761087.112,0,7,kernel,0.047619,"[1, 1, 1]",48,89442,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27682,0,0.011905,1,"[32, 4, 1]"
0,0,2.144,7,6939565761906.247,5,7,kernel,2.380952,"[50, 1, 1]",22,89505,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,27724,0,0.595238,1,"[128, 1, 1]"
16,0,3.552,7,6939565761909.223,10,7,kernel,4.761905,"[25, 1, 1]",32,89519,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27725,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565761913.575,0,7,kernel,0.095238,"[2, 1, 1]",16,89532,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,27733,0,0.02381,1,"[128, 1, 1]"
53504,0,36.736,7,6939565761916.391,0,7,kernel,0.380952,"[1, 8, 1]",236,89546,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,27718,0,0.095238,1,"[128, 1, 1]"
0,0,1.152,7,6939565761953.991,15,7,kernel,7.142857,"[150, 1, 1]",16,89604,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27783,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565761958.087,15,7,kernel,7.142857,"[150, 1, 1]",16,89629,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27793,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565761962.215,15,7,kernel,7.142857,"[150, 1, 1]",22,89643,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27797,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565761964.679,15,7,kernel,7.142857,"[150, 1, 1]",16,89663,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,27804,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565761968.807,15,7,kernel,7.142857,"[150, 1, 1]",22,89677,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27808,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565761971.303,30,7,kernel,14.285714,"[300, 1, 1]",16,89712,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,27829,0,3.571429,1,"[128, 1, 1]"
16384,0,13.952,7,6939565762208.774,38,7,kernel,18.285715,"[4, 2, 24]",57,89738,X,ampere_sgemm_128x32_nn,0,27840,0,2.285714,1,"[256, 1, 1]"
0,0,3.073,7,6939565762223.59,25,7,kernel,12.190476,"[16, 4, 1]",44,89740,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27840,0,0.761905,1,"[32, 16, 1]"
16384,0,12.161,7,6939565762227.366,38,7,kernel,18.285715,"[16, 12, 1]",57,89757,X,ampere_sgemm_32x128_nt,0,27844,0,2.285714,1,"[256, 1, 1]"
16,0,7.744,7,6939565762240.295,0,7,kernel,0.142857,"[3, 1, 1]",48,89770,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27848,0,0.035714,1,"[32, 4, 1]"
0,0,2.112,7,6939565762481.286,5,7,kernel,2.380952,"[50, 1, 1]",22,89793,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27867,0,0.595238,1,"[128, 1, 1]"
16,0,3.104,7,6939565762484.166,5,7,kernel,2.380952,"[50, 1, 1]",40,89828,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,27870,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565762488.326,0,7,kernel,0.190476,"[2, 1, 1]",38,89830,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,27870,0,0.02381,1,"[256, 1, 1]"
0,0,2.049,7,6939565762803.973,5,7,kernel,2.380952,"[50, 1, 1]",22,89859,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27886,0,0.595238,1,"[128, 1, 1]"
16384,0,16.769,7,6939565762806.757,25,7,kernel,12.190476,"[16, 2, 4]",57,89885,X,ampere_sgemm_128x32_nn,0,27898,0,1.52381,1,"[256, 1, 1]"
0,0,2.848,7,6939565762824.39,67,7,kernel,48.761906,"[64, 4, 1]",44,89887,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27898,0,3.047619,1,"[32, 16, 1]"
16384,0,14.272,7,6939565762828.07,51,7,kernel,24.380953,"[16, 16, 1]",57,89904,X,ampere_sgemm_128x32_nt,0,27902,0,3.047619,1,"[256, 1, 1]"
16,0,8.032,7,6939565762843.173,0,7,kernel,0.047619,"[1, 1, 1]",48,89917,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27906,0,0.011905,1,"[32, 4, 1]"
0,0,2.368,7,6939565762851.909,20,7,kernel,9.523809,"[200, 1, 1]",22,89950,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27927,0,2.380952,1,"[128, 1, 1]"
0,0,3.456,7,6939565763061.285,20,7,kernel,9.523809,"[200, 1, 1]",22,89964,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,27932,0,2.380952,1,"[128, 1, 1]"
16384,0,16.992,7,6939565763065.541,44,7,kernel,21.333334,"[4, 2, 28]",57,89989,X,ampere_sgemm_128x32_nn,0,27942,0,2.666667,1,"[256, 1, 1]"
0,0,3.584,7,6939565763083.301,25,7,kernel,12.190476,"[16, 4, 1]",44,89991,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,27942,0,0.761905,1,"[32, 16, 1]"
16384,0,14.368,7,6939565763087.717,51,7,kernel,24.380953,"[16, 16, 1]",57,90008,X,ampere_sgemm_32x128_nt,0,27946,0,3.047619,1,"[256, 1, 1]"
16,0,7.648,7,6939565763102.917,0,7,kernel,0.190476,"[4, 1, 1]",48,90021,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,27950,0,0.047619,1,"[32, 4, 1]"
0,0,2.272,7,6939565763348.325,5,7,kernel,2.380952,"[50, 1, 1]",22,90044,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,27969,0,0.595238,1,"[128, 1, 1]"
16,0,3.168,7,6939565763351.397,5,7,kernel,2.380952,"[50, 1, 1]",40,90079,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,27972,0,0.595238,1,"[128, 1, 1]"
0,0,4.512,7,6939565763355.365,0,7,kernel,0.190476,"[2, 1, 1]",38,90081,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,27972,0,0.02381,1,"[256, 1, 1]"
0,0,2.016,7,6939565763560.26,5,7,kernel,2.380952,"[50, 1, 1]",22,90110,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,27988,0,0.595238,1,"[128, 1, 1]"
25600,0,7.135,7,6939565763563.109,6,7,kernel,3.047619,"[8, 1, 8]",80,90140,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,28000,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565763571.076,25,7,kernel,12.190476,"[16, 4, 1]",44,90141,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,28000,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565763573.924,6,7,kernel,3.047619,"[64, 1, 1]",80,90163,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,28004,0,0.761905,1,"[128, 1, 1]"
16,0,8.0,7,6939565763580.996,0,7,kernel,0.047619,"[1, 1, 1]",48,90175,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28008,0,0.011905,1,"[32, 4, 1]"
0,0,2.272,7,6939565765860.065,5,7,kernel,2.380952,"[50, 1, 1]",22,90238,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,28050,0,0.595238,1,"[128, 1, 1]"
16,0,3.521,7,6939565765863.136,10,7,kernel,4.761905,"[25, 1, 1]",32,90252,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28051,0,0.297619,1,"[32, 16, 1]"
0,0,2.049,7,6939565765867.392,0,7,kernel,0.095238,"[2, 1, 1]",16,90265,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,28059,0,0.02381,1,"[128, 1, 1]"
53504,0,37.057,7,6939565765870.176,0,7,kernel,0.380952,"[1, 8, 1]",236,90279,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,28044,0,0.095238,1,"[128, 1, 1]"
0,0,1.12,7,6939565765908.192,15,7,kernel,7.142857,"[150, 1, 1]",16,90337,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28109,0,1.785714,1,"[128, 1, 1]"
0,0,1.217,7,6939565765912.288,15,7,kernel,7.142857,"[150, 1, 1]",16,90362,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28119,0,1.785714,1,"[128, 1, 1]"
0,0,1.729,7,6939565765916.416,15,7,kernel,7.142857,"[150, 1, 1]",22,90376,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28123,0,1.785714,1,"[128, 1, 1]"
0,0,1.217,7,6939565765918.912,15,7,kernel,7.142857,"[150, 1, 1]",16,90396,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28130,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565765923.008,15,7,kernel,7.142857,"[150, 1, 1]",22,90410,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28134,0,1.785714,1,"[128, 1, 1]"
0,0,2.273,7,6939565765925.504,30,7,kernel,14.285714,"[300, 1, 1]",16,90445,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,28155,0,3.571429,1,"[128, 1, 1]"
16384,0,13.151,7,6939565765928.641,38,7,kernel,18.285715,"[4, 2, 24]",57,90471,X,ampere_sgemm_128x32_nn,0,28166,0,2.285714,1,"[256, 1, 1]"
0,0,2.784,7,6939565765942.593,25,7,kernel,12.190476,"[16, 4, 1]",44,90473,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,28166,0,0.761905,1,"[32, 16, 1]"
16384,0,12.127,7,6939565765946.177,38,7,kernel,18.285715,"[16, 12, 1]",57,90490,X,ampere_sgemm_32x128_nt,0,28170,0,2.285714,1,"[256, 1, 1]"
16,0,7.681,7,6939565765959.072,0,7,kernel,0.142857,"[3, 1, 1]",48,90503,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28174,0,0.035714,1,"[32, 4, 1]"
0,0,1.888,7,6939565765967.456,5,7,kernel,2.380952,"[50, 1, 1]",22,90526,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28193,0,0.595238,1,"[128, 1, 1]"
16,0,2.816,7,6939565765970.08,5,7,kernel,2.380952,"[50, 1, 1]",40,90561,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,28196,0,0.595238,1,"[128, 1, 1]"
0,0,4.544,7,6939565765973.664,0,7,kernel,0.190476,"[2, 1, 1]",38,90563,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,28196,0,0.02381,1,"[256, 1, 1]"
0,0,1.792,7,6939565765979.008,5,7,kernel,2.380952,"[50, 1, 1]",22,90592,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,28212,0,0.595238,1,"[128, 1, 1]"
16384,0,15.712,7,6939565765981.632,25,7,kernel,12.190476,"[16, 2, 4]",57,90618,X,ampere_sgemm_128x32_nn,0,28224,0,1.52381,1,"[256, 1, 1]"
0,0,3.2,7,6939565765998.177,67,7,kernel,48.761906,"[64, 4, 1]",44,90620,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,28224,0,3.047619,1,"[32, 16, 1]"
16384,0,14.368,7,6939565766002.144,51,7,kernel,24.380953,"[16, 16, 1]",57,90637,X,ampere_sgemm_128x32_nt,0,28228,0,3.047619,1,"[256, 1, 1]"
16,0,5.792,7,6939565766017.28,0,7,kernel,0.047619,"[1, 1, 1]",48,90650,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28232,0,0.011905,1,"[32, 4, 1]"
0,0,2.592,7,6939565766023.936,20,7,kernel,9.523809,"[200, 1, 1]",22,90683,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,28253,0,2.380952,1,"[128, 1, 1]"
0,0,2.88,7,6939565766027.2,20,7,kernel,9.523809,"[200, 1, 1]",22,90697,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,28258,0,2.380952,1,"[128, 1, 1]"
16384,0,15.232,7,6939565766030.848,44,7,kernel,21.333334,"[4, 2, 28]",57,90722,X,ampere_sgemm_128x32_nn,0,28268,0,2.666667,1,"[256, 1, 1]"
0,0,3.36,7,6939565766046.784,25,7,kernel,12.190476,"[16, 4, 1]",44,90724,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,28268,0,0.761905,1,"[32, 16, 1]"
16384,0,14.72,7,6939565766050.912,51,7,kernel,24.380953,"[16, 16, 1]",57,90741,X,ampere_sgemm_32x128_nt,0,28272,0,3.047619,1,"[256, 1, 1]"
16,0,6.912,7,6939565766066.496,0,7,kernel,0.190476,"[4, 1, 1]",48,90754,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28276,0,0.047619,1,"[32, 4, 1]"
0,0,1.888,7,6939565766074.208,5,7,kernel,2.380952,"[50, 1, 1]",22,90777,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28295,0,0.595238,1,"[128, 1, 1]"
16,0,2.752,7,6939565766076.832,5,7,kernel,2.380952,"[50, 1, 1]",40,90812,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,28298,0,0.595238,1,"[128, 1, 1]"
0,0,4.48,7,6939565766080.448,0,7,kernel,0.190476,"[2, 1, 1]",38,90814,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,28298,0,0.02381,1,"[256, 1, 1]"
0,0,2.079,7,6939565768433.053,5,7,kernel,2.380952,"[50, 1, 1]",22,90843,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,28314,0,0.595238,1,"[128, 1, 1]"
25600,0,7.136,7,6939565768435.996,6,7,kernel,3.047619,"[8, 1, 8]",80,90873,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,28326,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565768443.868,25,7,kernel,12.190476,"[16, 4, 1]",44,90874,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,28326,0,0.761905,1,"[32, 16, 1]"
25600,0,6.047,7,6939565768446.941,6,7,kernel,3.047619,"[64, 1, 1]",80,90896,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,28330,0,0.761905,1,"[128, 1, 1]"
16,0,8.128,7,6939565768453.724,0,7,kernel,0.047619,"[1, 1, 1]",48,90908,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28334,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565768462.62,5,7,kernel,2.380952,"[50, 1, 1]",22,90971,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,28376,0,0.595238,1,"[128, 1, 1]"
16,0,3.424,7,6939565768464.828,10,7,kernel,4.761905,"[25, 1, 1]",32,90985,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28377,0,0.297619,1,"[32, 16, 1]"
0,0,1.985,7,6939565768469.116,0,7,kernel,0.095238,"[2, 1, 1]",16,90998,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,28385,0,0.02381,1,"[128, 1, 1]"
53504,0,36.864,7,6939565768471.9,0,7,kernel,0.380952,"[1, 8, 1]",236,91012,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,28370,0,0.095238,1,"[128, 1, 1]"
0,0,1.12,7,6939565768509.692,15,7,kernel,7.142857,"[150, 1, 1]",16,91070,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28435,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565768513.82,15,7,kernel,7.142857,"[150, 1, 1]",16,91095,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28445,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565768518.108,15,7,kernel,7.142857,"[150, 1, 1]",22,91109,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28449,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565768520.572,15,7,kernel,7.142857,"[150, 1, 1]",16,91129,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28456,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565768524.668,15,7,kernel,7.142857,"[150, 1, 1]",22,91143,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28460,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565768527.196,30,7,kernel,14.285714,"[300, 1, 1]",16,91178,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,28481,0,3.571429,1,"[128, 1, 1]"
16384,0,12.32,7,6939565768530.268,38,7,kernel,18.285715,"[4, 2, 24]",57,91204,X,ampere_sgemm_128x32_nn,0,28492,0,2.285714,1,"[256, 1, 1]"
0,0,3.872,7,6939565768543.356,25,7,kernel,12.190476,"[16, 4, 1]",44,91206,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,28492,0,0.761905,1,"[32, 16, 1]"
16384,0,11.232,7,6939565768547.964,38,7,kernel,18.285715,"[16, 12, 1]",57,91223,X,ampere_sgemm_32x128_nt,0,28496,0,2.285714,1,"[256, 1, 1]"
16,0,7.84,7,6939565768559.996,0,7,kernel,0.142857,"[3, 1, 1]",48,91236,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,28500,0,0.035714,1,"[32, 4, 1]"
0,0,1.664,7,6939565768568.572,5,7,kernel,2.380952,"[50, 1, 1]",22,91259,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28519,0,0.595238,1,"[128, 1, 1]"
0,0,1.152,7,6939565768571.004,5,7,kernel,2.380952,"[50, 1, 1]",16,91291,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28536,0,0.595238,1,"[128, 1, 1]"
0,0,1.568,7,6939565768575.772,51,7,kernel,24.380953,"[512, 1, 1]",16,91316,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28546,0,6.095238,1,"[128, 1, 1]"
0,0,1.952,7,6939565768580.188,51,7,kernel,24.380953,"[512, 1, 1]",16,91341,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28556,0,6.095238,1,"[128, 1, 1]"
0,0,35.392,7,6939565768585.628,100,7,kernel,476.190491,"[10000, 1, 1]",16,91368,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28568,0,119.047623,1,"[128, 1, 1]"
8192,0,4.8,7,6939565768621.788,13,7,kernel,6.095238,"[16, 1, 1]",24,91375,X,"void at::native::(anonymous namespace)::embedding_backward_feature_kernel<float, float, long>(long const*, float const*, float*, int, long, int)",0,28563,0,0.190476,1,"[32, 32, 1]"
0,0,1.472,7,6939565768627.324,5,7,kernel,2.380952,"[50, 1, 1]",16,91401,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28581,0,0.595238,1,"[128, 1, 1]"
0,0,1.824,7,6939565768632.092,51,7,kernel,24.380953,"[512, 1, 1]",16,91426,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28591,0,6.095238,1,"[128, 1, 1]"
0,0,1.568,7,6939565768636.892,51,7,kernel,24.380953,"[512, 1, 1]",16,91451,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28601,0,6.095238,1,"[128, 1, 1]"
0,0,4.224,7,6939565768641.98,51,7,kernel,24.380953,"[512, 1, 1]",22,91465,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,28605,0,6.095238,1,"[128, 1, 1]"
0,0,35.904,7,6939565768646.94,100,7,kernel,476.190491,"[10000, 1, 1]",16,91491,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,28618,0,119.047623,1,"[128, 1, 1]"
8192,0,4.8,7,6939565768683.644,13,7,kernel,6.095238,"[16, 1, 1]",24,91498,X,"void at::native::(anonymous namespace)::embedding_backward_feature_kernel<float, float, long>(long const*, float const*, float*, int, long, int)",0,28613,0,0.190476,1,"[32, 32, 1]"
0,0,431.743,7,6939565773132.661,100,7,kernel,60.952381,"[320, 1, 1]",40,91510,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,22536,0,3.809524,1,"[512, 1, 1]"
0,0,325.696,7,6939565773565.204,100,7,kernel,51.238094,"[269, 1, 1]",40,91513,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,22536,0,3.202381,1,"[512, 1, 1]"
0,0,350.88,7,6939565773891.635,100,7,kernel,54.285713,"[285, 1, 1]",40,91516,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,22536,0,3.392857,1,"[512, 1, 1]"
0,0,203.648,7,6939565774243.315,65,7,kernel,31.238094,"[164, 1, 1]",40,91519,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,22536,0,1.952381,1,"[512, 1, 1]"
