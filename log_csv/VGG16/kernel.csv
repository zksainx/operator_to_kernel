shared memory,pid,dur,stream,ts,est. achieved occupancy %,tid,cat,warps per SM,grid,registers per thread,correlation,ph,name,device,External id,queued,blocks per SM,context,block
0,0,2.209,7,6083647120702.857,39,7,kernel,18.761906,"[197, 1, 1]",16,15,X,"void cask__5x_cudnn::computeOffsetsKernel<false, false>(cask__5x_cudnn::ComputeOffsetsParams)",0,5,0,2.345238,1,"[256, 1, 1]"
16384,0,39.52,7,6083647120705.801,33,7,kernel,18.666666,"[392, 1, 1]",128,18,X,_5x_cudnn_ampere_scudnn_128x64_relu_medium_nn_v1,0,5,0,4.666667,1,"[128, 1, 1]"
0,0,46.015,7,6083647120746.122,100,7,kernel,597.333313,"[12544, 1, 1]",16,26,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,8,0,149.333328,1,"[128, 1, 1]"
0,0,2.272,7,6083647120792.905,0,7,kernel,0.047619,"[1, 1, 1]",24,32,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,9,0,0.011905,1,"[128, 1, 1]"
144,0,64.096,7,6083647120795.913,25,7,kernel,12.190476,"[64, 1, 1]",40,84,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,12,0,0.761905,1,"[512, 1, 1]"
0,0,46.368,7,6083647120907.945,100,7,kernel,298.666656,"[6272, 1, 1]",18,104,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,25,0,74.666664,1,"[128, 1, 1]"
8704,0,3.136,7,6083647120955.113,3,7,kernel,1.52381,"[2, 16, 1]",40,127,X,"void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)",0,30,0,0.380952,1,"[32, 4, 1]"
49152,0,144.192,7,6083647120959.049,33,7,kernel,74.666664,"[2, 28, 14]",126,129,X,_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1,0,30,0,9.333333,1,"[256, 1, 1]"
0,0,46.016,7,6083647121104.137,100,7,kernel,597.333313,"[12544, 1, 1]",16,137,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,33,0,149.333328,1,"[128, 1, 1]"
0,0,2.112,7,6083647121150.857,0,7,kernel,0.047619,"[1, 1, 1]",24,143,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,34,0,0.011905,1,"[128, 1, 1]"
144,0,64.705,7,6083647121153.736,25,7,kernel,12.190476,"[64, 1, 1]",40,195,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,37,0,0.761905,1,"[512, 1, 1]"
0,0,46.271,7,6083647121265.929,100,7,kernel,298.666656,"[6272, 1, 1]",18,215,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,50,0,74.666664,1,"[128, 1, 1]"
0,0,44.417,7,6083647121313.0,100,7,kernel,298.666656,"[3136, 1, 1]",26,234,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,53,0,37.333332,1,"[256, 1, 1]"
4224,0,10.336,7,6083647121358.216,100,7,kernel,74.666664,"[392, 2, 1]",38,252,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,57,0,9.333333,1,"[256, 1, 1]"
4224,0,4.0,7,6083647121369.352,51,7,kernel,24.380953,"[1, 2, 128]",38,254,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,57,0,3.047619,1,"[256, 1, 1]"
98304,0,62.336,7,6083647121374.12,0,7,kernel,4.666667,"[1, 49, 1]",254,259,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize256x128x32_stage2_warpsize4x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,57,0,0.583333,1,"[256, 1, 1]"
0,0,17.568,7,6083647121437.608,100,7,kernel,298.666656,"[6272, 1, 1]",16,268,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,60,0,74.666664,1,"[128, 1, 1]"
0,0,2.24,7,6083647121455.912,0,7,kernel,0.047619,"[1, 1, 1]",24,274,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,61,0,0.011905,1,"[128, 1, 1]"
144,0,22.048,7,6083647121458.952,51,7,kernel,24.380953,"[128, 1, 1]",40,326,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,64,0,1.52381,1,"[512, 1, 1]"
0,0,23.552,7,6083647121505.8,100,7,kernel,149.333328,"[3136, 1, 1]",18,346,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,77,0,37.333332,1,"[128, 1, 1]"
4224,0,21.376,7,6083647121530.152,100,7,kernel,149.333328,"[392, 4, 1]",38,368,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,82,0,18.666666,1,"[256, 1, 1]"
4224,0,4.96,7,6083647121552.712,100,7,kernel,48.761906,"[1, 4, 128]",38,370,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,82,0,6.095238,1,"[256, 1, 1]"
49152,0,102.08,7,6083647121560.872,17,7,kernel,9.333333,"[1, 98, 2]",230,374,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,82,0,2.333333,1,"[128, 1, 1]"
0,0,20.384,7,6083647121663.752,100,7,kernel,298.666656,"[6272, 1, 1]",16,383,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,85,0,74.666664,1,"[128, 1, 1]"
0,0,2.304,7,6083647121684.936,0,7,kernel,0.047619,"[1, 1, 1]",24,389,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,86,0,0.011905,1,"[128, 1, 1]"
144,0,22.24,7,6083647121688.136,51,7,kernel,24.380953,"[128, 1, 1]",40,441,X,"void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)",0,89,0,1.52381,1,"[512, 1, 1]"
0,0,24.0,7,6083647121735.208,100,7,kernel,149.333328,"[3136, 1, 1]",18,461,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,102,0,37.333332,1,"[128, 1, 1]"
0,0,20.48,7,6083647121759.943,100,7,kernel,149.333328,"[1568, 1, 1]",26,480,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,105,0,18.666666,1,"[256, 1, 1]"
4224,0,4.736,7,6083647121781.16,78,7,kernel,37.333332,"[98, 4, 1]",38,498,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,109,0,4.666667,1,"[256, 1, 1]"
4224,0,6.945,7,6083647121786.663,100,7,kernel,97.523811,"[1, 4, 256]",38,500,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,109,0,12.190476,1,"[256, 1, 1]"
49152,0,53.28,7,6083647121794.343,5,7,kernel,2.380952,"[2, 25, 1]",230,503,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,109,0,0.595238,1,"[128, 1, 1]"
0,0,6.943,7,6083647121848.52,100,7,kernel,149.333328,"[3136, 1, 1]",16,512,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,112,0,37.333332,1,"[128, 1, 1]"
0,0,1.983,7,6083647121856.264,0,7,kernel,0.047619,"[1, 1, 1]",24,518,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,113,0,0.011905,1,"[128, 1, 1]"
14480,0,11.232,7,6083647121859.143,100,7,kernel,48.761906,"[256, 1, 1]",40,571,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,116,0,3.047619,1,"[512, 1, 1]"
0,0,6.657,7,6083647121880.775,100,7,kernel,74.666664,"[1568, 1, 1]",18,592,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,129,0,18.666666,1,"[128, 1, 1]"
4224,0,7.488,7,6083647121888.263,100,7,kernel,74.666664,"[98, 8, 1]",38,614,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,134,0,9.333333,1,"[256, 1, 1]"
4224,0,11.072,7,6083647121896.519,100,7,kernel,195.047623,"[1, 8, 256]",38,616,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,134,0,24.380953,1,"[256, 1, 1]"
49152,0,83.967,7,6083647121910.536,17,7,kernel,19.047619,"[2, 25, 8]",230,620,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,134,0,4.761905,1,"[128, 1, 1]"
0,0,7.296,7,6083647121995.367,100,7,kernel,149.333328,"[3136, 1, 1]",16,629,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,137,0,37.333332,1,"[128, 1, 1]"
0,0,1.952,7,6083647122003.399,0,7,kernel,0.047619,"[1, 1, 1]",24,635,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,138,0,0.011905,1,"[128, 1, 1]"
14480,0,10.56,7,6083647122006.119,100,7,kernel,48.761906,"[256, 1, 1]",40,688,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,141,0,3.047619,1,"[512, 1, 1]"
0,0,7.168,7,6083647122026.695,100,7,kernel,74.666664,"[1568, 1, 1]",18,709,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,154,0,18.666666,1,"[128, 1, 1]"
4224,0,6.464,7,6083647122034.631,100,7,kernel,74.666664,"[98, 8, 1]",38,731,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,159,0,9.333333,1,"[256, 1, 1]"
4224,0,10.784,7,6083647122041.895,100,7,kernel,195.047623,"[1, 8, 256]",38,733,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,159,0,24.380953,1,"[256, 1, 1]"
49152,0,88.416,7,6083647122056.007,17,7,kernel,19.047619,"[2, 25, 8]",230,737,X,sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn,0,159,0,4.761905,1,"[128, 1, 1]"
0,0,7.616,7,6083647122145.255,100,7,kernel,149.333328,"[3136, 1, 1]",16,746,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,162,0,37.333332,1,"[128, 1, 1]"
0,0,1.92,7,6083647122153.735,0,7,kernel,0.047619,"[1, 1, 1]",24,752,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,163,0,0.011905,1,"[128, 1, 1]"
14480,0,10.112,7,6083647122156.423,100,7,kernel,48.761906,"[256, 1, 1]",40,805,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,166,0,3.047619,1,"[512, 1, 1]"
0,0,6.944,7,6083647122176.583,100,7,kernel,74.666664,"[1568, 1, 1]",18,826,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,179,0,18.666666,1,"[128, 1, 1]"
0,0,6.688,7,6083647122184.423,100,7,kernel,74.666664,"[784, 1, 1]",26,845,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,182,0,9.333333,1,"[256, 1, 1]"
4224,0,2.464,7,6083647122191.847,40,7,kernel,19.047619,"[25, 8, 1]",38,863,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,186,0,2.380952,1,"[256, 1, 1]"
4224,0,18.848,7,6083647122195.111,100,7,kernel,390.095245,"[1, 8, 512]",38,865,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,186,0,48.761906,1,"[256, 1, 1]"
73728,0,51.168,7,6083647122214.695,0,7,kernel,2.666667,"[28, 2, 1]",162,869,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_16x6_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_16x6_nhwc_align4::Params),0,186,0,0.666667,1,"[128, 1, 1]"
4224,0,3.392,7,6083647122266.631,79,7,kernel,38.095238,"[25, 16, 1]",40,873,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,186,0,4.761905,1,"[256, 1, 1]"
0,0,4.512,7,6083647122270.855,100,7,kernel,74.666664,"[1568, 1, 1]",16,881,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,189,0,18.666666,1,"[128, 1, 1]"
0,0,1.568,7,6083647122276.135,0,7,kernel,0.047619,"[1, 1, 1]",24,887,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,190,0,0.011905,1,"[128, 1, 1]"
4240,0,9.856,7,6083647122278.503,100,7,kernel,97.523811,"[512, 1, 1]",40,940,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,193,0,6.095238,1,"[512, 1, 1]"
0,0,2.431,7,6083647122292.423,78,7,kernel,37.333332,"[784, 1, 1]",18,961,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,206,0,9.333333,1,"[128, 1, 1]"
4224,0,2.976,7,6083647122295.591,79,7,kernel,38.095238,"[25, 16, 1]",38,983,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,211,0,4.761905,1,"[256, 1, 1]"
4224,0,33.857,7,6083647122299.366,100,7,kernel,780.190491,"[1, 16, 512]",38,985,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,211,0,97.523811,1,"[256, 1, 1]"
73728,0,96.063,7,6083647122333.959,0,7,kernel,2.666667,"[28, 2, 1]",168,989,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4::Params),0,211,0,0.666667,1,"[128, 1, 1]"
4224,0,3.424,7,6083647122430.854,79,7,kernel,38.095238,"[25, 16, 1]",40,993,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,211,0,4.761905,1,"[256, 1, 1]"
0,0,4.096,7,6083647122435.11,100,7,kernel,74.666664,"[1568, 1, 1]",16,1001,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,214,0,18.666666,1,"[128, 1, 1]"
0,0,1.601,7,6083647122440.07,0,7,kernel,0.047619,"[1, 1, 1]",24,1007,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,215,0,0.011905,1,"[128, 1, 1]"
4240,0,9.728,7,6083647122442.47,100,7,kernel,97.523811,"[512, 1, 1]",40,1060,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,218,0,6.095238,1,"[512, 1, 1]"
0,0,2.433,7,6083647122456.326,78,7,kernel,37.333332,"[784, 1, 1]",18,1081,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,231,0,9.333333,1,"[128, 1, 1]"
4224,0,2.976,7,6083647122459.494,79,7,kernel,38.095238,"[25, 16, 1]",38,1103,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,236,0,4.761905,1,"[256, 1, 1]"
4224,0,34.016,7,6083647122463.27,100,7,kernel,780.190491,"[1, 16, 512]",38,1105,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,236,0,97.523811,1,"[256, 1, 1]"
73728,0,95.648,7,6083647122498.054,0,7,kernel,2.666667,"[28, 2, 1]",168,1109,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4::Params),0,236,0,0.666667,1,"[128, 1, 1]"
4224,0,3.296,7,6083647122594.47,79,7,kernel,38.095238,"[25, 16, 1]",40,1113,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,236,0,4.761905,1,"[256, 1, 1]"
0,0,4.192,7,6083647122598.566,100,7,kernel,74.666664,"[1568, 1, 1]",16,1121,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,239,0,18.666666,1,"[128, 1, 1]"
0,0,1.536,7,6083647122603.526,0,7,kernel,0.047619,"[1, 1, 1]",24,1127,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,240,0,0.011905,1,"[128, 1, 1]"
4240,0,9.952,7,6083647122605.894,100,7,kernel,97.523811,"[512, 1, 1]",40,1180,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,243,0,6.095238,1,"[512, 1, 1]"
0,0,2.432,7,6083647122619.974,78,7,kernel,37.333332,"[784, 1, 1]",18,1201,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,256,0,9.333333,1,"[128, 1, 1]"
0,0,4.096,7,6083647122623.142,78,7,kernel,37.333332,"[392, 1, 1]",26,1220,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,259,0,4.666667,1,"[256, 1, 1]"
4224,0,2.176,7,6083647122627.942,22,7,kernel,10.666667,"[7, 16, 1]",38,1238,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,263,0,1.333333,1,"[256, 1, 1]"
4224,0,33.856,7,6083647122630.854,100,7,kernel,780.190491,"[1, 16, 512]",38,1240,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,263,0,97.523811,1,"[256, 1, 1]"
100352,0,38.4,7,6083647122667.27,0,7,kernel,3.047619,"[8, 2, 4]",198,1246,X,sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,263,0,0.761905,1,"[128, 1, 1]"
4224,0,2.752,7,6083647122706.406,22,7,kernel,10.666667,"[7, 16, 1]",40,1249,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,263,0,1.333333,1,"[256, 1, 1]"
0,0,2.944,7,6083647122710.054,39,7,kernel,18.666666,"[392, 1, 1]",16,1257,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,266,0,4.666667,1,"[128, 1, 1]"
0,0,1.568,7,6083647122713.894,0,7,kernel,0.047619,"[1, 1, 1]",24,1263,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,267,0,0.011905,1,"[128, 1, 1]"
2192,0,9.793,7,6083647127310.301,100,7,kernel,97.523811,"[512, 1, 1]",40,1316,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,270,0,6.095238,1,"[512, 1, 1]"
0,0,1.728,7,6083647127323.901,19,7,kernel,9.333333,"[196, 1, 1]",18,1337,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,283,0,2.333333,1,"[128, 1, 1]"
4224,0,2.144,7,6083647127326.397,22,7,kernel,10.666667,"[7, 16, 1]",38,1359,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,288,0,1.333333,1,"[256, 1, 1]"
4224,0,33.952,7,6083647127329.405,100,7,kernel,780.190491,"[1, 16, 512]",38,1361,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,288,0,97.523811,1,"[256, 1, 1]"
100352,0,38.56,7,6083647127366.621,0,7,kernel,3.047619,"[8, 2, 4]",198,1367,X,sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,288,0,0.761905,1,"[128, 1, 1]"
4224,0,2.88,7,6083647127405.949,22,7,kernel,10.666667,"[7, 16, 1]",40,1370,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,288,0,1.333333,1,"[256, 1, 1]"
0,0,2.848,7,6083647127409.629,39,7,kernel,18.666666,"[392, 1, 1]",16,1378,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,291,0,4.666667,1,"[128, 1, 1]"
0,0,1.568,7,6083647127413.277,0,7,kernel,0.047619,"[1, 1, 1]",24,1384,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,292,0,0.011905,1,"[128, 1, 1]"
2192,0,8.704,7,6083647127415.645,100,7,kernel,97.523811,"[512, 1, 1]",40,1437,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,295,0,6.095238,1,"[512, 1, 1]"
0,0,1.696,7,6083647127427.581,19,7,kernel,9.333333,"[196, 1, 1]",18,1458,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,308,0,2.333333,1,"[128, 1, 1]"
4224,0,2.112,7,6083647127430.109,22,7,kernel,10.666667,"[7, 16, 1]",38,1480,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,313,0,1.333333,1,"[256, 1, 1]"
4224,0,33.184,7,6083647127433.469,100,7,kernel,780.190491,"[1, 16, 512]",38,1482,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,313,0,97.523811,1,"[256, 1, 1]"
100352,0,38.496,7,6083647127469.981,0,7,kernel,3.047619,"[8, 2, 4]",198,1488,X,sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,313,0,0.761905,1,"[128, 1, 1]"
4224,0,2.464,7,6083647127509.629,22,7,kernel,10.666667,"[7, 16, 1]",40,1491,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,313,0,1.333333,1,"[256, 1, 1]"
0,0,2.88,7,6083647127512.861,39,7,kernel,18.666666,"[392, 1, 1]",16,1499,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})",0,316,0,4.666667,1,"[128, 1, 1]"
0,0,1.568,7,6083647127516.541,0,7,kernel,0.047619,"[1, 1, 1]",24,1505,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)",0,317,0,0.011905,1,"[128, 1, 1]"
2192,0,8.672,7,6083647127518.909,100,7,kernel,97.523811,"[512, 1, 1]",40,1558,X,"void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)",0,320,0,6.095238,1,"[512, 1, 1]"
0,0,1.664,7,6083647127531.005,19,7,kernel,9.333333,"[196, 1, 1]",18,1579,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)",0,333,0,2.333333,1,"[128, 1, 1]"
0,0,2.72,7,6083647127533.533,19,7,kernel,9.333333,"[98, 1, 1]",26,1598,X,"void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)",0,336,0,1.166667,1,"[256, 1, 1]"
528,0,692.991,7,6083647127537.117,25,7,kernel,48.761906,"[1024, 1, 1]",159,1617,X,"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, true, false, 7, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",0,342,0,12.190476,1,"[32, 4, 1]"
0,0,2.368,7,6083647128230.94,3,7,kernel,1.52381,"[16, 1, 1]",47,1648,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,344,0,0.190476,1,"[256, 1, 1]"
2560,0,115.807,7,6083647128234.14,51,7,kernel,24.380953,"[512, 1, 1]",58,1667,X,"void gemv2T_kernel_val<int, int, float, float, float, float, 128, 16, 4, 4, false, true, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>, float, float)",0,353,0,6.095238,1,"[128, 1, 1]"
0,0,3.265,7,6083647128350.779,3,7,kernel,1.52381,"[16, 1, 1]",47,1698,X,"void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)",0,355,0,0.190476,1,"[256, 1, 1]"
528,0,30.336,7,6083647128354.875,25,7,kernel,11.904762,"[250, 1, 1]",159,1717,X,"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, true, false, 7, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",0,364,0,2.976191,1,"[32, 4, 1]"
