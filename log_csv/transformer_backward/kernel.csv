shared memory,pid,dur,stream,ts,est. achieved occupancy %,tid,cat,warps per SM,grid,registers per thread,correlation,ph,name,device,External id,queued,blocks per SM,context,block
2064,0,6.368,7,6939565488265.648,25,7,kernel,11.809524,"[1, 62, 1]",32,60823,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,16387,0,0.738095,1,"[512, 1, 1]"
0,0,1.472,7,6939565488560.303,0,7,kernel,0.047619,"[1, 1, 1]",16,60836,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,16392,0,0.011905,1,"[128, 1, 1]"
0,0,3.2,7,6939565489698.156,100,7,kernel,93.047623,"[1954, 1, 1]",16,60866,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,16914,0,23.261906,1,"[128, 1, 1]"
30720,0,50.528,7,6939565489858.604,14,7,kernel,6.857143,"[4, 1, 36]",126,60884,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x64_8x5_nn_align1>(cutlass_80_simt_sgemm_128x64_8x5_nn_align1::Params),0,16910,0,1.714286,1,"[128, 1, 1]"
0,0,4.288,7,6939565489909.836,25,7,kernel,12.190476,"[16, 4, 1]",44,60885,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16910,0,0.761905,1,"[32, 16, 1]"
0,0,3.616,7,6939565489971.884,100,7,kernel,93.047623,"[1954, 1, 1]",16,60903,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,16922,0,23.261906,1,"[128, 1, 1]"
16384,0,46.4,7,6939565490030.251,67,7,kernel,119.238098,"[4, 313, 1]",57,60917,X,ampere_sgemm_128x32_nn,0,16918,0,14.904762,1,"[256, 1, 1]"
16,0,3.775,7,6939565490105.164,8,7,kernel,3.809524,"[20, 1, 1]",32,60930,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,16926,0,0.238095,1,"[32, 16, 1]"
16,0,2.88,7,6939565490463.755,5,7,kernel,2.380952,"[50, 1, 1]",40,60983,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,16951,0,0.595238,1,"[128, 1, 1]"
0,0,4.48,7,6939565490487.018,0,7,kernel,0.190476,"[2, 1, 1]",38,60985,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,16951,0,0.02381,1,"[256, 1, 1]"
16,0,2.656,7,6939565490637.418,5,7,kernel,2.380952,"[50, 1, 1]",40,61026,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,16965,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565490649.29,0,7,kernel,0.190476,"[2, 1, 1]",38,61028,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,16965,0,0.02381,1,"[256, 1, 1]"
0,0,1.792,7,6939565490822.058,5,7,kernel,2.380952,"[50, 1, 1]",22,61057,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,16981,0,0.595238,1,"[128, 1, 1]"
16384,0,15.199,7,6939565490956.746,25,7,kernel,12.190476,"[16, 2, 4]",57,61083,X,ampere_sgemm_128x32_nn,0,16993,0,1.52381,1,"[256, 1, 1]"
0,0,2.847,7,6939565490972.746,67,7,kernel,48.761906,"[64, 4, 1]",44,61085,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,16993,0,3.047619,1,"[32, 16, 1]"
16384,0,15.104,7,6939565491031.945,51,7,kernel,24.380953,"[16, 16, 1]",57,61102,X,ampere_sgemm_128x32_nt,0,16997,0,3.047619,1,"[256, 1, 1]"
16,0,7.872,7,6939565491095.689,0,7,kernel,0.047619,"[1, 1, 1]",48,61115,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17001,0,0.011905,1,"[32, 4, 1]"
0,0,2.401,7,6939565491289.16,20,7,kernel,9.523809,"[200, 1, 1]",22,61148,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,17022,0,2.380952,1,"[128, 1, 1]"
0,0,3.168,7,6939565491401.864,20,7,kernel,9.523809,"[200, 1, 1]",22,61162,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,17027,0,2.380952,1,"[128, 1, 1]"
16384,0,14.976,7,6939565491520.232,44,7,kernel,21.333334,"[4, 2, 28]",57,61187,X,ampere_sgemm_128x32_nn,0,17037,0,2.666667,1,"[256, 1, 1]"
0,0,3.2,7,6939565491535.912,25,7,kernel,12.190476,"[16, 4, 1]",44,61189,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17037,0,0.761905,1,"[32, 16, 1]"
16384,0,14.368,7,6939565491606.568,51,7,kernel,24.380953,"[16, 16, 1]",57,61206,X,ampere_sgemm_32x128_nt,0,17041,0,3.047619,1,"[256, 1, 1]"
16,0,6.912,7,6939565491670.728,0,7,kernel,0.190476,"[4, 1, 1]",48,61219,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17045,0,0.047619,1,"[32, 4, 1]"
0,0,1.888,7,6939565491883.463,5,7,kernel,2.380952,"[50, 1, 1]",22,61242,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17064,0,0.595238,1,"[128, 1, 1]"
16,0,2.784,7,6939565491992.007,5,7,kernel,2.380952,"[50, 1, 1]",40,61277,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,17067,0,0.595238,1,"[128, 1, 1]"
0,0,4.384,7,6939565492016.839,0,7,kernel,0.190476,"[2, 1, 1]",38,61279,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,17067,0,0.02381,1,"[256, 1, 1]"
0,0,1.568,7,6939565492150.023,5,7,kernel,2.380952,"[50, 1, 1]",22,61308,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,17083,0,0.595238,1,"[128, 1, 1]"
25600,0,7.008,7,6939565492384.262,6,7,kernel,3.047619,"[8, 1, 8]",80,61338,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,17095,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565492392.134,25,7,kernel,12.190476,"[16, 4, 1]",44,61339,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17095,0,0.761905,1,"[32, 16, 1]"
25600,0,6.144,7,6939565492394.95,6,7,kernel,3.047619,"[64, 1, 1]",80,61361,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,17099,0,0.761905,1,"[128, 1, 1]"
16,0,8.0,7,6939565492443.846,0,7,kernel,0.047619,"[1, 1, 1]",48,61373,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17103,0,0.011905,1,"[32, 4, 1]"
0,0,2.304,7,6939565495120.288,5,7,kernel,2.380952,"[50, 1, 1]",22,61436,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,17145,0,0.595238,1,"[128, 1, 1]"
16,0,3.552,7,6939565495123.36,10,7,kernel,4.761905,"[25, 1, 1]",32,61450,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17146,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565495127.648,0,7,kernel,0.095238,"[2, 1, 1]",16,61463,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,17154,0,0.02381,1,"[128, 1, 1]"
53504,0,36.8,7,6939565495130.464,0,7,kernel,0.380952,"[1, 8, 1]",236,61477,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,17139,0,0.095238,1,"[128, 1, 1]"
0,0,1.087,7,6939565495168.224,10,7,kernel,4.761905,"[100, 1, 1]",16,61535,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,17204,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565495172.16,10,7,kernel,4.761905,"[100, 1, 1]",16,61560,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,17214,0,1.190476,1,"[128, 1, 1]"
0,0,1.568,7,6939565495176.352,10,7,kernel,4.761905,"[100, 1, 1]",22,61574,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17218,0,1.190476,1,"[128, 1, 1]"
0,0,2.144,7,6939565495178.656,20,7,kernel,9.523809,"[200, 1, 1]",16,61609,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,17239,0,2.380952,1,"[128, 1, 1]"
16384,0,10.176,7,6939565495181.632,25,7,kernel,12.190476,"[4, 2, 16]",57,61635,X,ampere_sgemm_128x32_nn,0,17250,0,1.52381,1,"[256, 1, 1]"
0,0,2.368,7,6939565495192.512,25,7,kernel,12.190476,"[16, 4, 1]",44,61637,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17250,0,0.761905,1,"[32, 16, 1]"
16384,0,8.193,7,6939565495195.615,25,7,kernel,12.190476,"[16, 8, 1]",57,61654,X,ampere_sgemm_32x128_nt,0,17254,0,1.52381,1,"[256, 1, 1]"
16,0,7.839,7,6939565495204.672,0,7,kernel,0.095238,"[2, 1, 1]",48,61667,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17258,0,0.02381,1,"[32, 4, 1]"
25600,0,6.912,7,6939565495213.248,6,7,kernel,3.047619,"[8, 1, 8]",80,61705,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,17278,0,0.761905,1,"[128, 1, 1]"
0,0,2.017,7,6939565495220.895,25,7,kernel,12.190476,"[16, 4, 1]",44,61706,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17278,0,0.761905,1,"[32, 16, 1]"
25600,0,6.111,7,6939565495223.712,6,7,kernel,3.047619,"[64, 1, 1]",80,61728,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,17282,0,0.761905,1,"[128, 1, 1]"
16,0,5.856,7,6939565495230.655,0,7,kernel,0.047619,"[1, 1, 1]",48,61740,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17286,0,0.011905,1,"[32, 4, 1]"
0,0,1.92,7,6939565495237.279,5,7,kernel,2.380952,"[50, 1, 1]",22,61755,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17297,0,0.595238,1,"[128, 1, 1]"
0,0,2.017,7,6939565495239.903,0,7,kernel,0.190476,"[2, 2, 1]",30,61772,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,17300,0,0.047619,1,"[128, 1, 1]"
0,0,8.544,7,6939565495242.687,100,7,kernel,97.523811,"[1024, 2, 1]",30,61791,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,17307,0,24.380953,1,"[128, 1, 1]"
16,0,3.712,7,6939565495251.936,5,7,kernel,2.380952,"[50, 1, 1]",40,61828,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,17314,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565495256.351,0,7,kernel,0.190476,"[2, 1, 1]",38,61830,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,17314,0,0.02381,1,"[256, 1, 1]"
0,0,1.791,7,6939565495261.6,5,7,kernel,2.380952,"[50, 1, 1]",22,61859,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,17330,0,0.595238,1,"[128, 1, 1]"
25600,0,7.264,7,6939565497622.906,6,7,kernel,3.047619,"[8, 1, 8]",80,61889,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,17342,0,0.761905,1,"[128, 1, 1]"
0,0,2.336,7,6939565497630.874,25,7,kernel,12.190476,"[16, 4, 1]",44,61890,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17342,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565497633.978,6,7,kernel,3.047619,"[64, 1, 1]",80,61912,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,17346,0,0.761905,1,"[128, 1, 1]"
16,0,7.904,7,6939565497640.794,0,7,kernel,0.047619,"[1, 1, 1]",48,61924,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17350,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565497649.498,5,7,kernel,2.380952,"[50, 1, 1]",22,61987,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,17392,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565497651.93,10,7,kernel,4.761905,"[25, 1, 1]",32,62001,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17393,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565497656.058,0,7,kernel,0.095238,"[2, 1, 1]",16,62014,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,17401,0,0.02381,1,"[128, 1, 1]"
53504,0,37.12,7,6939565497658.778,0,7,kernel,0.380952,"[1, 8, 1]",236,62028,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,17386,0,0.095238,1,"[128, 1, 1]"
0,0,1.12,7,6939565497696.794,15,7,kernel,7.142857,"[150, 1, 1]",16,62086,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,17451,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565497700.89,15,7,kernel,7.142857,"[150, 1, 1]",16,62111,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,17461,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565497705.018,15,7,kernel,7.142857,"[150, 1, 1]",22,62125,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17465,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565497707.482,15,7,kernel,7.142857,"[150, 1, 1]",16,62145,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,17472,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565497711.578,15,7,kernel,7.142857,"[150, 1, 1]",22,62159,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17476,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565497714.106,30,7,kernel,14.285714,"[300, 1, 1]",16,62194,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,17497,0,3.571429,1,"[128, 1, 1]"
16384,0,12.768,7,6939565497717.21,38,7,kernel,18.285715,"[4, 2, 24]",57,62220,X,ampere_sgemm_128x32_nn,0,17508,0,2.285714,1,"[256, 1, 1]"
0,0,3.968,7,6939565497730.81,25,7,kernel,12.190476,"[16, 4, 1]",44,62222,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17508,0,0.761905,1,"[32, 16, 1]"
16384,0,11.199,7,6939565497735.482,38,7,kernel,18.285715,"[16, 12, 1]",57,62239,X,ampere_sgemm_32x128_nt,0,17512,0,2.285714,1,"[256, 1, 1]"
16,0,7.808,7,6939565497747.482,0,7,kernel,0.142857,"[3, 1, 1]",48,62252,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17516,0,0.035714,1,"[32, 4, 1]"
0,0,1.792,7,6939565497756.122,5,7,kernel,2.380952,"[50, 1, 1]",22,62275,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17535,0,0.595238,1,"[128, 1, 1]"
16,0,2.848,7,6939565497758.714,5,7,kernel,2.380952,"[50, 1, 1]",40,62310,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,17538,0,0.595238,1,"[128, 1, 1]"
0,0,4.544,7,6939565497762.266,0,7,kernel,0.190476,"[2, 1, 1]",38,62312,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,17538,0,0.02381,1,"[256, 1, 1]"
0,0,1.696,7,6939565497767.641,5,7,kernel,2.380952,"[50, 1, 1]",22,62341,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,17554,0,0.595238,1,"[128, 1, 1]"
16384,0,15.424,7,6939565497770.106,25,7,kernel,12.190476,"[16, 2, 4]",57,62367,X,ampere_sgemm_128x32_nn,0,17566,0,1.52381,1,"[256, 1, 1]"
0,0,2.784,7,6939565497786.362,67,7,kernel,48.761906,"[64, 4, 1]",44,62369,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17566,0,3.047619,1,"[32, 16, 1]"
16384,0,14.56,7,6939565497789.946,51,7,kernel,24.380953,"[16, 16, 1]",57,62386,X,ampere_sgemm_128x32_nt,0,17570,0,3.047619,1,"[256, 1, 1]"
16,0,5.697,7,6939565497805.273,0,7,kernel,0.047619,"[1, 1, 1]",48,62399,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17574,0,0.011905,1,"[32, 4, 1]"
0,0,3.04,7,6939565500108.244,20,7,kernel,9.523809,"[200, 1, 1]",22,62432,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,17595,0,2.380952,1,"[128, 1, 1]"
0,0,3.488,7,6939565500111.988,20,7,kernel,9.523809,"[200, 1, 1]",22,62446,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,17600,0,2.380952,1,"[128, 1, 1]"
16384,0,17.056,7,6939565500116.276,44,7,kernel,21.333334,"[4, 2, 28]",57,62471,X,ampere_sgemm_128x32_nn,0,17610,0,2.666667,1,"[256, 1, 1]"
0,0,3.744,7,6939565500134.164,25,7,kernel,12.190476,"[16, 4, 1]",44,62473,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17610,0,0.761905,1,"[32, 16, 1]"
16384,0,15.072,7,6939565500139.028,51,7,kernel,24.380953,"[16, 16, 1]",57,62490,X,ampere_sgemm_32x128_nt,0,17614,0,3.047619,1,"[256, 1, 1]"
16,0,7.616,7,6939565500154.836,0,7,kernel,0.190476,"[4, 1, 1]",48,62503,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17618,0,0.047619,1,"[32, 4, 1]"
0,0,1.696,7,6939565500163.252,5,7,kernel,2.380952,"[50, 1, 1]",22,62526,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17637,0,0.595238,1,"[128, 1, 1]"
16,0,2.752,7,6939565500165.716,5,7,kernel,2.380952,"[50, 1, 1]",40,62561,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,17640,0,0.595238,1,"[128, 1, 1]"
0,0,4.512,7,6939565500169.3,0,7,kernel,0.190476,"[2, 1, 1]",38,62563,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,17640,0,0.02381,1,"[256, 1, 1]"
0,0,1.664,7,6939565500174.548,5,7,kernel,2.380952,"[50, 1, 1]",22,62592,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,17656,0,0.595238,1,"[128, 1, 1]"
25600,0,6.976,7,6939565500177.044,6,7,kernel,3.047619,"[8, 1, 8]",80,62622,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,17668,0,0.761905,1,"[128, 1, 1]"
0,0,1.952,7,6939565500184.788,25,7,kernel,12.190476,"[16, 4, 1]",44,62623,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17668,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565500187.444,6,7,kernel,3.047619,"[64, 1, 1]",80,62645,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,17672,0,0.761905,1,"[128, 1, 1]"
16,0,6.656,7,6939565500194.196,0,7,kernel,0.047619,"[1, 1, 1]",48,62657,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17676,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565500201.684,5,7,kernel,2.380952,"[50, 1, 1]",22,62720,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,17718,0,0.595238,1,"[128, 1, 1]"
16,0,3.424,7,6939565500203.892,10,7,kernel,4.761905,"[25, 1, 1]",32,62734,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17719,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565500208.148,0,7,kernel,0.095238,"[2, 1, 1]",16,62747,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,17727,0,0.02381,1,"[128, 1, 1]"
53504,0,36.992,7,6939565500210.964,0,7,kernel,0.380952,"[1, 8, 1]",236,62761,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,17712,0,0.095238,1,"[128, 1, 1]"
0,0,0.928,7,6939565500249.012,10,7,kernel,4.761905,"[100, 1, 1]",16,62819,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,17777,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565500252.788,10,7,kernel,4.761905,"[100, 1, 1]",16,62844,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,17787,0,1.190476,1,"[128, 1, 1]"
0,0,1.568,7,6939565500256.884,10,7,kernel,4.761905,"[100, 1, 1]",22,62858,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17791,0,1.190476,1,"[128, 1, 1]"
0,0,2.176,7,6939565500259.22,20,7,kernel,9.523809,"[200, 1, 1]",16,62893,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,17812,0,2.380952,1,"[128, 1, 1]"
16384,0,9.088,7,6939565500262.164,25,7,kernel,12.190476,"[4, 2, 16]",57,62919,X,ampere_sgemm_128x32_nn,0,17823,0,1.52381,1,"[256, 1, 1]"
0,0,2.656,7,6939565500272.084,25,7,kernel,12.190476,"[16, 4, 1]",44,62921,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17823,0,0.761905,1,"[32, 16, 1]"
16384,0,8.192,7,6939565500275.476,25,7,kernel,12.190476,"[16, 8, 1]",57,62938,X,ampere_sgemm_32x128_nt,0,17827,0,1.52381,1,"[256, 1, 1]"
16,0,9.76,7,6939565502672.782,0,7,kernel,0.095238,"[2, 1, 1]",48,62951,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17831,0,0.02381,1,"[32, 4, 1]"
0,0,2.112,7,6939565502683.31,5,7,kernel,2.380952,"[50, 1, 1]",22,62970,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17842,0,0.595238,1,"[128, 1, 1]"
25600,0,6.976,7,6939565502686.126,6,7,kernel,3.047619,"[8, 1, 8]",80,63002,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,17852,0,0.761905,1,"[128, 1, 1]"
0,0,2.08,7,6939565502693.806,25,7,kernel,12.190476,"[16, 4, 1]",44,63003,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17852,0,0.761905,1,"[32, 16, 1]"
25600,0,6.144,7,6939565502696.654,6,7,kernel,3.047619,"[64, 1, 1]",80,63025,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,17856,0,0.761905,1,"[128, 1, 1]"
16,0,6.144,7,6939565502703.534,0,7,kernel,0.047619,"[1, 1, 1]",48,63037,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17860,0,0.011905,1,"[32, 4, 1]"
0,0,1.984,7,6939565502710.446,5,7,kernel,2.380952,"[50, 1, 1]",22,63052,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,17871,0,0.595238,1,"[128, 1, 1]"
0,0,2.016,7,6939565502713.262,0,7,kernel,0.190476,"[2, 2, 1]",30,63069,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,17874,0,0.047619,1,"[128, 1, 1]"
0,0,9.984,7,6939565502716.014,100,7,kernel,97.523811,"[1024, 2, 1]",30,63088,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,17881,0,24.380953,1,"[128, 1, 1]"
16,0,3.2,7,6939565502726.766,5,7,kernel,2.380952,"[50, 1, 1]",40,63125,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,17888,0,0.595238,1,"[128, 1, 1]"
0,0,4.544,7,6939565502730.83,0,7,kernel,0.190476,"[2, 1, 1]",38,63127,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,17888,0,0.02381,1,"[256, 1, 1]"
0,0,1.76,7,6939565502736.238,5,7,kernel,2.380952,"[50, 1, 1]",22,63156,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,17904,0,0.595238,1,"[128, 1, 1]"
25600,0,6.56,7,6939565502738.862,6,7,kernel,3.047619,"[8, 1, 8]",80,63186,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,17916,0,0.761905,1,"[128, 1, 1]"
0,0,2.016,7,6939565502746.254,25,7,kernel,12.190476,"[16, 4, 1]",44,63187,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,17916,0,0.761905,1,"[32, 16, 1]"
25600,0,5.76,7,6939565502749.038,6,7,kernel,3.047619,"[64, 1, 1]",80,63209,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,17920,0,0.761905,1,"[128, 1, 1]"
16,0,7.68,7,6939565502755.662,0,7,kernel,0.047619,"[1, 1, 1]",48,63221,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17924,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565502764.078,5,7,kernel,2.380952,"[50, 1, 1]",22,63284,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,17966,0,0.595238,1,"[128, 1, 1]"
16,0,3.328,7,6939565502766.286,10,7,kernel,4.761905,"[25, 1, 1]",32,63298,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,17967,0,0.297619,1,"[32, 16, 1]"
0,0,2.048,7,6939565502770.382,0,7,kernel,0.095238,"[2, 1, 1]",16,63311,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,17975,0,0.02381,1,"[128, 1, 1]"
53504,0,36.8,7,6939565502773.198,0,7,kernel,0.380952,"[1, 8, 1]",236,63325,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,17960,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565502811.086,15,7,kernel,7.142857,"[150, 1, 1]",16,63383,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18025,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565502815.054,15,7,kernel,7.142857,"[150, 1, 1]",16,63408,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18035,0,1.785714,1,"[128, 1, 1]"
0,0,1.76,7,6939565502819.566,15,7,kernel,7.142857,"[150, 1, 1]",22,63422,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18039,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565502822.19,15,7,kernel,7.142857,"[150, 1, 1]",16,63442,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18046,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565502826.286,15,7,kernel,7.142857,"[150, 1, 1]",22,63456,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18050,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565502828.814,30,7,kernel,14.285714,"[300, 1, 1]",16,63491,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,18071,0,3.571429,1,"[128, 1, 1]"
16384,0,12.768,7,6939565502831.918,38,7,kernel,18.285715,"[4, 2, 24]",57,63517,X,ampere_sgemm_128x32_nn,0,18082,0,2.285714,1,"[256, 1, 1]"
0,0,4.064,7,6939565502845.422,25,7,kernel,12.190476,"[16, 4, 1]",44,63519,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18082,0,0.761905,1,"[32, 16, 1]"
16384,0,11.424,7,6939565502850.19,38,7,kernel,18.285715,"[16, 12, 1]",57,63536,X,ampere_sgemm_32x128_nt,0,18086,0,2.285714,1,"[256, 1, 1]"
16,0,7.68,7,6939565502862.35,0,7,kernel,0.142857,"[3, 1, 1]",48,63549,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18090,0,0.035714,1,"[32, 4, 1]"
0,0,1.728,7,6939565502870.83,5,7,kernel,2.380952,"[50, 1, 1]",22,63572,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18109,0,0.595238,1,"[128, 1, 1]"
16,0,2.912,7,6939565502873.262,5,7,kernel,2.380952,"[50, 1, 1]",40,63607,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,18112,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565502876.878,0,7,kernel,0.190476,"[2, 1, 1]",38,63609,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,18112,0,0.02381,1,"[256, 1, 1]"
0,0,2.208,7,6939565505212.296,5,7,kernel,2.380952,"[50, 1, 1]",22,63638,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,18128,0,0.595238,1,"[128, 1, 1]"
16384,0,17.503,7,6939565505215.401,25,7,kernel,12.190476,"[16, 2, 4]",57,63664,X,ampere_sgemm_128x32_nn,0,18140,0,1.52381,1,"[256, 1, 1]"
0,0,3.008,7,6939565505233.672,67,7,kernel,48.761906,"[64, 4, 1]",44,63666,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18140,0,3.047619,1,"[32, 16, 1]"
16384,0,14.464,7,6939565505237.448,51,7,kernel,24.380953,"[16, 16, 1]",57,63683,X,ampere_sgemm_128x32_nt,0,18144,0,3.047619,1,"[256, 1, 1]"
16,0,7.968,7,6939565505252.68,0,7,kernel,0.047619,"[1, 1, 1]",48,63696,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18148,0,0.011905,1,"[32, 4, 1]"
0,0,2.304,7,6939565505261.416,20,7,kernel,9.523809,"[200, 1, 1]",22,63729,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,18169,0,2.380952,1,"[128, 1, 1]"
0,0,3.04,7,6939565505264.52,20,7,kernel,9.523809,"[200, 1, 1]",22,63743,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,18174,0,2.380952,1,"[128, 1, 1]"
16384,0,15.456,7,6939565505268.296,44,7,kernel,21.333334,"[4, 2, 28]",57,63768,X,ampere_sgemm_128x32_nn,0,18184,0,2.666667,1,"[256, 1, 1]"
0,0,3.616,7,6939565505284.584,25,7,kernel,12.190476,"[16, 4, 1]",44,63770,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18184,0,0.761905,1,"[32, 16, 1]"
16384,0,13.728,7,6939565505289.0,51,7,kernel,24.380953,"[16, 16, 1]",57,63787,X,ampere_sgemm_32x128_nt,0,18188,0,3.047619,1,"[256, 1, 1]"
16,0,6.944,7,6939565505303.56,0,7,kernel,0.190476,"[4, 1, 1]",48,63800,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18192,0,0.047619,1,"[32, 4, 1]"
0,0,1.856,7,6939565505311.336,5,7,kernel,2.380952,"[50, 1, 1]",22,63823,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18211,0,0.595238,1,"[128, 1, 1]"
16,0,2.848,7,6939565505313.96,5,7,kernel,2.380952,"[50, 1, 1]",40,63858,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,18214,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565505317.544,0,7,kernel,0.190476,"[2, 1, 1]",38,63860,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,18214,0,0.02381,1,"[256, 1, 1]"
0,0,1.664,7,6939565505322.728,5,7,kernel,2.380952,"[50, 1, 1]",22,63889,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,18230,0,0.595238,1,"[128, 1, 1]"
25600,0,6.912,7,6939565505325.224,6,7,kernel,3.047619,"[8, 1, 8]",80,63919,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,18242,0,0.761905,1,"[128, 1, 1]"
0,0,1.952,7,6939565505332.968,25,7,kernel,12.190476,"[16, 4, 1]",44,63920,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18242,0,0.761905,1,"[32, 16, 1]"
25600,0,6.112,7,6939565505335.624,6,7,kernel,3.047619,"[64, 1, 1]",80,63942,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,18246,0,0.761905,1,"[128, 1, 1]"
16,0,6.912,7,6939565505342.504,0,7,kernel,0.047619,"[1, 1, 1]",48,63954,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18250,0,0.011905,1,"[32, 4, 1]"
0,0,1.472,7,6939565505350.28,5,7,kernel,2.380952,"[50, 1, 1]",22,64017,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,18292,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565505352.488,10,7,kernel,4.761905,"[25, 1, 1]",32,64031,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18293,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565505356.616,0,7,kernel,0.095238,"[2, 1, 1]",16,64044,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,18301,0,0.02381,1,"[128, 1, 1]"
53504,0,36.736,7,6939565505359.4,0,7,kernel,0.380952,"[1, 8, 1]",236,64058,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,18286,0,0.095238,1,"[128, 1, 1]"
0,0,0.928,7,6939565505397.224,10,7,kernel,4.761905,"[100, 1, 1]",16,64116,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18351,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565505401.032,10,7,kernel,4.761905,"[100, 1, 1]",16,64141,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18361,0,1.190476,1,"[128, 1, 1]"
0,0,1.536,7,6939565505405.128,10,7,kernel,4.761905,"[100, 1, 1]",22,64155,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18365,0,1.190476,1,"[128, 1, 1]"
0,0,2.176,7,6939565505407.464,20,7,kernel,9.523809,"[200, 1, 1]",16,64190,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,18386,0,2.380952,1,"[128, 1, 1]"
16384,0,9.152,7,6939565505410.408,25,7,kernel,12.190476,"[4, 2, 16]",57,64216,X,ampere_sgemm_128x32_nn,0,18397,0,1.52381,1,"[256, 1, 1]"
0,0,2.88,7,6939565505420.776,25,7,kernel,12.190476,"[16, 4, 1]",44,64218,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18397,0,0.761905,1,"[32, 16, 1]"
16384,0,8.096,7,6939565505424.488,25,7,kernel,12.190476,"[16, 8, 1]",57,64235,X,ampere_sgemm_32x128_nt,0,18401,0,1.52381,1,"[256, 1, 1]"
16,0,7.872,7,6939565505433.384,0,7,kernel,0.095238,"[2, 1, 1]",48,64248,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18405,0,0.02381,1,"[32, 4, 1]"
0,0,1.856,7,6939565505441.96,5,7,kernel,2.380952,"[50, 1, 1]",22,64263,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18416,0,0.595238,1,"[128, 1, 1]"
25600,0,7.2,7,6939565505444.616,6,7,kernel,3.047619,"[8, 1, 8]",80,64295,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,18426,0,0.761905,1,"[128, 1, 1]"
0,0,2.016,7,6939565505452.552,25,7,kernel,12.190476,"[16, 4, 1]",44,64296,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18426,0,0.761905,1,"[32, 16, 1]"
25600,0,6.271,7,6939565507797.475,6,7,kernel,3.047619,"[64, 1, 1]",80,64318,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,18430,0,0.761905,1,"[128, 1, 1]"
16,0,8.065,7,6939565507804.578,0,7,kernel,0.047619,"[1, 1, 1]",48,64330,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18434,0,0.011905,1,"[32, 4, 1]"
0,0,2.304,7,6939565507813.442,5,7,kernel,2.380952,"[50, 1, 1]",22,64345,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18445,0,0.595238,1,"[128, 1, 1]"
0,0,2.272,7,6939565507816.546,0,7,kernel,0.190476,"[2, 2, 1]",30,64362,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,18448,0,0.047619,1,"[128, 1, 1]"
0,0,10.753,7,6939565507819.65,100,7,kernel,97.523811,"[1024, 2, 1]",30,64381,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,18455,0,24.380953,1,"[128, 1, 1]"
16,0,3.2,7,6939565507831.106,5,7,kernel,2.380952,"[50, 1, 1]",40,64418,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,18462,0,0.595238,1,"[128, 1, 1]"
0,0,4.608,7,6939565507835.042,0,7,kernel,0.190476,"[2, 1, 1]",38,64420,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,18462,0,0.02381,1,"[256, 1, 1]"
0,0,1.761,7,6939565507840.386,5,7,kernel,2.380952,"[50, 1, 1]",22,64449,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,18478,0,0.595238,1,"[128, 1, 1]"
25600,0,7.296,7,6939565507842.85,6,7,kernel,3.047619,"[8, 1, 8]",80,64479,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,18490,0,0.761905,1,"[128, 1, 1]"
0,0,2.016,7,6939565507850.882,25,7,kernel,12.190476,"[16, 4, 1]",44,64480,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18490,0,0.761905,1,"[32, 16, 1]"
25600,0,5.856,7,6939565507853.634,6,7,kernel,3.047619,"[64, 1, 1]",80,64502,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,18494,0,0.761905,1,"[128, 1, 1]"
16,0,6.976,7,6939565507860.322,0,7,kernel,0.047619,"[1, 1, 1]",48,64514,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18498,0,0.011905,1,"[32, 4, 1]"
0,0,1.472,7,6939565507868.066,5,7,kernel,2.380952,"[50, 1, 1]",22,64577,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,18540,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565507870.274,10,7,kernel,4.761905,"[25, 1, 1]",32,64591,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18541,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565507874.37,0,7,kernel,0.095238,"[2, 1, 1]",16,64604,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,18549,0,0.02381,1,"[128, 1, 1]"
53504,0,36.544,7,6939565507877.186,0,7,kernel,0.380952,"[1, 8, 1]",236,64618,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,18534,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565507914.818,15,7,kernel,7.142857,"[150, 1, 1]",16,64676,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18599,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565507919.01,15,7,kernel,7.142857,"[150, 1, 1]",16,64701,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18609,0,1.785714,1,"[128, 1, 1]"
0,0,1.76,7,6939565507923.138,15,7,kernel,7.142857,"[150, 1, 1]",22,64715,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18613,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565507925.634,15,7,kernel,7.142857,"[150, 1, 1]",16,64735,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18620,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565507929.698,15,7,kernel,7.142857,"[150, 1, 1]",22,64749,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18624,0,1.785714,1,"[128, 1, 1]"
0,0,2.272,7,6939565507932.162,30,7,kernel,14.285714,"[300, 1, 1]",16,64784,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,18645,0,3.571429,1,"[128, 1, 1]"
16384,0,12.224,7,6939565507935.298,38,7,kernel,18.285715,"[4, 2, 24]",57,64810,X,ampere_sgemm_128x32_nn,0,18656,0,2.285714,1,"[256, 1, 1]"
0,0,3.616,7,6939565507948.29,25,7,kernel,12.190476,"[16, 4, 1]",44,64812,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18656,0,0.761905,1,"[32, 16, 1]"
16384,0,11.2,7,6939565507952.674,38,7,kernel,18.285715,"[16, 12, 1]",57,64829,X,ampere_sgemm_32x128_nt,0,18660,0,2.285714,1,"[256, 1, 1]"
16,0,7.84,7,6939565507964.674,0,7,kernel,0.142857,"[3, 1, 1]",48,64842,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18664,0,0.035714,1,"[32, 4, 1]"
0,0,1.856,7,6939565507973.282,5,7,kernel,2.380952,"[50, 1, 1]",22,64865,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18683,0,0.595238,1,"[128, 1, 1]"
16,0,2.816,7,6939565507975.906,5,7,kernel,2.380952,"[50, 1, 1]",40,64900,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,18686,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565507979.426,0,7,kernel,0.190476,"[2, 1, 1]",38,64902,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,18686,0,0.02381,1,"[256, 1, 1]"
0,0,1.632,7,6939565507984.61,5,7,kernel,2.380952,"[50, 1, 1]",22,64931,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,18702,0,0.595238,1,"[128, 1, 1]"
16384,0,15.648,7,6939565507987.074,25,7,kernel,12.190476,"[16, 2, 4]",57,64957,X,ampere_sgemm_128x32_nn,0,18714,0,1.52381,1,"[256, 1, 1]"
0,0,2.848,7,6939565508003.586,67,7,kernel,48.761906,"[64, 4, 1]",44,64959,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18714,0,3.047619,1,"[32, 16, 1]"
16384,0,14.624,7,6939565508007.17,51,7,kernel,24.380953,"[16, 16, 1]",57,64976,X,ampere_sgemm_128x32_nt,0,18718,0,3.047619,1,"[256, 1, 1]"
16,0,10.209,7,6939565510318.908,0,7,kernel,0.047619,"[1, 1, 1]",48,64989,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18722,0,0.011905,1,"[32, 4, 1]"
0,0,2.88,7,6939565510329.853,20,7,kernel,9.523809,"[200, 1, 1]",22,65022,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,18743,0,2.380952,1,"[128, 1, 1]"
0,0,2.881,7,6939565510333.468,20,7,kernel,9.523809,"[200, 1, 1]",22,65036,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,18748,0,2.380952,1,"[128, 1, 1]"
16384,0,17.024,7,6939565510337.116,44,7,kernel,21.333334,"[4, 2, 28]",57,65061,X,ampere_sgemm_128x32_nn,0,18758,0,2.666667,1,"[256, 1, 1]"
0,0,3.808,7,6939565510354.941,25,7,kernel,12.190476,"[16, 4, 1]",44,65063,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18758,0,0.761905,1,"[32, 16, 1]"
16384,0,13.952,7,6939565510359.516,51,7,kernel,24.380953,"[16, 16, 1]",57,65080,X,ampere_sgemm_32x128_nt,0,18762,0,3.047619,1,"[256, 1, 1]"
16,0,7.072,7,6939565510374.236,0,7,kernel,0.190476,"[4, 1, 1]",48,65093,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18766,0,0.047619,1,"[32, 4, 1]"
0,0,1.856,7,6939565510382.044,5,7,kernel,2.380952,"[50, 1, 1]",22,65116,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18785,0,0.595238,1,"[128, 1, 1]"
16,0,2.847,7,6939565510384.669,5,7,kernel,2.380952,"[50, 1, 1]",40,65151,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,18788,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565510388.252,0,7,kernel,0.190476,"[2, 1, 1]",38,65153,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,18788,0,0.02381,1,"[256, 1, 1]"
0,0,1.599,7,6939565510393.437,5,7,kernel,2.380952,"[50, 1, 1]",22,65182,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,18804,0,0.595238,1,"[128, 1, 1]"
25600,0,7.136,7,6939565510395.74,6,7,kernel,3.047619,"[8, 1, 8]",80,65212,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,18816,0,0.761905,1,"[128, 1, 1]"
0,0,2.017,7,6939565510403.58,25,7,kernel,12.190476,"[16, 4, 1]",44,65213,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18816,0,0.761905,1,"[32, 16, 1]"
25600,0,6.049,7,6939565510406.396,6,7,kernel,3.047619,"[64, 1, 1]",80,65235,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,18820,0,0.761905,1,"[128, 1, 1]"
16,0,6.464,7,6939565510413.148,0,7,kernel,0.047619,"[1, 1, 1]",48,65247,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18824,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565510420.316,5,7,kernel,2.380952,"[50, 1, 1]",22,65310,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,18866,0,0.595238,1,"[128, 1, 1]"
16,0,3.424,7,6939565510422.525,10,7,kernel,4.761905,"[25, 1, 1]",32,65324,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18867,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565510426.716,0,7,kernel,0.095238,"[2, 1, 1]",16,65337,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,18875,0,0.02381,1,"[128, 1, 1]"
53504,0,36.512,7,6939565510429.532,0,7,kernel,0.380952,"[1, 8, 1]",236,65351,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,18860,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565510467.068,10,7,kernel,4.761905,"[100, 1, 1]",16,65409,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18925,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565510470.844,10,7,kernel,4.761905,"[100, 1, 1]",16,65434,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,18935,0,1.190476,1,"[128, 1, 1]"
0,0,1.536,7,6939565510474.94,10,7,kernel,4.761905,"[100, 1, 1]",22,65448,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18939,0,1.190476,1,"[128, 1, 1]"
0,0,2.176,7,6939565510477.276,20,7,kernel,9.523809,"[200, 1, 1]",16,65483,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,18960,0,2.380952,1,"[128, 1, 1]"
16384,0,9.12,7,6939565510480.252,25,7,kernel,12.190476,"[4, 2, 16]",57,65509,X,ampere_sgemm_128x32_nn,0,18971,0,1.52381,1,"[256, 1, 1]"
0,0,3.04,7,6939565510490.108,25,7,kernel,12.190476,"[16, 4, 1]",44,65511,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,18971,0,0.761905,1,"[32, 16, 1]"
16384,0,8.16,7,6939565510493.884,25,7,kernel,12.190476,"[16, 8, 1]",57,65528,X,ampere_sgemm_32x128_nt,0,18975,0,1.52381,1,"[256, 1, 1]"
16,0,7.776,7,6939565510502.812,0,7,kernel,0.095238,"[2, 1, 1]",48,65541,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,18979,0,0.02381,1,"[32, 4, 1]"
0,0,1.76,7,6939565510511.388,5,7,kernel,2.380952,"[50, 1, 1]",22,65556,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,18990,0,0.595238,1,"[128, 1, 1]"
25600,0,7.04,7,6939565510513.884,6,7,kernel,3.047619,"[8, 1, 8]",80,65588,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,19000,0,0.761905,1,"[128, 1, 1]"
0,0,1.984,7,6939565510521.724,25,7,kernel,12.190476,"[16, 4, 1]",44,65589,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19000,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565510524.54,6,7,kernel,3.047619,"[64, 1, 1]",80,65611,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,19004,0,0.761905,1,"[128, 1, 1]"
16,0,6.176,7,6939565510531.292,0,7,kernel,0.047619,"[1, 1, 1]",48,65623,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19008,0,0.011905,1,"[32, 4, 1]"
0,0,2.176,7,6939565512894.647,5,7,kernel,2.380952,"[50, 1, 1]",22,65638,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19019,0,0.595238,1,"[128, 1, 1]"
0,0,2.335,7,6939565512897.591,0,7,kernel,0.190476,"[2, 2, 1]",30,65655,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,19022,0,0.047619,1,"[128, 1, 1]"
0,0,11.455,7,6939565512900.663,100,7,kernel,97.523811,"[1024, 2, 1]",30,65674,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,19029,0,24.380953,1,"[128, 1, 1]"
16,0,2.944,7,6939565512912.855,5,7,kernel,2.380952,"[50, 1, 1]",40,65711,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,19036,0,0.595238,1,"[128, 1, 1]"
0,0,4.48,7,6939565512916.598,0,7,kernel,0.190476,"[2, 1, 1]",38,65713,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,19036,0,0.02381,1,"[256, 1, 1]"
0,0,1.759,7,6939565512921.847,5,7,kernel,2.380952,"[50, 1, 1]",22,65742,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19052,0,0.595238,1,"[128, 1, 1]"
25600,0,7.361,7,6939565512924.31,6,7,kernel,3.047619,"[8, 1, 8]",80,65772,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,19064,0,0.761905,1,"[128, 1, 1]"
0,0,2.049,7,6939565512932.374,25,7,kernel,12.190476,"[16, 4, 1]",44,65773,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19064,0,0.761905,1,"[32, 16, 1]"
25600,0,6.081,7,6939565512935.158,6,7,kernel,3.047619,"[64, 1, 1]",80,65795,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,19068,0,0.761905,1,"[128, 1, 1]"
16,0,7.809,7,6939565512942.07,0,7,kernel,0.047619,"[1, 1, 1]",48,65807,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19072,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565512950.614,5,7,kernel,2.380952,"[50, 1, 1]",22,65870,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,19114,0,0.595238,1,"[128, 1, 1]"
16,0,3.359,7,6939565512952.823,10,7,kernel,4.761905,"[25, 1, 1]",32,65884,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19115,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565512956.95,0,7,kernel,0.095238,"[2, 1, 1]",16,65897,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,19123,0,0.02381,1,"[128, 1, 1]"
53504,0,36.673,7,6939565512959.766,0,7,kernel,0.380952,"[1, 8, 1]",236,65911,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,19108,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565512997.59,15,7,kernel,7.142857,"[150, 1, 1]",16,65969,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19173,0,1.785714,1,"[128, 1, 1]"
0,0,1.215,7,6939565513001.783,15,7,kernel,7.142857,"[150, 1, 1]",16,65994,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19183,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565513005.847,15,7,kernel,7.142857,"[150, 1, 1]",22,66008,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19187,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565513008.31,15,7,kernel,7.142857,"[150, 1, 1]",16,66028,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19194,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565513012.438,15,7,kernel,7.142857,"[150, 1, 1]",22,66042,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19198,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565513014.934,30,7,kernel,14.285714,"[300, 1, 1]",16,66077,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,19219,0,3.571429,1,"[128, 1, 1]"
16384,0,12.767,7,6939565513018.039,38,7,kernel,18.285715,"[4, 2, 24]",57,66103,X,ampere_sgemm_128x32_nn,0,19230,0,2.285714,1,"[256, 1, 1]"
0,0,3.872,7,6939565513031.542,25,7,kernel,12.190476,"[16, 4, 1]",44,66105,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19230,0,0.761905,1,"[32, 16, 1]"
16384,0,11.392,7,6939565513036.118,38,7,kernel,18.285715,"[16, 12, 1]",57,66122,X,ampere_sgemm_32x128_nt,0,19234,0,2.285714,1,"[256, 1, 1]"
16,0,7.712,7,6939565513048.278,0,7,kernel,0.142857,"[3, 1, 1]",48,66135,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19238,0,0.035714,1,"[32, 4, 1]"
0,0,1.792,7,6939565513056.726,5,7,kernel,2.380952,"[50, 1, 1]",22,66158,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19257,0,0.595238,1,"[128, 1, 1]"
16,0,2.784,7,6939565513059.35,5,7,kernel,2.380952,"[50, 1, 1]",40,66193,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,19260,0,0.595238,1,"[128, 1, 1]"
0,0,4.384,7,6939565513062.87,0,7,kernel,0.190476,"[2, 1, 1]",38,66195,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,19260,0,0.02381,1,"[256, 1, 1]"
0,0,1.6,7,6939565513068.118,5,7,kernel,2.380952,"[50, 1, 1]",22,66224,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19276,0,0.595238,1,"[128, 1, 1]"
16384,0,15.648,7,6939565513070.55,25,7,kernel,12.190476,"[16, 2, 4]",57,66250,X,ampere_sgemm_128x32_nn,0,19288,0,1.52381,1,"[256, 1, 1]"
0,0,2.88,7,6939565513086.966,67,7,kernel,48.761906,"[64, 4, 1]",44,66252,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19288,0,3.047619,1,"[32, 16, 1]"
16384,0,14.208,7,6939565513090.614,51,7,kernel,24.380953,"[16, 16, 1]",57,66269,X,ampere_sgemm_128x32_nt,0,19292,0,3.047619,1,"[256, 1, 1]"
16,0,5.728,7,6939565513105.558,0,7,kernel,0.047619,"[1, 1, 1]",48,66282,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19296,0,0.011905,1,"[32, 4, 1]"
0,0,2.464,7,6939565513112.054,20,7,kernel,9.523809,"[200, 1, 1]",22,66315,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19317,0,2.380952,1,"[128, 1, 1]"
0,0,3.36,7,6939565515464.177,20,7,kernel,9.523809,"[200, 1, 1]",22,66329,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,19322,0,2.380952,1,"[128, 1, 1]"
16384,0,16.736,7,6939565515468.305,44,7,kernel,21.333334,"[4, 2, 28]",57,66354,X,ampere_sgemm_128x32_nn,0,19332,0,2.666667,1,"[256, 1, 1]"
0,0,3.647,7,6939565515485.777,25,7,kernel,12.190476,"[16, 4, 1]",44,66356,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19332,0,0.761905,1,"[32, 16, 1]"
16384,0,14.431,7,6939565515490.161,51,7,kernel,24.380953,"[16, 16, 1]",57,66373,X,ampere_sgemm_32x128_nt,0,19336,0,3.047619,1,"[256, 1, 1]"
16,0,7.521,7,6939565515505.296,0,7,kernel,0.190476,"[4, 1, 1]",48,66386,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19340,0,0.047619,1,"[32, 4, 1]"
0,0,1.889,7,6939565515513.584,5,7,kernel,2.380952,"[50, 1, 1]",22,66409,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19359,0,0.595238,1,"[128, 1, 1]"
16,0,2.817,7,6939565515516.208,5,7,kernel,2.380952,"[50, 1, 1]",40,66444,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,19362,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565515519.824,0,7,kernel,0.190476,"[2, 1, 1]",38,66446,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,19362,0,0.02381,1,"[256, 1, 1]"
0,0,1.695,7,6939565515525.009,5,7,kernel,2.380952,"[50, 1, 1]",22,66475,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19378,0,0.595238,1,"[128, 1, 1]"
25600,0,6.88,7,6939565515527.472,6,7,kernel,3.047619,"[8, 1, 8]",80,66505,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,19390,0,0.761905,1,"[128, 1, 1]"
0,0,1.951,7,6939565515535.153,25,7,kernel,12.190476,"[16, 4, 1]",44,66506,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19390,0,0.761905,1,"[32, 16, 1]"
25600,0,6.113,7,6939565515537.936,6,7,kernel,3.047619,"[64, 1, 1]",80,66528,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,19394,0,0.761905,1,"[128, 1, 1]"
16,0,6.559,7,6939565515544.913,0,7,kernel,0.047619,"[1, 1, 1]",48,66540,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19398,0,0.011905,1,"[32, 4, 1]"
0,0,1.409,7,6939565515552.272,5,7,kernel,2.380952,"[50, 1, 1]",22,66603,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,19440,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565515554.48,10,7,kernel,4.761905,"[25, 1, 1]",32,66617,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19441,0,0.297619,1,"[32, 16, 1]"
0,0,1.983,7,6939565515558.609,0,7,kernel,0.095238,"[2, 1, 1]",16,66630,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,19449,0,0.02381,1,"[128, 1, 1]"
53504,0,36.544,7,6939565515561.425,0,7,kernel,0.380952,"[1, 8, 1]",236,66644,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,19434,0,0.095238,1,"[128, 1, 1]"
0,0,0.927,7,6939565515599.057,10,7,kernel,4.761905,"[100, 1, 1]",16,66702,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19499,0,1.190476,1,"[128, 1, 1]"
0,0,1.184,7,6939565515602.864,10,7,kernel,4.761905,"[100, 1, 1]",16,66727,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19509,0,1.190476,1,"[128, 1, 1]"
0,0,1.535,7,6939565515606.961,10,7,kernel,4.761905,"[100, 1, 1]",22,66741,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19513,0,1.190476,1,"[128, 1, 1]"
0,0,2.144,7,6939565515609.297,20,7,kernel,9.523809,"[200, 1, 1]",16,66776,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,19534,0,2.380952,1,"[128, 1, 1]"
16384,0,9.184,7,6939565515612.24,25,7,kernel,12.190476,"[4, 2, 16]",57,66802,X,ampere_sgemm_128x32_nn,0,19545,0,1.52381,1,"[256, 1, 1]"
0,0,3.008,7,6939565515622.288,25,7,kernel,12.190476,"[16, 4, 1]",44,66804,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19545,0,0.761905,1,"[32, 16, 1]"
16384,0,8.096,7,6939565515626.064,25,7,kernel,12.190476,"[16, 8, 1]",57,66821,X,ampere_sgemm_32x128_nt,0,19549,0,1.52381,1,"[256, 1, 1]"
16,0,7.84,7,6939565515634.992,0,7,kernel,0.095238,"[2, 1, 1]",48,66834,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19553,0,0.02381,1,"[32, 4, 1]"
0,0,1.824,7,6939565515643.6,5,7,kernel,2.380952,"[50, 1, 1]",22,66849,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19564,0,0.595238,1,"[128, 1, 1]"
25600,0,6.976,7,6939565515646.256,6,7,kernel,3.047619,"[8, 1, 8]",80,66881,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,19574,0,0.761905,1,"[128, 1, 1]"
0,0,1.984,7,6939565515654.032,25,7,kernel,12.190476,"[16, 4, 1]",44,66882,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19574,0,0.761905,1,"[32, 16, 1]"
25600,0,5.984,7,6939565515656.816,6,7,kernel,3.047619,"[64, 1, 1]",80,66904,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,19578,0,0.761905,1,"[128, 1, 1]"
16,0,6.048,7,6939565515663.6,0,7,kernel,0.047619,"[1, 1, 1]",48,66916,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19582,0,0.011905,1,"[32, 4, 1]"
0,0,1.824,7,6939565515670.416,5,7,kernel,2.380952,"[50, 1, 1]",22,66931,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19593,0,0.595238,1,"[128, 1, 1]"
0,0,1.984,7,6939565515673.008,0,7,kernel,0.190476,"[2, 2, 1]",30,66948,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,19596,0,0.047619,1,"[128, 1, 1]"
0,0,8.672,7,6939565515675.792,100,7,kernel,97.523811,"[1024, 2, 1]",30,66967,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,19603,0,24.380953,1,"[128, 1, 1]"
16,0,3.008,7,6939565515685.232,5,7,kernel,2.380952,"[50, 1, 1]",40,67004,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,19610,0,0.595238,1,"[128, 1, 1]"
0,0,4.736,7,6939565515689.008,0,7,kernel,0.190476,"[2, 1, 1]",38,67006,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,19610,0,0.02381,1,"[256, 1, 1]"
0,0,2.112,7,6939565518022.763,5,7,kernel,2.380952,"[50, 1, 1]",22,67035,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19626,0,0.595238,1,"[128, 1, 1]"
25600,0,7.296,7,6939565518025.643,6,7,kernel,3.047619,"[8, 1, 8]",80,67065,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,19638,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565518033.643,25,7,kernel,12.190476,"[16, 4, 1]",44,67066,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19638,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565518036.715,6,7,kernel,3.047619,"[64, 1, 1]",80,67088,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,19642,0,0.761905,1,"[128, 1, 1]"
16,0,8.159,7,6939565518043.467,0,7,kernel,0.047619,"[1, 1, 1]",48,67100,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19646,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565518052.363,5,7,kernel,2.380952,"[50, 1, 1]",22,67163,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,19688,0,0.595238,1,"[128, 1, 1]"
16,0,3.296,7,6939565518054.571,10,7,kernel,4.761905,"[25, 1, 1]",32,67177,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19689,0,0.297619,1,"[32, 16, 1]"
0,0,2.017,7,6939565518058.666,0,7,kernel,0.095238,"[2, 1, 1]",16,67190,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,19697,0,0.02381,1,"[128, 1, 1]"
53504,0,36.959,7,6939565518061.483,0,7,kernel,0.380952,"[1, 8, 1]",236,67204,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,19682,0,0.095238,1,"[128, 1, 1]"
0,0,1.12,7,6939565518099.403,15,7,kernel,7.142857,"[150, 1, 1]",16,67262,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19747,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565518103.755,15,7,kernel,7.142857,"[150, 1, 1]",16,67287,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19757,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565518107.85,15,7,kernel,7.142857,"[150, 1, 1]",22,67301,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19761,0,1.785714,1,"[128, 1, 1]"
0,0,1.215,7,6939565518110.315,15,7,kernel,7.142857,"[150, 1, 1]",16,67321,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,19768,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565518114.314,15,7,kernel,7.142857,"[150, 1, 1]",22,67335,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19772,0,1.785714,1,"[128, 1, 1]"
0,0,2.241,7,6939565518116.81,30,7,kernel,14.285714,"[300, 1, 1]",16,67370,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,19793,0,3.571429,1,"[128, 1, 1]"
16384,0,12.735,7,6939565518119.755,38,7,kernel,18.285715,"[4, 2, 24]",57,67396,X,ampere_sgemm_128x32_nn,0,19804,0,2.285714,1,"[256, 1, 1]"
0,0,3.679,7,6939565518133.227,25,7,kernel,12.190476,"[16, 4, 1]",44,67398,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19804,0,0.761905,1,"[32, 16, 1]"
16384,0,11.615,7,6939565518137.675,38,7,kernel,18.285715,"[16, 12, 1]",57,67415,X,ampere_sgemm_32x128_nt,0,19808,0,2.285714,1,"[256, 1, 1]"
16,0,7.68,7,6939565518150.123,0,7,kernel,0.142857,"[3, 1, 1]",48,67428,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19812,0,0.035714,1,"[32, 4, 1]"
0,0,1.76,7,6939565518158.635,5,7,kernel,2.380952,"[50, 1, 1]",22,67451,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19831,0,0.595238,1,"[128, 1, 1]"
16,0,2.881,7,6939565518161.098,5,7,kernel,2.380952,"[50, 1, 1]",40,67486,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,19834,0,0.595238,1,"[128, 1, 1]"
0,0,4.417,7,6939565518164.714,0,7,kernel,0.190476,"[2, 1, 1]",38,67488,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,19834,0,0.02381,1,"[256, 1, 1]"
0,0,1.601,7,6939565518169.898,5,7,kernel,2.380952,"[50, 1, 1]",22,67517,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19850,0,0.595238,1,"[128, 1, 1]"
16384,0,15.839,7,6939565518172.363,25,7,kernel,12.190476,"[16, 2, 4]",57,67543,X,ampere_sgemm_128x32_nn,0,19862,0,1.52381,1,"[256, 1, 1]"
0,0,2.849,7,6939565518188.938,67,7,kernel,48.761906,"[64, 4, 1]",44,67545,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19862,0,3.047619,1,"[32, 16, 1]"
16384,0,14.88,7,6939565518192.586,51,7,kernel,24.380953,"[16, 16, 1]",57,67562,X,ampere_sgemm_128x32_nt,0,19866,0,3.047619,1,"[256, 1, 1]"
16,0,5.728,7,6939565518208.202,0,7,kernel,0.047619,"[1, 1, 1]",48,67575,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19870,0,0.011905,1,"[32, 4, 1]"
0,0,2.272,7,6939565518214.698,20,7,kernel,9.523809,"[200, 1, 1]",22,67608,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19891,0,2.380952,1,"[128, 1, 1]"
0,0,3.008,7,6939565518217.802,20,7,kernel,9.523809,"[200, 1, 1]",22,67622,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,19896,0,2.380952,1,"[128, 1, 1]"
16384,0,15.104,7,6939565518221.546,44,7,kernel,21.333334,"[4, 2, 28]",57,67647,X,ampere_sgemm_128x32_nn,0,19906,0,2.666667,1,"[256, 1, 1]"
0,0,3.616,7,6939565518237.354,25,7,kernel,12.190476,"[16, 4, 1]",44,67649,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19906,0,0.761905,1,"[32, 16, 1]"
16384,0,13.92,7,6939565518241.77,51,7,kernel,24.380953,"[16, 16, 1]",57,67666,X,ampere_sgemm_32x128_nt,0,19910,0,3.047619,1,"[256, 1, 1]"
16,0,6.88,7,6939565518256.426,0,7,kernel,0.190476,"[4, 1, 1]",48,67679,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19914,0,0.047619,1,"[32, 4, 1]"
0,0,2.112,7,6939565520615.845,5,7,kernel,2.380952,"[50, 1, 1]",22,67702,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,19933,0,0.595238,1,"[128, 1, 1]"
16,0,3.104,7,6939565520618.757,5,7,kernel,2.380952,"[50, 1, 1]",40,67737,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,19936,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565520622.693,0,7,kernel,0.190476,"[2, 1, 1]",38,67739,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,19936,0,0.02381,1,"[256, 1, 1]"
0,0,1.792,7,6939565520627.877,5,7,kernel,2.380952,"[50, 1, 1]",22,67768,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,19952,0,0.595238,1,"[128, 1, 1]"
25600,0,7.296,7,6939565520630.501,6,7,kernel,3.047619,"[8, 1, 8]",80,67798,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,19964,0,0.761905,1,"[128, 1, 1]"
0,0,2.016,7,6939565520638.533,25,7,kernel,12.190476,"[16, 4, 1]",44,67799,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,19964,0,0.761905,1,"[32, 16, 1]"
25600,0,6.015,7,6939565520641.349,6,7,kernel,3.047619,"[64, 1, 1]",80,67821,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,19968,0,0.761905,1,"[128, 1, 1]"
16,0,7.968,7,6939565520648.133,0,7,kernel,0.047619,"[1, 1, 1]",48,67833,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,19972,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565520656.901,5,7,kernel,2.380952,"[50, 1, 1]",22,67896,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,20014,0,0.595238,1,"[128, 1, 1]"
16,0,3.393,7,6939565520659.108,10,7,kernel,4.761905,"[25, 1, 1]",32,67910,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20015,0,0.297619,1,"[32, 16, 1]"
0,0,1.985,7,6939565520663.204,0,7,kernel,0.095238,"[2, 1, 1]",16,67923,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,20023,0,0.02381,1,"[128, 1, 1]"
53504,0,36.672,7,6939565520666.021,0,7,kernel,0.380952,"[1, 8, 1]",236,67937,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,20008,0,0.095238,1,"[128, 1, 1]"
0,0,1.088,7,6939565520703.685,10,7,kernel,4.761905,"[100, 1, 1]",16,67995,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20073,0,1.190476,1,"[128, 1, 1]"
0,0,1.153,7,6939565520707.652,10,7,kernel,4.761905,"[100, 1, 1]",16,68020,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20083,0,1.190476,1,"[128, 1, 1]"
0,0,1.569,7,6939565520711.748,10,7,kernel,4.761905,"[100, 1, 1]",22,68034,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20087,0,1.190476,1,"[128, 1, 1]"
0,0,2.144,7,6939565520714.052,20,7,kernel,9.523809,"[200, 1, 1]",16,68069,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,20108,0,2.380952,1,"[128, 1, 1]"
16384,0,9.376,7,6939565520716.997,25,7,kernel,12.190476,"[4, 2, 16]",57,68095,X,ampere_sgemm_128x32_nn,0,20119,0,1.52381,1,"[256, 1, 1]"
0,0,2.817,7,6939565520727.172,25,7,kernel,12.190476,"[16, 4, 1]",44,68097,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20119,0,0.761905,1,"[32, 16, 1]"
16384,0,8.065,7,6939565520730.756,25,7,kernel,12.190476,"[16, 8, 1]",57,68114,X,ampere_sgemm_32x128_nt,0,20123,0,1.52381,1,"[256, 1, 1]"
16,0,7.775,7,6939565520739.653,0,7,kernel,0.095238,"[2, 1, 1]",48,68127,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20127,0,0.02381,1,"[32, 4, 1]"
0,0,1.855,7,6939565520748.197,5,7,kernel,2.380952,"[50, 1, 1]",22,68142,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20138,0,0.595238,1,"[128, 1, 1]"
25600,0,7.104,7,6939565520750.82,6,7,kernel,3.047619,"[8, 1, 8]",80,68174,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,20148,0,0.761905,1,"[128, 1, 1]"
0,0,1.983,7,6939565520758.693,25,7,kernel,12.190476,"[16, 4, 1]",44,68175,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20148,0,0.761905,1,"[32, 16, 1]"
25600,0,5.984,7,6939565520761.508,6,7,kernel,3.047619,"[64, 1, 1]",80,68197,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,20152,0,0.761905,1,"[128, 1, 1]"
16,0,5.984,7,6939565520768.26,0,7,kernel,0.047619,"[1, 1, 1]",48,68209,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20156,0,0.011905,1,"[32, 4, 1]"
0,0,1.888,7,6939565520775.044,5,7,kernel,2.380952,"[50, 1, 1]",22,68224,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20167,0,0.595238,1,"[128, 1, 1]"
0,0,1.985,7,6939565520777.668,0,7,kernel,0.190476,"[2, 2, 1]",30,68241,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 1, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,20170,0,0.047619,1,"[128, 1, 1]"
0,0,9.216,7,6939565520780.42,100,7,kernel,97.523811,"[1024, 2, 1]",30,68260,X,"void at::native::(anonymous namespace)::CatArrayBatchedCopy_aligned16_contig<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 2, 128, 1>(at::native::(anonymous namespace)::OpaqueType<4u>*, at::native::(anonymous namespace)::CatArrInputTensorMetadata<at::native::(anonymous namespace)::OpaqueType<4u>, unsigned int, 128, 1>, at::native::(anonymous namespace)::TensorSizeStride<unsigned int, 4u>, int, unsigned int)",0,20177,0,24.380953,1,"[128, 1, 1]"
16,0,3.072,7,6939565520790.404,5,7,kernel,2.380952,"[50, 1, 1]",40,68297,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,20184,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565520794.34,0,7,kernel,0.190476,"[2, 1, 1]",38,68299,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,20184,0,0.02381,1,"[256, 1, 1]"
0,0,1.76,7,6939565520799.492,5,7,kernel,2.380952,"[50, 1, 1]",22,68328,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,20200,0,0.595238,1,"[128, 1, 1]"
25600,0,6.784,7,6939565520802.244,6,7,kernel,3.047619,"[8, 1, 8]",80,68358,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,20212,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565520809.764,25,7,kernel,12.190476,"[16, 4, 1]",44,68359,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20212,0,0.761905,1,"[32, 16, 1]"
25600,0,5.888,7,6939565520812.612,6,7,kernel,3.047619,"[64, 1, 1]",80,68381,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,20216,0,0.761905,1,"[128, 1, 1]"
16,0,10.08,7,6939565523172.991,0,7,kernel,0.047619,"[1, 1, 1]",48,68393,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20220,0,0.011905,1,"[32, 4, 1]"
0,0,2.144,7,6939565523183.871,5,7,kernel,2.380952,"[50, 1, 1]",22,68456,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,20262,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565523186.719,10,7,kernel,4.761905,"[25, 1, 1]",32,68470,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20263,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565523190.847,0,7,kernel,0.095238,"[2, 1, 1]",16,68483,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,20271,0,0.02381,1,"[128, 1, 1]"
53504,0,36.608,7,6939565523193.663,0,7,kernel,0.380952,"[1, 8, 1]",236,68497,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,20256,0,0.095238,1,"[128, 1, 1]"
0,0,1.12,7,6939565523231.199,15,7,kernel,7.142857,"[150, 1, 1]",16,68555,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20321,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565523235.231,15,7,kernel,7.142857,"[150, 1, 1]",16,68580,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20331,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565523239.327,15,7,kernel,7.142857,"[150, 1, 1]",22,68594,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20335,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565523241.759,15,7,kernel,7.142857,"[150, 1, 1]",16,68614,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20342,0,1.785714,1,"[128, 1, 1]"
0,0,1.665,7,6939565523245.886,15,7,kernel,7.142857,"[150, 1, 1]",22,68628,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20346,0,1.785714,1,"[128, 1, 1]"
0,0,2.239,7,6939565523248.383,30,7,kernel,14.285714,"[300, 1, 1]",16,68663,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,20367,0,3.571429,1,"[128, 1, 1]"
16384,0,13.183,7,6939565523251.519,38,7,kernel,18.285715,"[4, 2, 24]",57,68689,X,ampere_sgemm_128x32_nn,0,20378,0,2.285714,1,"[256, 1, 1]"
0,0,2.912,7,6939565523265.503,25,7,kernel,12.190476,"[16, 4, 1]",44,68691,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20378,0,0.761905,1,"[32, 16, 1]"
16384,0,11.328,7,6939565523269.151,38,7,kernel,18.285715,"[16, 12, 1]",57,68708,X,ampere_sgemm_32x128_nt,0,20382,0,2.285714,1,"[256, 1, 1]"
16,0,7.617,7,6939565523281.214,0,7,kernel,0.142857,"[3, 1, 1]",48,68721,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20386,0,0.035714,1,"[32, 4, 1]"
0,0,1.791,7,6939565523289.631,5,7,kernel,2.380952,"[50, 1, 1]",22,68744,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20405,0,0.595238,1,"[128, 1, 1]"
16,0,2.976,7,6939565523292.223,5,7,kernel,2.380952,"[50, 1, 1]",40,68779,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,20408,0,0.595238,1,"[128, 1, 1]"
0,0,4.48,7,6939565523296.031,0,7,kernel,0.190476,"[2, 1, 1]",38,68781,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,20408,0,0.02381,1,"[256, 1, 1]"
16,0,2.624,7,6939565523301.246,5,7,kernel,2.380952,"[50, 1, 1]",40,68822,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,20422,0,0.595238,1,"[128, 1, 1]"
0,0,4.417,7,6939565523304.67,0,7,kernel,0.190476,"[2, 1, 1]",38,68824,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,20422,0,0.02381,1,"[256, 1, 1]"
0,0,1.761,7,6939565523309.854,5,7,kernel,2.380952,"[50, 1, 1]",22,68853,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,20438,0,0.595238,1,"[128, 1, 1]"
16384,0,15.648,7,6939565523312.479,25,7,kernel,12.190476,"[16, 2, 4]",57,68879,X,ampere_sgemm_128x32_nn,0,20450,0,1.52381,1,"[256, 1, 1]"
0,0,2.976,7,6939565523328.83,67,7,kernel,48.761906,"[64, 4, 1]",44,68881,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20450,0,3.047619,1,"[32, 16, 1]"
16384,0,15.136,7,6939565523332.639,51,7,kernel,24.380953,"[16, 16, 1]",57,68898,X,ampere_sgemm_128x32_nt,0,20454,0,3.047619,1,"[256, 1, 1]"
16,0,5.792,7,6939565523348.542,0,7,kernel,0.047619,"[1, 1, 1]",48,68911,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20458,0,0.011905,1,"[32, 4, 1]"
0,0,2.56,7,6939565523355.166,20,7,kernel,9.523809,"[200, 1, 1]",22,68944,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,20479,0,2.380952,1,"[128, 1, 1]"
0,0,3.104,7,6939565523358.462,20,7,kernel,9.523809,"[200, 1, 1]",22,68958,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,20484,0,2.380952,1,"[128, 1, 1]"
16384,0,14.816,7,6939565523362.398,44,7,kernel,21.333334,"[4, 2, 28]",57,68983,X,ampere_sgemm_128x32_nn,0,20494,0,2.666667,1,"[256, 1, 1]"
0,0,3.392,7,6939565523378.014,25,7,kernel,12.190476,"[16, 4, 1]",44,68985,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20494,0,0.761905,1,"[32, 16, 1]"
16384,0,13.92,7,6939565523382.142,51,7,kernel,24.380953,"[16, 16, 1]",57,69002,X,ampere_sgemm_32x128_nt,0,20498,0,3.047619,1,"[256, 1, 1]"
16,0,6.592,7,6939565523396.83,0,7,kernel,0.190476,"[4, 1, 1]",48,69015,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20502,0,0.047619,1,"[32, 4, 1]"
0,0,1.856,7,6939565523404.158,5,7,kernel,2.380952,"[50, 1, 1]",22,69038,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20521,0,0.595238,1,"[128, 1, 1]"
16,0,3.136,7,6939565525701.721,5,7,kernel,2.380952,"[50, 1, 1]",40,69073,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,20524,0,0.595238,1,"[128, 1, 1]"
0,0,4.704,7,6939565525705.625,0,7,kernel,0.190476,"[2, 1, 1]",38,69075,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,20524,0,0.02381,1,"[256, 1, 1]"
0,0,1.76,7,6939565525711.161,5,7,kernel,2.380952,"[50, 1, 1]",22,69104,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,20540,0,0.595238,1,"[128, 1, 1]"
25600,0,7.232,7,6939565525713.625,6,7,kernel,3.047619,"[8, 1, 8]",80,69134,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,20552,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565525721.625,25,7,kernel,12.190476,"[16, 4, 1]",44,69135,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20552,0,0.761905,1,"[32, 16, 1]"
25600,0,5.984,7,6939565525724.473,6,7,kernel,3.047619,"[64, 1, 1]",80,69157,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,20556,0,0.761905,1,"[128, 1, 1]"
16,0,7.84,7,6939565525731.225,0,7,kernel,0.047619,"[1, 1, 1]",48,69169,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20560,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565525739.769,5,7,kernel,2.380952,"[50, 1, 1]",22,69232,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,20602,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565525741.977,10,7,kernel,4.761905,"[25, 1, 1]",32,69246,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20603,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565525746.201,0,7,kernel,0.095238,"[2, 1, 1]",16,69259,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,20611,0,0.02381,1,"[128, 1, 1]"
53504,0,36.736,7,6939565525748.985,0,7,kernel,0.380952,"[1, 8, 1]",236,69273,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,20596,0,0.095238,1,"[128, 1, 1]"
0,0,1.12,7,6939565525786.617,15,7,kernel,7.142857,"[150, 1, 1]",16,69331,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20661,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565525790.617,15,7,kernel,7.142857,"[150, 1, 1]",16,69356,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20671,0,1.785714,1,"[128, 1, 1]"
0,0,1.728,7,6939565525795.129,15,7,kernel,7.142857,"[150, 1, 1]",22,69370,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20675,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565525797.593,15,7,kernel,7.142857,"[150, 1, 1]",16,69390,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20682,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565525801.785,15,7,kernel,7.142857,"[150, 1, 1]",22,69404,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20686,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565525804.281,30,7,kernel,14.285714,"[300, 1, 1]",16,69439,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,20707,0,3.571429,1,"[128, 1, 1]"
16384,0,12.224,7,6939565525807.257,38,7,kernel,18.285715,"[4, 2, 24]",57,69465,X,ampere_sgemm_128x32_nn,0,20718,0,2.285714,1,"[256, 1, 1]"
0,0,3.68,7,6939565525820.185,25,7,kernel,12.190476,"[16, 4, 1]",44,69467,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20718,0,0.761905,1,"[32, 16, 1]"
16384,0,11.744,7,6939565525824.601,38,7,kernel,18.285715,"[16, 12, 1]",57,69484,X,ampere_sgemm_32x128_nt,0,20722,0,2.285714,1,"[256, 1, 1]"
16,0,7.68,7,6939565525837.176,0,7,kernel,0.142857,"[3, 1, 1]",48,69497,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20726,0,0.035714,1,"[32, 4, 1]"
0,0,1.728,7,6939565525845.593,5,7,kernel,2.380952,"[50, 1, 1]",22,69520,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20745,0,0.595238,1,"[128, 1, 1]"
16,0,2.849,7,6939565525848.056,5,7,kernel,2.380952,"[50, 1, 1]",40,69555,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,20748,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565525851.673,0,7,kernel,0.190476,"[2, 1, 1]",38,69557,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,20748,0,0.02381,1,"[256, 1, 1]"
0,0,1.729,7,6939565525856.856,5,7,kernel,2.380952,"[50, 1, 1]",22,69586,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,20764,0,0.595238,1,"[128, 1, 1]"
16384,0,15.519,7,6939565525859.321,25,7,kernel,12.190476,"[16, 2, 4]",57,69612,X,ampere_sgemm_128x32_nn,0,20776,0,1.52381,1,"[256, 1, 1]"
0,0,2.944,7,6939565525875.577,67,7,kernel,48.761906,"[64, 4, 1]",44,69614,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20776,0,3.047619,1,"[32, 16, 1]"
16384,0,14.625,7,6939565525879.288,51,7,kernel,24.380953,"[16, 16, 1]",57,69631,X,ampere_sgemm_128x32_nt,0,20780,0,3.047619,1,"[256, 1, 1]"
16,0,5.792,7,6939565525894.712,0,7,kernel,0.047619,"[1, 1, 1]",48,69644,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20784,0,0.011905,1,"[32, 4, 1]"
0,0,2.816,7,6939565525901.336,20,7,kernel,9.523809,"[200, 1, 1]",22,69677,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,20805,0,2.380952,1,"[128, 1, 1]"
0,0,2.944,7,6939565525904.889,20,7,kernel,9.523809,"[200, 1, 1]",22,69691,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,20810,0,2.380952,1,"[128, 1, 1]"
16384,0,15.295,7,6939565525908.697,44,7,kernel,21.333334,"[4, 2, 28]",57,69716,X,ampere_sgemm_128x32_nn,0,20820,0,2.666667,1,"[256, 1, 1]"
0,0,3.616,7,6939565525924.824,25,7,kernel,12.190476,"[16, 4, 1]",44,69718,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20820,0,0.761905,1,"[32, 16, 1]"
16384,0,17.888,7,6939565528288.915,51,7,kernel,24.380953,"[16, 16, 1]",57,69735,X,ampere_sgemm_32x128_nt,0,20824,0,3.047619,1,"[256, 1, 1]"
16,0,7.68,7,6939565528307.507,0,7,kernel,0.190476,"[4, 1, 1]",48,69748,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20828,0,0.047619,1,"[32, 4, 1]"
0,0,2.112,7,6939565528315.923,5,7,kernel,2.380952,"[50, 1, 1]",22,69771,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,20847,0,0.595238,1,"[128, 1, 1]"
16,0,2.88,7,6939565528318.867,5,7,kernel,2.380952,"[50, 1, 1]",40,69806,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,20850,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565528322.451,0,7,kernel,0.190476,"[2, 1, 1]",38,69808,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,20850,0,0.02381,1,"[256, 1, 1]"
0,0,1.728,7,6939565528327.603,5,7,kernel,2.380952,"[50, 1, 1]",22,69837,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,20866,0,0.595238,1,"[128, 1, 1]"
25600,0,6.784,7,6939565528330.067,6,7,kernel,3.047619,"[8, 1, 8]",80,69867,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,20878,0,0.761905,1,"[128, 1, 1]"
0,0,2.048,7,6939565528337.555,25,7,kernel,12.190476,"[16, 4, 1]",44,69868,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,20878,0,0.761905,1,"[32, 16, 1]"
25600,0,6.144,7,6939565528340.371,6,7,kernel,3.047619,"[64, 1, 1]",80,69890,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,20882,0,0.761905,1,"[128, 1, 1]"
16,0,6.08,7,6939565528347.347,0,7,kernel,0.047619,"[1, 1, 1]",48,69902,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20886,0,0.011905,1,"[32, 4, 1]"
0,0,1.472,7,6939565528354.099,5,7,kernel,2.380952,"[50, 1, 1]",22,69965,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,20928,0,0.595238,1,"[128, 1, 1]"
16,0,3.232,7,6939565528356.339,10,7,kernel,4.761905,"[25, 1, 1]",32,69979,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,20929,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565528360.435,0,7,kernel,0.095238,"[2, 1, 1]",16,69992,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,20937,0,0.02381,1,"[128, 1, 1]"
53504,0,36.544,7,6939565528363.251,0,7,kernel,0.380952,"[1, 8, 1]",236,70006,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,20922,0,0.095238,1,"[128, 1, 1]"
0,0,1.12,7,6939565528400.819,15,7,kernel,7.142857,"[150, 1, 1]",16,70064,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20987,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565528404.915,15,7,kernel,7.142857,"[150, 1, 1]",16,70089,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,20997,0,1.785714,1,"[128, 1, 1]"
0,0,1.761,7,6939565528409.394,15,7,kernel,7.142857,"[150, 1, 1]",22,70103,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21001,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565528411.859,15,7,kernel,7.142857,"[150, 1, 1]",16,70123,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21008,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565528415.923,15,7,kernel,7.142857,"[150, 1, 1]",22,70137,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21012,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565528418.451,30,7,kernel,14.285714,"[300, 1, 1]",16,70172,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,21033,0,3.571429,1,"[128, 1, 1]"
16384,0,12.0,7,6939565528421.395,38,7,kernel,18.285715,"[4, 2, 24]",57,70198,X,ampere_sgemm_128x32_nn,0,21044,0,2.285714,1,"[256, 1, 1]"
0,0,3.584,7,6939565528434.099,25,7,kernel,12.190476,"[16, 4, 1]",44,70200,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21044,0,0.761905,1,"[32, 16, 1]"
16384,0,11.648,7,6939565528438.547,38,7,kernel,18.285715,"[16, 12, 1]",57,70217,X,ampere_sgemm_32x128_nt,0,21048,0,2.285714,1,"[256, 1, 1]"
16,0,7.681,7,6939565528451.058,0,7,kernel,0.142857,"[3, 1, 1]",48,70230,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21052,0,0.035714,1,"[32, 4, 1]"
0,0,1.728,7,6939565528459.507,5,7,kernel,2.380952,"[50, 1, 1]",22,70253,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21071,0,0.595238,1,"[128, 1, 1]"
16,0,2.913,7,6939565528461.97,5,7,kernel,2.380952,"[50, 1, 1]",40,70288,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,21074,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565528465.587,0,7,kernel,0.190476,"[2, 1, 1]",38,70290,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,21074,0,0.02381,1,"[256, 1, 1]"
0,0,1.632,7,6939565528470.802,5,7,kernel,2.380952,"[50, 1, 1]",22,70319,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21090,0,0.595238,1,"[128, 1, 1]"
16384,0,15.744,7,6939565528473.267,25,7,kernel,12.190476,"[16, 2, 4]",57,70345,X,ampere_sgemm_128x32_nn,0,21102,0,1.52381,1,"[256, 1, 1]"
0,0,3.168,7,6939565528489.81,67,7,kernel,48.761906,"[64, 4, 1]",44,70347,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21102,0,3.047619,1,"[32, 16, 1]"
16384,0,14.239,7,6939565528493.715,51,7,kernel,24.380953,"[16, 16, 1]",57,70364,X,ampere_sgemm_128x32_nt,0,21106,0,3.047619,1,"[256, 1, 1]"
16,0,5.695,7,6939565528508.691,0,7,kernel,0.047619,"[1, 1, 1]",48,70377,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21110,0,0.011905,1,"[32, 4, 1]"
0,0,3.104,7,6939565530805.453,20,7,kernel,9.523809,"[200, 1, 1]",22,70410,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21131,0,2.380952,1,"[128, 1, 1]"
0,0,2.88,7,6939565530809.325,20,7,kernel,9.523809,"[200, 1, 1]",22,70424,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,21136,0,2.380952,1,"[128, 1, 1]"
16384,0,16.352,7,6939565530812.909,44,7,kernel,21.333334,"[4, 2, 28]",57,70449,X,ampere_sgemm_128x32_nn,0,21146,0,2.666667,1,"[256, 1, 1]"
0,0,3.68,7,6939565530830.061,25,7,kernel,12.190476,"[16, 4, 1]",44,70451,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21146,0,0.761905,1,"[32, 16, 1]"
16384,0,13.984,7,6939565530834.509,51,7,kernel,24.380953,"[16, 16, 1]",57,70468,X,ampere_sgemm_32x128_nt,0,21150,0,3.047619,1,"[256, 1, 1]"
16,0,7.52,7,6939565530849.293,0,7,kernel,0.190476,"[4, 1, 1]",48,70481,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21154,0,0.047619,1,"[32, 4, 1]"
0,0,1.888,7,6939565530857.549,5,7,kernel,2.380952,"[50, 1, 1]",22,70504,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21173,0,0.595238,1,"[128, 1, 1]"
16,0,2.88,7,6939565530860.141,5,7,kernel,2.380952,"[50, 1, 1]",40,70539,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,21176,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565530863.757,0,7,kernel,0.190476,"[2, 1, 1]",38,70541,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,21176,0,0.02381,1,"[256, 1, 1]"
0,0,1.664,7,6939565530868.941,5,7,kernel,2.380952,"[50, 1, 1]",22,70570,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21192,0,0.595238,1,"[128, 1, 1]"
25600,0,6.88,7,6939565530871.405,6,7,kernel,3.047619,"[8, 1, 8]",80,70600,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,21204,0,0.761905,1,"[128, 1, 1]"
0,0,1.952,7,6939565530879.053,25,7,kernel,12.190476,"[16, 4, 1]",44,70601,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21204,0,0.761905,1,"[32, 16, 1]"
25600,0,6.08,7,6939565530881.709,6,7,kernel,3.047619,"[64, 1, 1]",80,70623,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,21208,0,0.761905,1,"[128, 1, 1]"
16,0,6.56,7,6939565530888.621,0,7,kernel,0.047619,"[1, 1, 1]",48,70635,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21212,0,0.011905,1,"[32, 4, 1]"
0,0,1.408,7,6939565530895.917,5,7,kernel,2.380952,"[50, 1, 1]",22,70698,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,21254,0,0.595238,1,"[128, 1, 1]"
16,0,3.36,7,6939565530898.061,10,7,kernel,4.761905,"[25, 1, 1]",32,70712,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21255,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565530902.157,0,7,kernel,0.095238,"[2, 1, 1]",16,70725,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,21263,0,0.02381,1,"[128, 1, 1]"
53504,0,36.448,7,6939565530904.973,0,7,kernel,0.380952,"[1, 8, 1]",236,70739,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,21248,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565530942.573,15,7,kernel,7.142857,"[150, 1, 1]",16,70797,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21313,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565530946.381,15,7,kernel,7.142857,"[150, 1, 1]",16,70822,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21323,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565530950.861,15,7,kernel,7.142857,"[150, 1, 1]",22,70836,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21327,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565530953.325,15,7,kernel,7.142857,"[150, 1, 1]",16,70856,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21334,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565530957.453,15,7,kernel,7.142857,"[150, 1, 1]",22,70870,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21338,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565530959.917,30,7,kernel,14.285714,"[300, 1, 1]",16,70905,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,21359,0,3.571429,1,"[128, 1, 1]"
16384,0,12.48,7,6939565530962.893,38,7,kernel,18.285715,"[4, 2, 24]",57,70931,X,ampere_sgemm_128x32_nn,0,21370,0,2.285714,1,"[256, 1, 1]"
0,0,3.776,7,6939565530976.205,25,7,kernel,12.190476,"[16, 4, 1]",44,70933,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21370,0,0.761905,1,"[32, 16, 1]"
16384,0,11.2,7,6939565530980.781,38,7,kernel,18.285715,"[16, 12, 1]",57,70950,X,ampere_sgemm_32x128_nt,0,21374,0,2.285714,1,"[256, 1, 1]"
16,0,7.68,7,6939565530992.685,0,7,kernel,0.142857,"[3, 1, 1]",48,70963,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21378,0,0.035714,1,"[32, 4, 1]"
0,0,1.76,7,6939565531001.133,5,7,kernel,2.380952,"[50, 1, 1]",22,70986,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21397,0,0.595238,1,"[128, 1, 1]"
16,0,2.848,7,6939565531003.597,5,7,kernel,2.380952,"[50, 1, 1]",40,71021,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,21400,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565531007.213,0,7,kernel,0.190476,"[2, 1, 1]",38,71023,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,21400,0,0.02381,1,"[256, 1, 1]"
0,0,1.759,7,6939565531012.365,5,7,kernel,2.380952,"[50, 1, 1]",22,71052,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21416,0,0.595238,1,"[128, 1, 1]"
16384,0,17.696,7,6939565533367.719,25,7,kernel,12.190476,"[16, 2, 4]",57,71078,X,ampere_sgemm_128x32_nn,0,21428,0,1.52381,1,"[256, 1, 1]"
0,0,3.136,7,6939565533386.279,67,7,kernel,48.761906,"[64, 4, 1]",44,71080,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21428,0,3.047619,1,"[32, 16, 1]"
16384,0,14.208,7,6939565533390.215,51,7,kernel,24.380953,"[16, 16, 1]",57,71097,X,ampere_sgemm_128x32_nt,0,21432,0,3.047619,1,"[256, 1, 1]"
16,0,7.808,7,6939565533405.191,0,7,kernel,0.047619,"[1, 1, 1]",48,71110,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21436,0,0.011905,1,"[32, 4, 1]"
0,0,2.368,7,6939565533413.767,20,7,kernel,9.523809,"[200, 1, 1]",22,71143,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21457,0,2.380952,1,"[128, 1, 1]"
0,0,2.912,7,6939565533416.871,20,7,kernel,9.523809,"[200, 1, 1]",22,71157,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,21462,0,2.380952,1,"[128, 1, 1]"
16384,0,15.232,7,6939565533420.647,44,7,kernel,21.333334,"[4, 2, 28]",57,71182,X,ampere_sgemm_128x32_nn,0,21472,0,2.666667,1,"[256, 1, 1]"
0,0,3.456,7,6939565533436.711,25,7,kernel,12.190476,"[16, 4, 1]",44,71184,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21472,0,0.761905,1,"[32, 16, 1]"
16384,0,14.176,7,6939565533440.999,51,7,kernel,24.380953,"[16, 16, 1]",57,71201,X,ampere_sgemm_32x128_nt,0,21476,0,3.047619,1,"[256, 1, 1]"
16,0,6.912,7,6939565533456.007,0,7,kernel,0.190476,"[4, 1, 1]",48,71214,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21480,0,0.047619,1,"[32, 4, 1]"
0,0,1.888,7,6939565533463.783,5,7,kernel,2.380952,"[50, 1, 1]",22,71237,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21499,0,0.595238,1,"[128, 1, 1]"
16,0,3.008,7,6939565533466.439,5,7,kernel,2.380952,"[50, 1, 1]",40,71272,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,21502,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565533470.215,0,7,kernel,0.190476,"[2, 1, 1]",38,71274,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,21502,0,0.02381,1,"[256, 1, 1]"
0,0,1.536,7,6939565533475.431,5,7,kernel,2.380952,"[50, 1, 1]",22,71303,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21518,0,0.595238,1,"[128, 1, 1]"
25600,0,7.2,7,6939565533477.735,6,7,kernel,3.047619,"[8, 1, 8]",80,71333,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,21530,0,0.761905,1,"[128, 1, 1]"
0,0,1.92,7,6939565533485.703,25,7,kernel,12.190476,"[16, 4, 1]",44,71334,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21530,0,0.761905,1,"[32, 16, 1]"
25600,0,6.112,7,6939565533488.359,6,7,kernel,3.047619,"[64, 1, 1]",80,71356,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,21534,0,0.761905,1,"[128, 1, 1]"
16,0,6.784,7,6939565533495.495,0,7,kernel,0.047619,"[1, 1, 1]",48,71368,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21538,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565533503.111,5,7,kernel,2.380952,"[50, 1, 1]",22,71431,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,21580,0,0.595238,1,"[128, 1, 1]"
16,0,3.328,7,6939565533505.319,10,7,kernel,4.761905,"[25, 1, 1]",32,71445,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21581,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565533509.415,0,7,kernel,0.095238,"[2, 1, 1]",16,71458,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,21589,0,0.02381,1,"[128, 1, 1]"
53504,0,36.352,7,6939565533512.231,0,7,kernel,0.380952,"[1, 8, 1]",236,71472,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,21574,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565533549.703,15,7,kernel,7.142857,"[150, 1, 1]",16,71530,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21639,0,1.785714,1,"[128, 1, 1]"
0,0,1.248,7,6939565533553.607,15,7,kernel,7.142857,"[150, 1, 1]",16,71555,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21649,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565533557.735,15,7,kernel,7.142857,"[150, 1, 1]",22,71569,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21653,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565533560.167,15,7,kernel,7.142857,"[150, 1, 1]",16,71589,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21660,0,1.785714,1,"[128, 1, 1]"
0,0,1.664,7,6939565533564.263,15,7,kernel,7.142857,"[150, 1, 1]",22,71603,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21664,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565533566.631,30,7,kernel,14.285714,"[300, 1, 1]",16,71638,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,21685,0,3.571429,1,"[128, 1, 1]"
16384,0,12.864,7,6939565533569.607,38,7,kernel,18.285715,"[4, 2, 24]",57,71664,X,ampere_sgemm_128x32_nn,0,21696,0,2.285714,1,"[256, 1, 1]"
0,0,3.999,7,6939565533583.207,25,7,kernel,12.190476,"[16, 4, 1]",44,71666,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21696,0,0.761905,1,"[32, 16, 1]"
16384,0,11.36,7,6939565533587.943,38,7,kernel,18.285715,"[16, 12, 1]",57,71683,X,ampere_sgemm_32x128_nt,0,21700,0,2.285714,1,"[256, 1, 1]"
16,0,7.808,7,6939565533600.103,0,7,kernel,0.142857,"[3, 1, 1]",48,71696,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21704,0,0.035714,1,"[32, 4, 1]"
0,0,1.76,7,6939565533608.679,5,7,kernel,2.380952,"[50, 1, 1]",22,71719,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21723,0,0.595238,1,"[128, 1, 1]"
16,0,3.104,7,6939565535983.649,5,7,kernel,2.380952,"[50, 1, 1]",40,71754,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,21726,0,0.595238,1,"[128, 1, 1]"
0,0,4.8,7,6939565535987.553,0,7,kernel,0.190476,"[2, 1, 1]",38,71756,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,21726,0,0.02381,1,"[256, 1, 1]"
0,0,1.856,7,6939565535993.217,5,7,kernel,2.380952,"[50, 1, 1]",22,71785,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21742,0,0.595238,1,"[128, 1, 1]"
16384,0,17.056,7,6939565535995.777,25,7,kernel,12.190476,"[16, 2, 4]",57,71811,X,ampere_sgemm_128x32_nn,0,21754,0,1.52381,1,"[256, 1, 1]"
0,0,3.168,7,6939565536013.665,67,7,kernel,48.761906,"[64, 4, 1]",44,71813,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21754,0,3.047619,1,"[32, 16, 1]"
16384,0,13.632,7,6939565536017.569,51,7,kernel,24.380953,"[16, 16, 1]",57,71830,X,ampere_sgemm_128x32_nt,0,21758,0,3.047619,1,"[256, 1, 1]"
16,0,7.808,7,6939565536032.033,0,7,kernel,0.047619,"[1, 1, 1]",48,71843,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21762,0,0.011905,1,"[32, 4, 1]"
0,0,2.464,7,6939565536040.609,20,7,kernel,9.523809,"[200, 1, 1]",22,71876,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21783,0,2.380952,1,"[128, 1, 1]"
0,0,2.88,7,6939565536043.873,20,7,kernel,9.523809,"[200, 1, 1]",22,71890,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,21788,0,2.380952,1,"[128, 1, 1]"
16384,0,15.648,7,6939565536047.489,44,7,kernel,21.333334,"[4, 2, 28]",57,71915,X,ampere_sgemm_128x32_nn,0,21798,0,2.666667,1,"[256, 1, 1]"
0,0,3.776,7,6939565536063.905,25,7,kernel,12.190476,"[16, 4, 1]",44,71917,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21798,0,0.761905,1,"[32, 16, 1]"
16384,0,13.952,7,6939565536068.513,51,7,kernel,24.380953,"[16, 16, 1]",57,71934,X,ampere_sgemm_32x128_nt,0,21802,0,3.047619,1,"[256, 1, 1]"
16,0,6.944,7,6939565536083.201,0,7,kernel,0.190476,"[4, 1, 1]",48,71947,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21806,0,0.047619,1,"[32, 4, 1]"
0,0,1.76,7,6939565536090.945,5,7,kernel,2.380952,"[50, 1, 1]",22,71970,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21825,0,0.595238,1,"[128, 1, 1]"
16,0,3.04,7,6939565536093.537,5,7,kernel,2.380952,"[50, 1, 1]",40,72005,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,21828,0,0.595238,1,"[128, 1, 1]"
0,0,4.384,7,6939565536097.313,0,7,kernel,0.190476,"[2, 1, 1]",38,72007,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,21828,0,0.02381,1,"[256, 1, 1]"
0,0,1.536,7,6939565536102.529,5,7,kernel,2.380952,"[50, 1, 1]",22,72036,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,21844,0,0.595238,1,"[128, 1, 1]"
25600,0,7.04,7,6939565536104.801,6,7,kernel,3.047619,"[8, 1, 8]",80,72066,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,21856,0,0.761905,1,"[128, 1, 1]"
0,0,1.92,7,6939565536112.609,25,7,kernel,12.190476,"[16, 4, 1]",44,72067,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,21856,0,0.761905,1,"[32, 16, 1]"
25600,0,6.016,7,6939565536115.233,6,7,kernel,3.047619,"[64, 1, 1]",80,72089,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,21860,0,0.761905,1,"[128, 1, 1]"
16,0,6.56,7,6939565536122.017,0,7,kernel,0.047619,"[1, 1, 1]",48,72101,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21864,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565536129.281,5,7,kernel,2.380952,"[50, 1, 1]",22,72164,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,21906,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565536131.489,10,7,kernel,4.761905,"[25, 1, 1]",32,72178,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,21907,0,0.297619,1,"[32, 16, 1]"
0,0,1.984,7,6939565536135.649,0,7,kernel,0.095238,"[2, 1, 1]",16,72191,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,21915,0,0.02381,1,"[128, 1, 1]"
53504,0,36.352,7,6939565536138.465,0,7,kernel,0.380952,"[1, 8, 1]",236,72205,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,21900,0,0.095238,1,"[128, 1, 1]"
0,0,0.96,7,6939565536175.841,15,7,kernel,7.142857,"[150, 1, 1]",16,72263,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21965,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565536179.905,15,7,kernel,7.142857,"[150, 1, 1]",16,72288,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21975,0,1.785714,1,"[128, 1, 1]"
0,0,1.663,7,6939565536184.065,15,7,kernel,7.142857,"[150, 1, 1]",22,72302,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21979,0,1.785714,1,"[128, 1, 1]"
0,0,1.184,7,6939565536186.529,15,7,kernel,7.142857,"[150, 1, 1]",16,72322,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,21986,0,1.785714,1,"[128, 1, 1]"
0,0,1.632,7,6939565536190.625,15,7,kernel,7.142857,"[150, 1, 1]",22,72336,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,21990,0,1.785714,1,"[128, 1, 1]"
0,0,2.24,7,6939565536192.961,30,7,kernel,14.285714,"[300, 1, 1]",16,72371,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,22011,0,3.571429,1,"[128, 1, 1]"
16384,0,11.968,7,6939565536195.937,38,7,kernel,18.285715,"[4, 2, 24]",57,72397,X,ampere_sgemm_128x32_nn,0,22022,0,2.285714,1,"[256, 1, 1]"
0,0,3.617,7,6939565536208.608,25,7,kernel,12.190476,"[16, 4, 1]",44,72399,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,22022,0,0.761905,1,"[32, 16, 1]"
16384,0,11.807,7,6939565536213.057,38,7,kernel,18.285715,"[16, 12, 1]",57,72416,X,ampere_sgemm_32x128_nt,0,22026,0,2.285714,1,"[256, 1, 1]"
16,0,7.808,7,6939565536225.665,0,7,kernel,0.142857,"[3, 1, 1]",48,72429,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,22030,0,0.035714,1,"[32, 4, 1]"
0,0,2.144,7,6939565538558.331,5,7,kernel,2.380952,"[50, 1, 1]",22,72452,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,22049,0,0.595238,1,"[128, 1, 1]"
16,0,3.008,7,6939565538561.275,5,7,kernel,2.380952,"[50, 1, 1]",40,72487,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,22052,0,0.595238,1,"[128, 1, 1]"
0,0,4.416,7,6939565538565.019,0,7,kernel,0.190476,"[2, 1, 1]",38,72489,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,22052,0,0.02381,1,"[256, 1, 1]"
0,0,1.76,7,6939565538570.235,5,7,kernel,2.380952,"[50, 1, 1]",22,72518,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,22068,0,0.595238,1,"[128, 1, 1]"
16384,0,17.12,7,6939565538572.699,25,7,kernel,12.190476,"[16, 2, 4]",57,72544,X,ampere_sgemm_128x32_nn,0,22080,0,1.52381,1,"[256, 1, 1]"
0,0,2.816,7,6939565538590.555,67,7,kernel,48.761906,"[64, 4, 1]",44,72546,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,22080,0,3.047619,1,"[32, 16, 1]"
16384,0,14.24,7,6939565538594.363,51,7,kernel,24.380953,"[16, 16, 1]",57,72563,X,ampere_sgemm_128x32_nt,0,22084,0,3.047619,1,"[256, 1, 1]"
16,0,7.84,7,6939565538609.403,0,7,kernel,0.047619,"[1, 1, 1]",48,72576,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,22088,0,0.011905,1,"[32, 4, 1]"
0,0,2.24,7,6939565538617.979,20,7,kernel,9.523809,"[200, 1, 1]",22,72609,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,22109,0,2.380952,1,"[128, 1, 1]"
0,0,2.784,7,6939565538620.955,20,7,kernel,9.523809,"[200, 1, 1]",22,72623,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)",0,22114,0,2.380952,1,"[128, 1, 1]"
16384,0,15.264,7,6939565538624.539,44,7,kernel,21.333334,"[4, 2, 28]",57,72648,X,ampere_sgemm_128x32_nn,0,22124,0,2.666667,1,"[256, 1, 1]"
0,0,3.392,7,6939565538640.603,25,7,kernel,12.190476,"[16, 4, 1]",44,72650,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,22124,0,0.761905,1,"[32, 16, 1]"
16384,0,14.304,7,6939565538644.795,51,7,kernel,24.380953,"[16, 16, 1]",57,72667,X,ampere_sgemm_32x128_nt,0,22128,0,3.047619,1,"[256, 1, 1]"
16,0,6.88,7,6939565538659.835,0,7,kernel,0.190476,"[4, 1, 1]",48,72680,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,22132,0,0.047619,1,"[32, 4, 1]"
0,0,1.856,7,6939565538667.451,5,7,kernel,2.380952,"[50, 1, 1]",22,72703,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,22151,0,0.595238,1,"[128, 1, 1]"
16,0,2.752,7,6939565538670.043,5,7,kernel,2.380952,"[50, 1, 1]",40,72738,X,"void at::native::(anonymous namespace)::layer_norm_grad_input_kernel_vectorized<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int)",0,22154,0,0.595238,1,"[128, 1, 1]"
0,0,4.448,7,6939565538673.627,0,7,kernel,0.190476,"[2, 1, 1]",38,72740,X,"void at::native::(anonymous namespace)::GammaBetaBackwardSimpleCUDAKernel<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*)",0,22154,0,0.02381,1,"[256, 1, 1]"
0,0,1.6,7,6939565538678.875,5,7,kernel,2.380952,"[50, 1, 1]",22,72769,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,22170,0,0.595238,1,"[128, 1, 1]"
25600,0,7.008,7,6939565538681.179,6,7,kernel,3.047619,"[8, 1, 8]",80,72799,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nn_align1>(cutlass_80_simt_sgemm_128x32_8x5_nn_align1::Params),0,22182,0,0.761905,1,"[128, 1, 1]"
0,0,1.92,7,6939565538689.019,25,7,kernel,12.190476,"[16, 4, 1]",44,72800,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,22182,0,0.761905,1,"[32, 16, 1]"
25600,0,6.048,7,6939565538691.675,6,7,kernel,3.047619,"[64, 1, 1]",80,72822,X,void cutlass::Kernel2<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params),0,22186,0,0.761905,1,"[128, 1, 1]"
16,0,6.56,7,6939565538698.459,0,7,kernel,0.047619,"[1, 1, 1]",48,72834,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,22190,0,0.011905,1,"[32, 4, 1]"
0,0,1.44,7,6939565538705.819,5,7,kernel,2.380952,"[50, 1, 1]",22,72897,X,"void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>)",0,22232,0,0.595238,1,"[128, 1, 1]"
16,0,3.392,7,6939565538708.251,10,7,kernel,4.761905,"[25, 1, 1]",32,72911,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,22233,0,0.297619,1,"[32, 16, 1]"
0,0,2.016,7,6939565538712.379,0,7,kernel,0.095238,"[2, 1, 1]",16,72924,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,22241,0,0.02381,1,"[128, 1, 1]"
53504,0,36.544,7,6939565538715.163,0,7,kernel,0.380952,"[1, 8, 1]",236,72938,X,"fmha_cutlassB_f32_aligned_64x64_k64_dropout_sm80(PyTorchMemEffAttention::AttentionBackwardKernel<cutlass::arch::Sm80, float, true, true, false, 64, 64, 64, false>::Params)",0,22226,0,0.095238,1,"[128, 1, 1]"
0,0,0.961,7,6939565538752.826,15,7,kernel,7.142857,"[150, 1, 1]",16,72996,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22291,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565538756.795,15,7,kernel,7.142857,"[150, 1, 1]",16,73021,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22301,0,1.785714,1,"[128, 1, 1]"
0,0,1.696,7,6939565538761.115,15,7,kernel,7.142857,"[150, 1, 1]",22,73035,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,22305,0,1.785714,1,"[128, 1, 1]"
0,0,1.216,7,6939565538763.579,15,7,kernel,7.142857,"[150, 1, 1]",16,73055,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22312,0,1.785714,1,"[128, 1, 1]"
0,0,1.599,7,6939565538767.707,15,7,kernel,7.142857,"[150, 1, 1]",22,73069,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,22316,0,1.785714,1,"[128, 1, 1]"
0,0,3.136,7,6939565541081.109,30,7,kernel,14.285714,"[300, 1, 1]",16,73104,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,22337,0,3.571429,1,"[128, 1, 1]"
16384,0,13.217,7,6939565541085.045,38,7,kernel,18.285715,"[4, 2, 24]",57,73130,X,ampere_sgemm_128x32_nn,0,22348,0,2.285714,1,"[256, 1, 1]"
0,0,2.912,7,6939565541099.029,25,7,kernel,12.190476,"[16, 4, 1]",44,73132,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,22348,0,0.761905,1,"[32, 16, 1]"
16384,0,12.48,7,6939565541102.709,38,7,kernel,18.285715,"[16, 12, 1]",57,73149,X,ampere_sgemm_32x128_nt,0,22352,0,2.285714,1,"[256, 1, 1]"
16,0,7.744,7,6939565541115.893,0,7,kernel,0.142857,"[3, 1, 1]",48,73162,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,22356,0,0.035714,1,"[32, 4, 1]"
0,0,1.76,7,6939565541124.501,5,7,kernel,2.380952,"[50, 1, 1]",22,73185,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,22375,0,0.595238,1,"[128, 1, 1]"
0,0,1.184,7,6939565541127.093,5,7,kernel,2.380952,"[50, 1, 1]",16,73217,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22392,0,0.595238,1,"[128, 1, 1]"
0,0,1.568,7,6939565541131.573,51,7,kernel,24.380953,"[512, 1, 1]",16,73242,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22402,0,6.095238,1,"[128, 1, 1]"
0,0,1.92,7,6939565541135.989,51,7,kernel,24.380953,"[512, 1, 1]",16,73267,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22412,0,6.095238,1,"[128, 1, 1]"
0,0,35.392,7,6939565541141.525,100,7,kernel,476.190491,"[10000, 1, 1]",16,73294,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22424,0,119.047623,1,"[128, 1, 1]"
8192,0,4.704,7,6939565541177.653,13,7,kernel,6.095238,"[16, 1, 1]",24,73301,X,"void at::native::(anonymous namespace)::embedding_backward_feature_kernel<float, float, long>(long const*, float const*, float*, int, long, int)",0,22419,0,0.190476,1,"[32, 32, 1]"
0,0,1.44,7,6939565541183.189,5,7,kernel,2.380952,"[50, 1, 1]",16,73327,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22437,0,0.595238,1,"[128, 1, 1]"
0,0,1.6,7,6939565541187.829,51,7,kernel,24.380953,"[512, 1, 1]",16,73352,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22447,0,6.095238,1,"[128, 1, 1]"
0,0,2.08,7,6939565541192.373,51,7,kernel,24.380953,"[512, 1, 1]",16,73377,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22457,0,6.095238,1,"[128, 1, 1]"
0,0,4.032,7,6939565541198.101,51,7,kernel,24.380953,"[512, 1, 1]",22,73391,X,"void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>)",0,22461,0,6.095238,1,"[128, 1, 1]"
0,0,35.52,7,6939565541202.933,100,7,kernel,476.190491,"[10000, 1, 1]",16,73417,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,22474,0,119.047623,1,"[128, 1, 1]"
8192,0,4.992,7,6939565541239.285,13,7,kernel,6.095238,"[16, 1, 1]",24,73424,X,"void at::native::(anonymous namespace)::embedding_backward_feature_kernel<float, float, long>(long const*, float const*, float*, int, long, int)",0,22469,0,0.190476,1,"[32, 32, 1]"
