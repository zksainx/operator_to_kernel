shared memory,pid,dur,stream,ts,est. achieved occupancy %,tid,cat,warps per SM,grid,registers per thread,correlation,ph,name,device,External id,queued,blocks per SM,context,block
0,0,1.472,7,6940680935972.229,0,7,kernel,0.047619,"[1, 1, 1]",16,9953,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,2054,0,0.011905,1,"[128, 1, 1]"
0,0,2.208,7,6940680935974.468,0,7,kernel,0.190476,"[4, 1, 1]",16,9979,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,2574,0,0.047619,1,"[128, 1, 1]"
2304,0,33.665,7,6940680935977.476,25,7,kernel,12.190476,"[64, 1, 1]",128,9992,X,"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, false, false, 9, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",0,2570,0,0.761905,1,"[64, 8, 1]"
0,0,1.6,7,6940680936011.909,0,7,kernel,0.190476,"[4, 1, 1]",16,10010,X,"void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})",0,2582,0,0.047619,1,"[128, 1, 1]"
1024,0,27.712,7,6940680936014.277,100,7,kernel,390.095245,"[128, 32, 1]",20,10023,X,"void gemmk1_kernel<int, float, 256, 5, false, false, false, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, 0>(cublasGemmk1Params<float, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, biasType<cublasGemvTensorStridedBatched<float>::value_type, float>::type>)",0,2578,0,48.761906,1,"[256, 1, 1]"
16,0,4.384,7,6940680936042.789,1,7,kernel,0.380952,"[2, 1, 1]",32,10036,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2586,0,0.02381,1,"[512, 1, 1]"
0,0,1.984,7,6940680936047.941,1,7,kernel,0.380952,"[8, 1, 1]",22,10065,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,2603,0,0.095238,1,"[128, 1, 1]"
2304,0,126.977,7,6940680936050.661,33,7,kernel,48.761906,"[64, 1, 4]",128,10086,X,"std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, false, false, 9, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)",0,2611,0,3.047619,1,"[64, 8, 1]"
0,0,2.72,7,6940680936178.342,67,7,kernel,48.761906,"[1, 256, 1]",44,10088,X,"void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)",0,2611,0,3.047619,1,"[32, 16, 1]"
16384,0,116.385,7,6940680936181.894,67,7,kernel,390.095245,"[32, 128, 1]",57,10105,X,ampere_sgemm_128x32_nn,0,2615,0,48.761906,1,"[256, 1, 1]"
16,0,4.961,7,6940680936299.046,1,7,kernel,0.380952,"[8, 1, 1]",48,10118,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2619,0,0.095238,1,"[128, 1, 1]"
0,0,1.504,7,6940680936304.871,1,7,kernel,0.380952,"[8, 1, 1]",22,10147,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)",0,2636,0,0.095238,1,"[128, 1, 1]"
16384,0,695.589,7,6940680938581.493,67,7,kernel,93.333336,"[196, 1, 5]",57,10169,X,ampere_sgemm_128x32_nn,0,2644,0,11.666667,1,"[256, 1, 1]"
16384,0,722.405,7,6940680939277.785,67,7,kernel,2389.333252,"[196, 128, 1]",57,10186,X,ampere_sgemm_128x32_nn,0,2648,0,298.666656,1,"[256, 1, 1]"
16,0,4.992,7,6940680940000.99,1,7,kernel,0.380952,"[8, 1, 1]",48,10199,X,"void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2652,0,0.095238,1,"[128, 1, 1]"
0,0,1.344,7,6940680940006.782,19,7,kernel,9.333333,"[196, 1, 1]",16,10231,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,2675,0,2.333333,1,"[128, 1, 1]"
0,0,5.152,7,6940680940008.862,100,7,kernel,48.761906,"[1, 1, 512]",40,10240,X,"void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)",0,2673,0,6.095238,1,"[256, 1, 1]"
0,0,2.336,7,6940680940014.75,19,7,kernel,9.333333,"[196, 1, 1]",23,10257,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2678,0,2.333333,1,"[128, 1, 1]"
4368,0,10.848,7,6940680940017.79,67,7,kernel,97.523811,"[512, 1, 1]",45,10301,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2682,0,6.095238,1,"[512, 1, 1]"
4224,0,33.248,7,6940680940029.374,100,7,kernel,780.190491,"[1, 16, 512]",38,10332,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2698,0,97.523811,1,"[256, 1, 1]"
4224,0,4.0,7,6940680940063.838,22,7,kernel,10.666667,"[7, 16, 1]",38,10334,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2698,0,1.333333,1,"[256, 1, 1]"
65536,0,44.161,7,6940680940071.134,0,7,kernel,3.047619,"[4, 2, 8]",250,10340,X,sm80_xmma_dgrad_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,2698,0,0.761905,1,"[128, 1, 1]"
4224,0,2.625,7,6940680940116.126,22,7,kernel,10.666667,"[7, 16, 1]",40,10343,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2698,0,1.333333,1,"[256, 1, 1]"
4224,0,3.361,7,6940680940119.55,22,7,kernel,10.666667,"[7, 16, 1]",38,10363,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2698,0,1.333333,1,"[256, 1, 1]"
4224,0,2.912,7,6940680940123.615,22,7,kernel,10.666667,"[7, 16, 1]",38,10365,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2698,0,1.333333,1,"[256, 1, 1]"
73728,0,37.504,7,6940680940127.263,0,7,kernel,6.857143,"[8, 9, 1]",229,10369,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4::Params),0,2698,0,0.857143,1,"[256, 1, 1]"
4224,0,33.472,7,6940680940165.599,100,7,kernel,780.190491,"[1, 16, 512]",40,10373,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2698,0,97.523811,1,"[256, 1, 1]"
16,0,5.216,7,6940680940199.871,13,7,kernel,6.095238,"[32, 1, 1]",32,10387,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2701,0,0.380952,1,"[32, 16, 1]"
0,0,3.584,7,6940680940205.919,19,7,kernel,9.333333,"[196, 1, 1]",23,10414,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2713,0,2.333333,1,"[128, 1, 1]"
4368,0,10.912,7,6940680940210.239,67,7,kernel,97.523811,"[512, 1, 1]",45,10458,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2717,0,6.095238,1,"[512, 1, 1]"
4224,0,32.992,7,6940680940221.951,100,7,kernel,780.190491,"[1, 16, 512]",38,10489,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2733,0,97.523811,1,"[256, 1, 1]"
4224,0,3.967,7,6940680940255.616,22,7,kernel,10.666667,"[7, 16, 1]",38,10491,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2733,0,1.333333,1,"[256, 1, 1]"
65536,0,44.576,7,6940680940263.104,0,7,kernel,3.047619,"[4, 2, 8]",250,10497,X,sm80_xmma_dgrad_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,2733,0,0.761905,1,"[128, 1, 1]"
4224,0,2.496,7,6940680940308.48,22,7,kernel,10.666667,"[7, 16, 1]",40,10500,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2733,0,1.333333,1,"[256, 1, 1]"
4224,0,3.392,7,6940680940311.776,22,7,kernel,10.666667,"[7, 16, 1]",38,10520,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2733,0,1.333333,1,"[256, 1, 1]"
4224,0,2.88,7,6940680940315.872,22,7,kernel,10.666667,"[7, 16, 1]",38,10522,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2733,0,1.333333,1,"[256, 1, 1]"
73728,0,37.376,7,6940680940319.52,0,7,kernel,6.857143,"[8, 9, 1]",229,10526,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4::Params),0,2733,0,0.857143,1,"[256, 1, 1]"
4224,0,33.12,7,6940680940357.632,100,7,kernel,780.190491,"[1, 16, 512]",40,10530,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2733,0,97.523811,1,"[256, 1, 1]"
16,0,5.473,7,6940680940391.552,13,7,kernel,6.095238,"[32, 1, 1]",32,10544,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2736,0,0.380952,1,"[32, 16, 1]"
0,0,3.391,7,6940680940397.825,19,7,kernel,9.333333,"[196, 1, 1]",23,10571,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2748,0,2.333333,1,"[128, 1, 1]"
4368,0,10.72,7,6940680940401.889,67,7,kernel,97.523811,"[512, 1, 1]",45,10615,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2752,0,6.095238,1,"[512, 1, 1]"
4224,0,33.248,7,6940680940413.441,100,7,kernel,780.190491,"[1, 16, 512]",38,10646,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2768,0,97.523811,1,"[256, 1, 1]"
4224,0,3.968,7,6940680940447.457,22,7,kernel,10.666667,"[7, 16, 1]",38,10648,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2768,0,1.333333,1,"[256, 1, 1]"
65536,0,44.128,7,6940680940454.273,0,7,kernel,3.047619,"[4, 2, 8]",250,10654,X,sm80_xmma_dgrad_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,2768,0,0.761905,1,"[128, 1, 1]"
4224,0,2.624,7,6940680940499.137,22,7,kernel,10.666667,"[7, 16, 1]",40,10657,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2768,0,1.333333,1,"[256, 1, 1]"
4224,0,3.168,7,6940680942772.687,22,7,kernel,10.666667,"[7, 16, 1]",38,10677,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2768,0,1.333333,1,"[256, 1, 1]"
4224,0,3.169,7,6940680942776.527,22,7,kernel,10.666667,"[7, 16, 1]",38,10679,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2768,0,1.333333,1,"[256, 1, 1]"
73728,0,37.601,7,6940680942780.431,0,7,kernel,6.857143,"[8, 9, 1]",229,10683,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4::Params),0,2768,0,0.857143,1,"[256, 1, 1]"
4224,0,33.12,7,6940680942818.736,100,7,kernel,780.190491,"[1, 16, 512]",40,10687,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2768,0,97.523811,1,"[256, 1, 1]"
16,0,5.376,7,6940680942852.56,13,7,kernel,6.095238,"[32, 1, 1]",32,10701,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2771,0,0.380952,1,"[32, 16, 1]"
0,0,2.24,7,6940680942858.704,78,7,kernel,37.333332,"[784, 1, 1]",16,10727,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,2785,0,9.333333,1,"[128, 1, 1]"
0,0,11.68,7,6940680942861.68,100,7,kernel,195.047623,"[4, 1, 512]",40,10736,X,"void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)",0,2783,0,24.380953,1,"[256, 1, 1]"
0,0,5.984,7,6940680942874.096,78,7,kernel,37.333332,"[784, 1, 1]",23,10753,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2788,0,9.333333,1,"[128, 1, 1]"
8464,0,13.664,7,6940680942880.784,67,7,kernel,97.523811,"[512, 1, 1]",45,10797,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2792,0,6.095238,1,"[512, 1, 1]"
4224,0,33.664,7,6940680942895.28,100,7,kernel,780.190491,"[1, 16, 512]",38,10828,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2808,0,97.523811,1,"[256, 1, 1]"
4224,0,7.327,7,6940680942929.777,79,7,kernel,38.095238,"[25, 16, 1]",38,10830,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2808,0,4.761905,1,"[256, 1, 1]"
73728,0,100.32,7,6940680942937.873,0,7,kernel,2.666667,"[7, 8, 1]",180,10834,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_align4::Params),0,2808,0,0.666667,1,"[128, 1, 1]"
4224,0,2.912,7,6940680943039.473,79,7,kernel,38.095238,"[25, 16, 1]",40,10838,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2808,0,4.761905,1,"[256, 1, 1]"
4224,0,7.104,7,6940680943043.089,79,7,kernel,38.095238,"[25, 16, 1]",38,10858,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2808,0,4.761905,1,"[256, 1, 1]"
4224,0,6.848,7,6940680943050.865,79,7,kernel,38.095238,"[25, 16, 1]",38,10860,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2808,0,4.761905,1,"[256, 1, 1]"
73728,0,86.337,7,6940680943058.513,0,7,kernel,6.857143,"[8, 9, 1]",251,10864,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_analytic_tf32_256x128_16x3_nhwc_align4>(cutlass_tensorop_s1688wgrad_analytic_tf32_256x128_16x3_nhwc_align4::Params),0,2808,0,0.857143,1,"[256, 1, 1]"
4224,0,32.832,7,6940680943145.586,100,7,kernel,780.190491,"[1, 16, 512]",40,10868,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2808,0,97.523811,1,"[256, 1, 1]"
16,0,8.64,7,6940680943179.218,13,7,kernel,6.095238,"[32, 1, 1]",32,10882,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2811,0,0.380952,1,"[32, 16, 1]"
0,0,9.408,7,6940680943188.69,78,7,kernel,37.333332,"[784, 1, 1]",23,10909,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2823,0,9.333333,1,"[128, 1, 1]"
8464,0,14.048,7,6940680943198.866,67,7,kernel,97.523811,"[512, 1, 1]",45,10953,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2827,0,6.095238,1,"[512, 1, 1]"
4224,0,33.376,7,6940680943213.778,100,7,kernel,780.190491,"[1, 16, 512]",38,10984,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2843,0,97.523811,1,"[256, 1, 1]"
4224,0,7.488,7,6940680943247.891,79,7,kernel,38.095238,"[25, 16, 1]",38,10986,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2843,0,4.761905,1,"[256, 1, 1]"
73728,0,100.512,7,6940680943256.211,0,7,kernel,2.666667,"[7, 8, 1]",180,10990,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_align4::Params),0,2843,0,0.666667,1,"[128, 1, 1]"
4224,0,3.04,7,6940680943357.779,79,7,kernel,38.095238,"[25, 16, 1]",40,10994,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2843,0,4.761905,1,"[256, 1, 1]"
4224,0,7.04,7,6940680943361.523,79,7,kernel,38.095238,"[25, 16, 1]",38,11014,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2843,0,4.761905,1,"[256, 1, 1]"
4224,0,6.784,7,6940680943369.395,79,7,kernel,38.095238,"[25, 16, 1]",38,11016,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2843,0,4.761905,1,"[256, 1, 1]"
73728,0,86.625,7,6940680943376.979,0,7,kernel,6.857143,"[8, 9, 1]",251,11020,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_analytic_tf32_256x128_16x3_nhwc_align4>(cutlass_tensorop_s1688wgrad_analytic_tf32_256x128_16x3_nhwc_align4::Params),0,2843,0,0.857143,1,"[256, 1, 1]"
4224,0,33.024,7,6940680943464.34,100,7,kernel,780.190491,"[1, 16, 512]",40,11024,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2843,0,97.523811,1,"[256, 1, 1]"
16,0,8.256,7,6940680943498.164,13,7,kernel,6.095238,"[32, 1, 1]",32,11038,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2846,0,0.380952,1,"[32, 16, 1]"
0,0,9.76,7,6940680943507.188,78,7,kernel,37.333332,"[784, 1, 1]",23,11065,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2858,0,9.333333,1,"[128, 1, 1]"
8464,0,13.504,7,6940680943517.716,67,7,kernel,97.523811,"[512, 1, 1]",45,11109,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2862,0,6.095238,1,"[512, 1, 1]"
4224,0,18.144,7,6940680945822.531,100,7,kernel,390.095245,"[1, 8, 512]",38,11140,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2878,0,48.761906,1,"[256, 1, 1]"
4224,0,7.52,7,6940680945841.475,79,7,kernel,38.095238,"[25, 16, 1]",38,11142,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2878,0,4.761905,1,"[256, 1, 1]"
81920,0,54.08,7,6940680945849.731,0,7,kernel,2.476191,"[52, 1, 1]",118,11146,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_64x64_32x5_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_64x64_32x5_nhwc_unity_stride_align4::Params),0,2878,0,0.619048,1,"[128, 1, 1]"
4224,0,2.848,7,6940680945904.579,40,7,kernel,19.047619,"[25, 8, 1]",40,11150,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2878,0,2.380952,1,"[256, 1, 1]"
4224,0,4.256,7,6940680945908.163,40,7,kernel,19.047619,"[25, 8, 1]",38,11170,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2878,0,2.380952,1,"[256, 1, 1]"
4224,0,5.697,7,6940680945913.123,79,7,kernel,38.095238,"[25, 16, 1]",38,11172,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2878,0,4.761905,1,"[256, 1, 1]"
81920,0,46.464,7,6940680945919.651,0,7,kernel,3.428571,"[8, 9, 1]",234,11176,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4::Params),0,2878,0,0.857143,1,"[128, 1, 1]"
4224,0,15.168,7,6940680945966.82,100,7,kernel,390.095245,"[1, 8, 512]",40,11180,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2878,0,48.761906,1,"[256, 1, 1]"
16,0,8.225,7,6940680945982.755,13,7,kernel,6.095238,"[32, 1, 1]",32,11194,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2881,0,0.380952,1,"[32, 16, 1]"
0,0,3.808,7,6940680945991.78,100,7,kernel,74.666664,"[1568, 1, 1]",16,11220,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,2895,0,18.666666,1,"[128, 1, 1]"
0,0,19.008,7,6940680945996.324,100,7,kernel,316.952393,"[13, 1, 256]",40,11229,X,"void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)",0,2893,0,39.619049,1,"[256, 1, 1]"
0,0,11.36,7,6940680946016.068,100,7,kernel,74.666664,"[1568, 1, 1]",23,11246,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2898,0,18.666666,1,"[128, 1, 1]"
28944,0,17.344,7,6940680946028.26,67,7,kernel,48.761906,"[256, 1, 1]",45,11290,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2902,0,3.047619,1,"[512, 1, 1]"
4224,0,10.624,7,6940680946046.372,100,7,kernel,195.047623,"[1, 8, 256]",38,11321,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2918,0,24.380953,1,"[256, 1, 1]"
4224,0,11.936,7,6940680946057.7,100,7,kernel,74.666664,"[98, 8, 1]",38,11323,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2918,0,9.333333,1,"[256, 1, 1]"
73728,0,101.025,7,6940680946070.468,0,7,kernel,4.761905,"[100, 1, 1]",161,11327,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_unity_stride_align4::Params),0,2918,0,1.190476,1,"[128, 1, 1]"
4224,0,7.776,7,6940680946172.261,100,7,kernel,74.666664,"[98, 8, 1]",40,11331,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2918,0,9.333333,1,"[256, 1, 1]"
4224,0,13.12,7,6940680946180.901,100,7,kernel,74.666664,"[98, 8, 1]",38,11351,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2918,0,9.333333,1,"[256, 1, 1]"
4224,0,13.28,7,6940680946194.757,100,7,kernel,74.666664,"[98, 8, 1]",38,11353,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2918,0,9.333333,1,"[256, 1, 1]"
81920,0,78.976,7,6940680946210.821,0,7,kernel,3.428571,"[4, 9, 2]",234,11358,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4::Params),0,2918,0,0.857143,1,"[128, 1, 1]"
4224,0,7.36,7,6940680946290.534,100,7,kernel,195.047623,"[1, 8, 256]",40,11362,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2918,0,24.380953,1,"[256, 1, 1]"
16,0,14.176,7,6940680946298.694,6,7,kernel,3.047619,"[16, 1, 1]",32,11376,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2921,0,0.190476,1,"[32, 16, 1]"
0,0,16.736,7,6940680946313.67,100,7,kernel,74.666664,"[1568, 1, 1]",23,11403,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2933,0,18.666666,1,"[128, 1, 1]"
28944,0,17.44,7,6940680946331.142,67,7,kernel,48.761906,"[256, 1, 1]",45,11447,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2937,0,3.047619,1,"[512, 1, 1]"
4224,0,10.56,7,6940680946349.318,100,7,kernel,195.047623,"[1, 8, 256]",38,11478,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2953,0,24.380953,1,"[256, 1, 1]"
4224,0,12.0,7,6940680946360.55,100,7,kernel,74.666664,"[98, 8, 1]",38,11480,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2953,0,9.333333,1,"[256, 1, 1]"
73728,0,100.833,7,6940680946373.254,0,7,kernel,4.761905,"[100, 1, 1]",161,11484,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_unity_stride_align4::Params),0,2953,0,1.190476,1,"[128, 1, 1]"
4224,0,8.896,7,6940680946474.791,100,7,kernel,74.666664,"[98, 8, 1]",40,11488,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2953,0,9.333333,1,"[256, 1, 1]"
4224,0,13.248,7,6940680946484.519,100,7,kernel,74.666664,"[98, 8, 1]",38,11508,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2953,0,9.333333,1,"[256, 1, 1]"
4224,0,13.088,7,6940680946498.471,100,7,kernel,74.666664,"[98, 8, 1]",38,11510,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2953,0,9.333333,1,"[256, 1, 1]"
81920,0,78.624,7,6940680946514.343,0,7,kernel,3.428571,"[4, 9, 2]",234,11515,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4::Params),0,2953,0,0.857143,1,"[128, 1, 1]"
4224,0,7.328,7,6940680946593.736,100,7,kernel,195.047623,"[1, 8, 256]",40,11519,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2953,0,24.380953,1,"[256, 1, 1]"
16,0,14.049,7,6940680946601.767,6,7,kernel,3.047619,"[16, 1, 1]",32,11533,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2956,0,0.190476,1,"[32, 16, 1]"
0,0,16.416,7,6940680946616.584,100,7,kernel,74.666664,"[1568, 1, 1]",23,11560,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,2968,0,18.666666,1,"[128, 1, 1]"
28944,0,17.28,7,6940680946633.768,67,7,kernel,48.761906,"[256, 1, 1]",45,11604,X,"void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)",0,2972,0,3.047619,1,"[512, 1, 1]"
4224,0,6.688,7,6940680946651.784,100,7,kernel,97.523811,"[1, 4, 256]",38,11635,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2988,0,12.190476,1,"[256, 1, 1]"
4224,0,9.024,7,6940680946659.208,100,7,kernel,74.666664,"[98, 8, 1]",38,11637,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2988,0,9.333333,1,"[256, 1, 1]"
73728,0,49.568,7,6940680946669.0,0,7,kernel,2.380952,"[50, 1, 1]",167,11641,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4::Params),0,2988,0,0.595238,1,"[128, 1, 1]"
4224,0,3.328,7,6940680946719.464,78,7,kernel,37.333332,"[98, 4, 1]",40,11645,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2988,0,4.666667,1,"[256, 1, 1]"
4224,0,6.145,7,6940680946723.624,78,7,kernel,37.333332,"[98, 4, 1]",38,11665,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2988,0,4.666667,1,"[256, 1, 1]"
4224,0,12.128,7,6940680946730.568,100,7,kernel,74.666664,"[98, 8, 1]",38,11667,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,2988,0,9.333333,1,"[256, 1, 1]"
98304,0,60.96,7,6940680946743.465,0,7,kernel,3.428571,"[18, 4, 1]",128,11672,X,sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,2988,0,0.857143,1,"[128, 1, 1]"
4224,0,4.736,7,6940680946805.257,100,7,kernel,97.523811,"[1, 4, 256]",40,11675,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,2988,0,12.190476,1,"[256, 1, 1]"
16,0,13.6,7,6940680946810.729,6,7,kernel,3.047619,"[16, 1, 1]",32,11689,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,2991,0,0.190476,1,"[32, 16, 1]"
0,0,9.056,7,6940680946825.065,100,7,kernel,149.333328,"[3136, 1, 1]",16,11715,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,3005,0,37.333332,1,"[128, 1, 1]"
0,0,35.168,7,6940680946834.921,100,7,kernel,597.333313,"[49, 1, 128]",40,11724,X,"void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)",0,3003,0,74.666664,1,"[256, 1, 1]"
0,0,35.617,7,6940680946870.921,100,7,kernel,149.333328,"[3136, 1, 1]",23,11741,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,3008,0,37.333332,1,"[128, 1, 1]"
400,0,51.489,7,6940680946907.337,51,7,kernel,24.380953,"[128, 1, 1]",40,11784,X,"void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)",0,3012,0,1.52381,1,"[512, 1, 1]"
4224,0,4.544,7,6940680946959.658,100,7,kernel,48.761906,"[1, 4, 128]",38,11814,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3028,0,6.095238,1,"[256, 1, 1]"
4224,0,21.856,7,6940680946964.938,100,7,kernel,149.333328,"[392, 4, 1]",38,11816,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3028,0,18.666666,1,"[256, 1, 1]"
73728,0,87.201,7,6940680946987.562,0,7,kernel,9.333333,"[196, 1, 1]",167,11820,X,void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4::Params),0,3028,0,2.333333,1,"[128, 1, 1]"
4224,0,22.048,7,6940680947075.499,100,7,kernel,149.333328,"[392, 4, 1]",40,11824,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,3028,0,18.666666,1,"[256, 1, 1]"
4224,0,25.088,7,6940680947098.251,100,7,kernel,149.333328,"[392, 4, 1]",38,11844,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3028,0,18.666666,1,"[256, 1, 1]"
4224,0,25.184,7,6940680947124.107,100,7,kernel,149.333328,"[392, 4, 1]",38,11846,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3028,0,18.666666,1,"[256, 1, 1]"
98304,0,99.105,7,6940680947154.443,0,7,kernel,3.428571,"[18, 1, 4]",168,11854,X,sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,3028,0,0.857143,1,"[128, 1, 1]"
9216,0,3.456,7,6940680947254.412,7,7,kernel,3.428571,"[18, 1, 4]",64,11856,X,sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_split_k_kernel__5x_cudnn,0,3028,0,0.857143,1,"[128, 1, 1]"
4224,0,3.392,7,6940680947258.668,100,7,kernel,48.761906,"[1, 4, 128]",40,11859,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,3028,0,6.095238,1,"[256, 1, 1]"
2064,0,15.936,7,6940680947262.924,51,7,kernel,24.380953,"[128, 1, 1]",32,11872,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,3031,0,1.52381,1,"[32, 16, 1]"
0,0,29.696,7,6940680947279.596,100,7,kernel,149.333328,"[3136, 1, 1]",23,11899,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,3043,0,37.333332,1,"[128, 1, 1]"
400,0,51.681,7,6940680947310.092,51,7,kernel,24.380953,"[128, 1, 1]",40,11942,X,"void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)",0,3047,0,1.52381,1,"[512, 1, 1]"
8704,0,2.784,7,6940680949651.803,6,7,kernel,3.047619,"[2, 32, 1]",40,11973,X,"void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)",0,3063,0,0.761905,1,"[32, 4, 1]"
49152,0,77.952,7,6940680949655.419,33,7,kernel,18.666666,"[2, 14, 7]",126,11975,X,_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1,0,3063,0,2.333333,1,"[256, 1, 1]"
4224,0,10.848,7,6940680949734.075,100,7,kernel,74.666664,"[392, 2, 1]",38,11995,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3063,0,9.333333,1,"[256, 1, 1]"
4224,0,25.632,7,6940680949745.595,100,7,kernel,149.333328,"[392, 4, 1]",38,11997,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3063,0,18.666666,1,"[256, 1, 1]"
98304,0,66.976,7,6940680949777.308,0,7,kernel,3.428571,"[9, 2, 4]",128,12005,X,sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,3063,0,0.857143,1,"[128, 1, 1]"
4224,0,2.88,7,6940680949845.116,51,7,kernel,24.380953,"[1, 2, 128]",40,12008,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,3063,0,3.047619,1,"[256, 1, 1]"
2064,0,15.616,7,6940680949848.828,51,7,kernel,24.380953,"[128, 1, 1]",32,12021,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,3066,0,1.52381,1,"[32, 16, 1]"
0,0,18.944,7,6940680949865.212,100,7,kernel,298.666656,"[6272, 1, 1]",16,12047,X,"void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)",0,3080,0,74.666664,1,"[128, 1, 1]"
0,0,66.625,7,6940680949884.86,100,7,kernel,1194.666626,"[196, 1, 64]",40,12056,X,"void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)",0,3078,0,149.333328,1,"[256, 1, 1]"
0,0,69.056,7,6940680949952.189,100,7,kernel,298.666656,"[6272, 1, 1]",23,12073,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,3083,0,74.666664,1,"[128, 1, 1]"
400,0,120.449,7,6940680950022.077,25,7,kernel,12.190476,"[64, 1, 1]",40,12116,X,"void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)",0,3087,0,0.761905,1,"[512, 1, 1]"
8704,0,2.688,7,6940680950143.262,3,7,kernel,1.52381,"[2, 16, 1]",40,12147,X,"void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)",0,3103,0,0.380952,1,"[32, 4, 1]"
49152,0,141.857,7,6940680950146.686,33,7,kernel,74.666664,"[2, 28, 14]",126,12149,X,_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1,0,3103,0,9.333333,1,"[256, 1, 1]"
4224,0,47.008,7,6940680950289.247,100,7,kernel,298.666656,"[1568, 2, 1]",38,12169,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3103,0,37.333332,1,"[256, 1, 1]"
4224,0,48.256,7,6940680950337.055,100,7,kernel,298.666656,"[1568, 2, 1]",38,12171,X,"void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)",0,3103,0,37.333332,1,"[256, 1, 1]"
98304,0,122.08,7,6940680950391.168,0,7,kernel,3.428571,"[9, 1, 8]",128,12179,X,sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn,0,3103,0,0.857143,1,"[128, 1, 1]"
4224,0,2.624,7,6940680950514.048,25,7,kernel,12.190476,"[1, 2, 64]",40,12182,X,"void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)",0,3103,0,1.52381,1,"[256, 1, 1]"
2064,0,27.649,7,6940680950517.44,25,7,kernel,12.190476,"[64, 1, 1]",32,12195,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,3106,0,0.761905,1,"[32, 16, 1]"
0,0,62.849,7,6940680950545.856,100,7,kernel,298.666656,"[6272, 1, 1]",23,12222,X,"void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)",0,3118,0,74.666664,1,"[128, 1, 1]"
400,0,120.353,7,6940680950609.505,25,7,kernel,12.190476,"[64, 1, 1]",40,12265,X,"void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)",0,3122,0,0.761905,1,"[512, 1, 1]"
2304,0,78.528,7,6940680950733.282,10,7,kernel,4.666667,"[1, 2, 98]",80,12299,X,"void wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)",0,3138,0,2.333333,1,"[8, 8, 1]"
2064,0,22.432,7,6940680950812.578,25,7,kernel,12.190476,"[64, 1, 1]",32,12312,X,"void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)",0,3141,0,0.761905,1,"[32, 16, 1]"
0,0,398.85,7,6940680979307.094,100,7,kernel,60.952381,"[320, 1, 1]",40,12330,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,2056,0,3.809524,1,"[512, 1, 1]"
0,0,456.995,7,6940680979706.744,100,7,kernel,60.952381,"[320, 1, 1]",40,12333,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,2056,0,3.809524,1,"[512, 1, 1]"
0,0,454.979,7,6940680980164.571,100,7,kernel,60.952381,"[320, 1, 1]",40,12336,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,2056,0,3.809524,1,"[512, 1, 1]"
0,0,459.107,7,6940680980620.318,100,7,kernel,60.952381,"[320, 1, 1]",40,12339,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,2056,0,3.809524,1,"[512, 1, 1]"
0,0,2985.715,7,6940680981080.225,100,7,kernel,60.952381,"[320, 1, 1]",40,12342,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,2056,0,3.809524,1,"[512, 1, 1]"
0,0,455.651,7,6940680984066.836,100,7,kernel,60.952381,"[320, 1, 1]",40,12345,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,2056,0,3.809524,1,"[512, 1, 1]"
0,0,336.77,7,6940680984523.351,94,7,kernel,45.142857,"[237, 1, 1]",40,12348,X,"void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)",0,2056,0,2.821429,1,"[512, 1, 1]"
