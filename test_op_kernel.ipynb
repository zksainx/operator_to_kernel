{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k8smaster/miniconda3/envs/k8s/lib/python3.10/site-packages/torch/profiler/profiler.py:445: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件 resnet18_backward.json...\n",
      "文件 resnet18_backward.json 已成功转化为 log_csv/resnet18_backward 目录下的csv\n",
      "正在处理文件 resnet18_forward.json...\n",
      "文件 resnet18_forward.json 已成功转化为 log_csv/resnet18_forward 目录下的csv\n",
      "正在处理文件 VGG16.json...\n",
      "文件 VGG16.json 已成功转化为 log_csv/VGG16 目录下的csv\n",
      "正在处理文件 resnet18_optimize.json...\n",
      "文件 resnet18_optimize.json 已成功转化为 log_csv/resnet18_optimize 目录下的csv\n",
      "所有文件转换完成。\n",
      "Profiling completed. Trace log saved to './log/resnet18_forward.json'\n",
      "CSV files saved to 'log_csv'\n",
      "正在处理文件 resnet18_backward.json...\n",
      "文件 resnet18_backward.json 已成功转化为 log_csv/resnet18_backward 目录下的csv\n",
      "正在处理文件 resnet18_forward.json...\n",
      "文件 resnet18_forward.json 已成功转化为 log_csv/resnet18_forward 目录下的csv\n",
      "正在处理文件 VGG16.json...\n",
      "文件 VGG16.json 已成功转化为 log_csv/VGG16 目录下的csv\n",
      "正在处理文件 resnet18_optimize.json...\n",
      "文件 resnet18_optimize.json 已成功转化为 log_csv/resnet18_optimize 目录下的csv\n",
      "所有文件转换完成。\n",
      "Profiling completed. Trace log saved to './log/resnet18_backward.json'\n",
      "CSV files saved to 'log_csv'\n",
      "正在处理文件 resnet18_backward.json...\n",
      "文件 resnet18_backward.json 已成功转化为 log_csv/resnet18_backward 目录下的csv\n",
      "正在处理文件 resnet18_forward.json...\n",
      "文件 resnet18_forward.json 已成功转化为 log_csv/resnet18_forward 目录下的csv\n",
      "正在处理文件 VGG16.json...\n",
      "文件 VGG16.json 已成功转化为 log_csv/VGG16 目录下的csv\n",
      "正在处理文件 resnet18_optimize.json...\n",
      "文件 resnet18_optimize.json 已成功转化为 log_csv/resnet18_optimize 目录下的csv\n",
      "所有文件转换完成。\n",
      "Profiling completed. Trace log saved to './log/resnet18_optimize.json'\n",
      "CSV files saved to 'log_csv'\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.fx as fx\n",
    "import os\n",
    "\n",
    "from model_static_graph import extract_graph, draw_graph\n",
    "from pytorch_tracing import py_tracing_forward, py_tracing_backward, py_tracing_optimize\n",
    "from op_kernel_dict import get_op_kernel\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "#model\n",
    "model = torchvision.models.resnet18().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "input_data = torch.randn(5, 3, 224, 224).cuda()\n",
    "model_name='resnet18'\n",
    "\n",
    "#静态图\n",
    "static_graph,name_module,adj=extract_graph(model)\n",
    "draw_graph(adj,name_module, model_name=model_name)\n",
    "#draw_graph(adj,name_module, model_name=model_name,t=1)\n",
    "\n",
    "#pytorch_tracing\n",
    "py_tracing_forward(model, input_data, model_name=model_name)\n",
    "py_tracing_backward(model, input_data, model_name=model_name)\n",
    "py_tracing_optimize(model, input_data, optimizer, model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k8smaster/miniconda3/envs/k8s/lib/python3.10/site-packages/torch/profiler/profiler.py:445: UserWarning: Profiler won't be using warmup, this can skew profiler results\n",
      "  warn(\"Profiler won't be using warmup, this can skew profiler results\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件 resnet18_backward.json...\n",
      "文件 resnet18_backward.json 已成功转化为 log_csv/resnet18_backward 目录下的csv\n",
      "正在处理文件 resnet18_forward.json...\n",
      "文件 resnet18_forward.json 已成功转化为 log_csv/resnet18_forward 目录下的csv\n",
      "正在处理文件 VGG16_forward.json...\n",
      "文件 VGG16_forward.json 已成功转化为 log_csv/VGG16_forward 目录下的csv\n",
      "正在处理文件 VGG16.json...\n",
      "文件 VGG16.json 已成功转化为 log_csv/VGG16 目录下的csv\n",
      "正在处理文件 resnet18_optimize.json...\n",
      "文件 resnet18_optimize.json 已成功转化为 log_csv/resnet18_optimize 目录下的csv\n",
      "所有文件转换完成。\n",
      "Profiling completed. Trace log saved to './log/VGG16_forward.json'\n",
      "CSV files saved to 'log_csv'\n",
      "正在处理文件 resnet18_backward.json...\n",
      "文件 resnet18_backward.json 已成功转化为 log_csv/resnet18_backward 目录下的csv\n",
      "正在处理文件 resnet18_forward.json...\n",
      "文件 resnet18_forward.json 已成功转化为 log_csv/resnet18_forward 目录下的csv\n",
      "正在处理文件 VGG16_forward.json...\n",
      "文件 VGG16_forward.json 已成功转化为 log_csv/VGG16_forward 目录下的csv\n",
      "正在处理文件 VGG16_backward.json...\n",
      "文件 VGG16_backward.json 已成功转化为 log_csv/VGG16_backward 目录下的csv\n",
      "正在处理文件 VGG16.json...\n",
      "文件 VGG16.json 已成功转化为 log_csv/VGG16 目录下的csv\n",
      "正在处理文件 resnet18_optimize.json...\n",
      "文件 resnet18_optimize.json 已成功转化为 log_csv/resnet18_optimize 目录下的csv\n",
      "所有文件转换完成。\n",
      "Profiling completed. Trace log saved to './log/VGG16_backward.json'\n",
      "CSV files saved to 'log_csv'\n",
      "正在处理文件 resnet18_backward.json...\n",
      "文件 resnet18_backward.json 已成功转化为 log_csv/resnet18_backward 目录下的csv\n",
      "正在处理文件 resnet18_forward.json...\n",
      "文件 resnet18_forward.json 已成功转化为 log_csv/resnet18_forward 目录下的csv\n",
      "正在处理文件 VGG16_forward.json...\n",
      "文件 VGG16_forward.json 已成功转化为 log_csv/VGG16_forward 目录下的csv\n",
      "正在处理文件 VGG16_backward.json...\n",
      "文件 VGG16_backward.json 已成功转化为 log_csv/VGG16_backward 目录下的csv\n",
      "正在处理文件 VGG16.json...\n",
      "文件 VGG16.json 已成功转化为 log_csv/VGG16 目录下的csv\n",
      "正在处理文件 resnet18_optimize.json...\n",
      "文件 resnet18_optimize.json 已成功转化为 log_csv/resnet18_optimize 目录下的csv\n",
      "正在处理文件 VGG16_optimize.json...\n",
      "文件 VGG16_optimize.json 已成功转化为 log_csv/VGG16_optimize 目录下的csv\n",
      "所有文件转换完成。\n",
      "Profiling completed. Trace log saved to './log/VGG16_optimize.json'\n",
      "CSV files saved to 'log_csv'\n"
     ]
    }
   ],
   "source": [
    "from VGG import VGG16\n",
    "\n",
    "model = VGG16().cuda()\n",
    "input_data = torch.randn(1, 3, 224, 224).cuda()\n",
    "model_name='VGG16'\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "py_tracing_forward(model, input_data, model_name=model_name)\n",
    "py_tracing_backward(model, input_data, model_name=model_name)\n",
    "py_tracing_optimize(model, input_data, optimizer, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from op_kernel_dict import get_op_kernel\n",
    "\n",
    "Type = ['forward','backward','optimize']\n",
    "model_name=['resnet18','VGG16']\n",
    "#op_kernel={'resnet18':{},'VGG16':{}}\n",
    "op_kernel={}\n",
    "for model in model_name:\n",
    "    for tp in Type:\n",
    "        op_tmp=get_op_kernel(tp, model)\n",
    "        #对于op_kernel 里没有的key 直接更新key value，对于已经存在的ker，计算op_tmp与op_kernel的并集\n",
    "        op_kernel.update(op_tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aten::cudnn_convolution aten::convolution_backward {'void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)', 'void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)', 'void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)', '_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1'}\n",
      "aten::clamp_min_ aten::clamp {'void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)'}\n",
      "aten::convolution_backward aten::cudnn_convolution {'void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)', 'void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)', 'void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)', '_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1'}\n",
      "aten::clamp aten::clamp_min_ {'void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aten::cudnn_convolution': {'_5x_cudnn_ampere_scudnn_128x64_relu_medium_nn_v1',\n",
       "  '_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1',\n",
       "  'sm86_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn',\n",
       "  'sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize128x128x16_stage3_warpsize2x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn',\n",
       "  'sm86_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nchw_tilesize256x128x32_stage2_warpsize4x2x1_g1_tensor16x8x8_alignc4_execute_kernel__5x_cudnn',\n",
       "  'void cask__5x_cudnn::computeOffsetsKernel<false, false>(cask__5x_cudnn::ComputeOffsetsParams)',\n",
       "  'void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)',\n",
       "  'void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)',\n",
       "  'void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_16x6_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_16x6_nhwc_align4::Params)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4>(cutlass_tensorop_s1688fprop_optimized_tf32_128x64_32x3_nhwc_align4::Params)'},\n",
       " 'aten::add_': {'void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::CUDAFunctor_add<float> >(at::TensorIteratorBase&, at::native::CUDAFunctor_add<float> const&)::{lambda(int)#1})',\n",
       "  'void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>)'},\n",
       " 'aten::cudnn_batch_norm': {'void cudnn::bn_fw_tr_1C11_kernel_NCHW<float, float, int, 512, true, 1, true>(cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float const*, float, float, float*, float*, float*, float*, float, float)',\n",
       "  'void cudnn::bn_fw_tr_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_fw_tr_1C11_args<float>)'},\n",
       " 'aten::clamp_min_': {'void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)'},\n",
       " 'aten::max_pool2d_with_indices': {'void at::native::(anonymous namespace)::max_pool_forward_nchw<float>(int, float const*, long, long, long, int, int, int, int, int, int, int, int, int, int, float*, long*)'},\n",
       " 'aten::mean': {'void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MeanOps<float, float, float, float>, unsigned int, float, 4>)'},\n",
       " 'aten::addmm': {'std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, true, false, 7, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)',\n",
       "  'void gemv2T_kernel_val<int, int, float, float, float, float, 128, 16, 4, 4, false, true, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>, float, float)'},\n",
       " 'aten::sum': {'void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)',\n",
       "  'void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>)'},\n",
       " 'aten::fill_': {'void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)'},\n",
       " 'aten::copy_': {'void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#3}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1})'},\n",
       " 'aten::mm': {'ampere_sgemm_128x32_nn',\n",
       "  'std::enable_if<!(false), void>::type internal::gemvx::kernel<int, int, float, float, float, float, false, true, false, false, 9, false, cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float> >(cublasGemvParamsEx<int, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float>)',\n",
       "  'void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, float, true, false, false>(cublasLt::cublasSplitKParams<float>, float const*, float const*, float*, float const*, float const*, float const*, float const*, float*, void*, long, float*, int*)',\n",
       "  'void gemmk1_kernel<int, float, 256, 5, false, false, false, false, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, 0>(cublasGemmk1Params<float, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float const>, cublasGemvTensorStridedBatched<float>, float, biasType<cublasGemvTensorStridedBatched<float>::value_type, float>::type>)'},\n",
       " 'aten::div': {'void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl_nocast<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl_nocast<at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#1})'},\n",
       " 'aten::threshold_backward': {'void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::(anonymous namespace)::threshold_kernel_impl<float>(at::TensorIteratorBase&, float, float)::{lambda(float, float)#1}>, at::detail::Array<char*, 3>)'},\n",
       " 'aten::cudnn_batch_norm_backward': {'void cudnn::bn_bw_1C11_kernel_new<float, float, float2, 512, true, 1>(float, float, float, float, cudnnTensorStruct, float const*, cudnnTensorStruct, float const*, cudnnTensorStruct, float*, float const*, float*, float*, float const*, float const*, float)',\n",
       "  'void cudnn::bn_bw_1C11_singleread<float, 512, true, 1, 2, 0>(cudnn::bn_bw_1C11_args<float>)'},\n",
       " 'aten::convolution_backward': {'_5x_cudnn_ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1',\n",
       "  'sm80_xmma_dgrad_implicit_gemm_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn',\n",
       "  'sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn',\n",
       "  'sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x1_g1_tensor16x8x8_execute_split_k_kernel__5x_cudnn',\n",
       "  'sm86_xmma_wgrad_implicit_gemm_indexed_tf32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage3_warpsize2x2x1_g1_tensor16x8x8_execute_kernel__5x_cudnn',\n",
       "  'void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, false, true, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<float>, float const*, float*)',\n",
       "  'void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, true, false, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<float>, float const*, float*)',\n",
       "  'void cudnn::winograd::generateWinogradTilesKernel<0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_align4::Params)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_16x6_nhwc_unity_stride_align4::Params)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4::Params)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_64x64_32x5_nhwc_unity_stride_align4>(cutlass_tensorop_s1688dgrad_optimized_tf32_64x64_32x5_nhwc_unity_stride_align4::Params)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_analytic_tf32_256x128_16x3_nhwc_align4>(cutlass_tensorop_s1688wgrad_analytic_tf32_256x128_16x3_nhwc_align4::Params)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x128_16x3_nhwc_align4::Params)',\n",
       "  'void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4>(cutlass_tensorop_s1688wgrad_optimized_tf32_256x64_16x4_nhwc_align4::Params)',\n",
       "  'void wgrad_alg0_engine<float, 128, 5, 5, 3, 3, 3, false, 512>(int, int, int, float const*, int, float*, float const*, kernel_grad_params, unsigned long long, int, float, int, int, int, int)'},\n",
       " 'aten::max_pool2d_with_indices_backward': {'void at::native::(anonymous namespace)::max_pool_backward_nchw<float, float>(float const*, long const*, int, long, long, long, int, int, int, int, int, int, int, int, int, int, float*)'},\n",
       " 'aten::_foreach_add_': {'void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float)'},\n",
       " 'aten::clamp': {'void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::(anonymous namespace)::launch_clamp_scalar(at::TensorIteratorBase&, c10::Scalar, c10::Scalar, at::native::detail::ClampLimits)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)'},\n",
       " 'aten::native_dropout': {'void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4, bool>(at::cuda::detail::TensorInfo<float const, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<bool, unsigned int>, unsigned int, float, at::PhiloxCudaState)'},\n",
       " 'aten::native_dropout_backward': {'void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<bool, float, float>(at::Tensor&, at::Tensor const&, at::Tensor const&, float)::{lambda(float, bool)#1}, at::detail::Array<char*, 3>)'},\n",
       " 'aten::hardtanh_backward': {'void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::hardtanh_backward_kernel(at::TensorIterator&, c10::Scalar const&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float, float)#1}, at::detail::Array<char*, 3>)'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#比较op_kernel里每个key的set是否有交集\n",
    "for key1 in op_kernel:\n",
    "    for key2 in op_kernel:\n",
    "        if key1!=key2:\n",
    "            if len(op_kernel[key1]&op_kernel[key2])>0:\n",
    "                print(key1,key2,op_kernel[key1]&op_kernel[key2])\n",
    "\n",
    "#key的set\n",
    "op_kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k8s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
